{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z62</th>\n",
       "      <th>X63</th>\n",
       "      <th>Y63</th>\n",
       "      <th>Z63</th>\n",
       "      <th>X64</th>\n",
       "      <th>Y64</th>\n",
       "      <th>Z64</th>\n",
       "      <th>X65</th>\n",
       "      <th>Y65</th>\n",
       "      <th>Z65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.290488</td>\n",
       "      <td>-75.818914</td>\n",
       "      <td>-3.229877</td>\n",
       "      <td>-0.166410</td>\n",
       "      <td>-77.328566</td>\n",
       "      <td>-3.061451</td>\n",
       "      <td>-0.098528</td>\n",
       "      <td>-82.594196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349284</td>\n",
       "      <td>0.610715</td>\n",
       "      <td>0.973082</td>\n",
       "      <td>-0.303378</td>\n",
       "      <td>0.405078</td>\n",
       "      <td>0.922159</td>\n",
       "      <td>-0.281479</td>\n",
       "      <td>0.559808</td>\n",
       "      <td>0.851646</td>\n",
       "      <td>-0.028510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346683</td>\n",
       "      <td>0.613981</td>\n",
       "      <td>0.946346</td>\n",
       "      <td>-0.395646</td>\n",
       "      <td>0.413621</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>-0.267950</td>\n",
       "      <td>0.558445</td>\n",
       "      <td>0.851427</td>\n",
       "      <td>-0.036818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820756</td>\n",
       "      <td>0.612683</td>\n",
       "      <td>0.950004</td>\n",
       "      <td>-0.379825</td>\n",
       "      <td>0.448817</td>\n",
       "      <td>0.581546</td>\n",
       "      <td>-0.748774</td>\n",
       "      <td>0.557367</td>\n",
       "      <td>0.867107</td>\n",
       "      <td>-0.040340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892051</td>\n",
       "      <td>0.607402</td>\n",
       "      <td>0.961182</td>\n",
       "      <td>-0.300694</td>\n",
       "      <td>0.439404</td>\n",
       "      <td>0.449596</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>0.556838</td>\n",
       "      <td>0.878659</td>\n",
       "      <td>-0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>-2.649841</td>\n",
       "      <td>-1.005892</td>\n",
       "      <td>0.190630</td>\n",
       "      <td>-5.362860</td>\n",
       "      <td>-1.022167</td>\n",
       "      <td>0.243538</td>\n",
       "      <td>-8.214614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829807</td>\n",
       "      <td>0.604378</td>\n",
       "      <td>0.950633</td>\n",
       "      <td>-0.247957</td>\n",
       "      <td>0.445261</td>\n",
       "      <td>0.344964</td>\n",
       "      <td>-0.765876</td>\n",
       "      <td>0.552638</td>\n",
       "      <td>0.877294</td>\n",
       "      <td>-0.045415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.255172</td>\n",
       "      <td>-14.686667</td>\n",
       "      <td>2.176972</td>\n",
       "      <td>-0.137746</td>\n",
       "      <td>-15.407824</td>\n",
       "      <td>3.206680</td>\n",
       "      <td>-0.088495</td>\n",
       "      <td>-16.454638</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189617</td>\n",
       "      <td>0.678464</td>\n",
       "      <td>0.430918</td>\n",
       "      <td>-0.998512</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>0.832112</td>\n",
       "      <td>-0.110380</td>\n",
       "      <td>0.559508</td>\n",
       "      <td>0.746648</td>\n",
       "      <td>0.006331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14136</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.247349</td>\n",
       "      <td>-18.125843</td>\n",
       "      <td>0.387121</td>\n",
       "      <td>-0.123823</td>\n",
       "      <td>-19.070908</td>\n",
       "      <td>-0.996675</td>\n",
       "      <td>-0.073921</td>\n",
       "      <td>-20.329048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235708</td>\n",
       "      <td>0.673772</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>-0.173289</td>\n",
       "      <td>0.301638</td>\n",
       "      <td>0.830727</td>\n",
       "      <td>-0.135007</td>\n",
       "      <td>0.558916</td>\n",
       "      <td>0.736754</td>\n",
       "      <td>0.016739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14137</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.252560</td>\n",
       "      <td>-28.690787</td>\n",
       "      <td>5.913587</td>\n",
       "      <td>-0.133411</td>\n",
       "      <td>-30.150542</td>\n",
       "      <td>5.626362</td>\n",
       "      <td>-0.083474</td>\n",
       "      <td>-32.168021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.643596</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>0.818583</td>\n",
       "      <td>-0.402061</td>\n",
       "      <td>0.293410</td>\n",
       "      <td>0.830436</td>\n",
       "      <td>-0.519518</td>\n",
       "      <td>0.550947</td>\n",
       "      <td>0.735413</td>\n",
       "      <td>0.027168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14138</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.263622</td>\n",
       "      <td>-44.649560</td>\n",
       "      <td>6.507309</td>\n",
       "      <td>-0.145083</td>\n",
       "      <td>-46.797549</td>\n",
       "      <td>6.096631</td>\n",
       "      <td>-0.096428</td>\n",
       "      <td>-49.825290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573453</td>\n",
       "      <td>0.627514</td>\n",
       "      <td>0.836671</td>\n",
       "      <td>-0.288298</td>\n",
       "      <td>0.281208</td>\n",
       "      <td>0.831862</td>\n",
       "      <td>-0.455801</td>\n",
       "      <td>0.543025</td>\n",
       "      <td>0.740520</td>\n",
       "      <td>0.026633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14139</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.281000</td>\n",
       "      <td>-67.864949</td>\n",
       "      <td>7.771813</td>\n",
       "      <td>-0.160708</td>\n",
       "      <td>-71.308601</td>\n",
       "      <td>7.211858</td>\n",
       "      <td>-0.112549</td>\n",
       "      <td>-76.094353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521230</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>-0.280609</td>\n",
       "      <td>0.273794</td>\n",
       "      <td>0.836519</td>\n",
       "      <td>-0.409446</td>\n",
       "      <td>0.534971</td>\n",
       "      <td>0.747671</td>\n",
       "      <td>0.022982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14140 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Frame       X00         Y00       Z00       X01         Y01  \\\n",
       "0      Boy      1 -0.290488  -75.818914 -3.229877 -0.166410  -77.328566   \n",
       "1      Boy      2 -2.630732  163.577346 -2.202535 -2.630732  163.577346   \n",
       "2      Boy      3 -2.649066   62.903229 -2.865200 -2.649066   62.903229   \n",
       "3      Boy      4 -2.641445   73.387394 -6.816939 -2.641445   73.387394   \n",
       "4      Boy      5  0.075973   -2.649841 -1.005892  0.190630   -5.362860   \n",
       "...    ...    ...       ...         ...       ...       ...         ...   \n",
       "14135  You     16 -0.255172  -14.686667  2.176972 -0.137746  -15.407824   \n",
       "14136  You     17 -0.247349  -18.125843  0.387121 -0.123823  -19.070908   \n",
       "14137  You     18 -0.252560  -28.690787  5.913587 -0.133411  -30.150542   \n",
       "14138  You     19 -0.263622  -44.649560  6.507309 -0.145083  -46.797549   \n",
       "14139  You     20 -0.281000  -67.864949  7.771813 -0.160708  -71.308601   \n",
       "\n",
       "            Z01       X02         Y02  ...       Z62       X63       Y63  \\\n",
       "0     -3.061451 -0.098528  -82.594196  ... -0.349284  0.610715  0.973082   \n",
       "1     -2.202535 -2.630732  163.577346  ... -0.346683  0.613981  0.946346   \n",
       "2     -2.865200 -2.649066   62.903229  ... -0.820756  0.612683  0.950004   \n",
       "3     -6.816939 -2.641445   73.387394  ... -0.892051  0.607402  0.961182   \n",
       "4     -1.022167  0.243538   -8.214614  ... -0.829807  0.604378  0.950633   \n",
       "...         ...       ...         ...  ...       ...       ...       ...   \n",
       "14135  3.206680 -0.088495  -16.454638  ... -0.189617  0.678464  0.430918   \n",
       "14136 -0.996675 -0.073921  -20.329048  ... -0.235708  0.673772  0.673639   \n",
       "14137  5.626362 -0.083474  -32.168021  ... -0.643596  0.653561  0.818583   \n",
       "14138  6.096631 -0.096428  -49.825290  ... -0.573453  0.627514  0.836671   \n",
       "14139  7.211858 -0.112549  -76.094353  ... -0.521230  0.608247  0.844743   \n",
       "\n",
       "            Z63       X64       Y64       Z64       X65       Y65       Z65  \n",
       "0     -0.303378  0.405078  0.922159 -0.281479  0.559808  0.851646 -0.028510  \n",
       "1     -0.395646  0.413621  0.845886 -0.267950  0.558445  0.851427 -0.036818  \n",
       "2     -0.379825  0.448817  0.581546 -0.748774  0.557367  0.867107 -0.040340  \n",
       "3     -0.300694  0.439404  0.449596 -0.823084  0.556838  0.878659 -0.022312  \n",
       "4     -0.247957  0.445261  0.344964 -0.765876  0.552638  0.877294 -0.045415  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "14135 -0.998512  0.307135  0.832112 -0.110380  0.559508  0.746648  0.006331  \n",
       "14136 -0.173289  0.301638  0.830727 -0.135007  0.558916  0.736754  0.016739  \n",
       "14137 -0.402061  0.293410  0.830436 -0.519518  0.550947  0.735413  0.027168  \n",
       "14138 -0.288298  0.281208  0.831862 -0.455801  0.543025  0.740520  0.026633  \n",
       "14139 -0.280609  0.273794  0.836519 -0.409446  0.534971  0.747671  0.022982  \n",
       "\n",
       "[14140 rows x 200 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load each .npy file and convert the list of dictionaries to a DataFrame\n",
    "dataframes = []\n",
    "for file in os.listdir('dataset'):\n",
    "    if file.endswith('.npy'):\n",
    "        data = np.load('dataset/' + file, allow_pickle=True)\n",
    "        dataframes.append(pd.DataFrame.from_records(data))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = merged_df['Frame'].max()\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_9192\\2834639197.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z62</th>\n",
       "      <th>X63</th>\n",
       "      <th>Y63</th>\n",
       "      <th>Z63</th>\n",
       "      <th>X64</th>\n",
       "      <th>Y64</th>\n",
       "      <th>Z64</th>\n",
       "      <th>X65</th>\n",
       "      <th>Y65</th>\n",
       "      <th>Z65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.290488</td>\n",
       "      <td>-75.818914</td>\n",
       "      <td>-3.229877</td>\n",
       "      <td>-0.166410</td>\n",
       "      <td>-77.328566</td>\n",
       "      <td>-3.061451</td>\n",
       "      <td>-0.098528</td>\n",
       "      <td>-82.594196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349284</td>\n",
       "      <td>0.610715</td>\n",
       "      <td>0.973082</td>\n",
       "      <td>-0.303378</td>\n",
       "      <td>0.405078</td>\n",
       "      <td>0.922159</td>\n",
       "      <td>-0.281479</td>\n",
       "      <td>0.559808</td>\n",
       "      <td>0.851646</td>\n",
       "      <td>-0.028510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346683</td>\n",
       "      <td>0.613981</td>\n",
       "      <td>0.946346</td>\n",
       "      <td>-0.395646</td>\n",
       "      <td>0.413621</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>-0.267950</td>\n",
       "      <td>0.558445</td>\n",
       "      <td>0.851427</td>\n",
       "      <td>-0.036818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820756</td>\n",
       "      <td>0.612683</td>\n",
       "      <td>0.950004</td>\n",
       "      <td>-0.379825</td>\n",
       "      <td>0.448817</td>\n",
       "      <td>0.581546</td>\n",
       "      <td>-0.748774</td>\n",
       "      <td>0.557367</td>\n",
       "      <td>0.867107</td>\n",
       "      <td>-0.040340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892051</td>\n",
       "      <td>0.607402</td>\n",
       "      <td>0.961182</td>\n",
       "      <td>-0.300694</td>\n",
       "      <td>0.439404</td>\n",
       "      <td>0.449596</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>0.556838</td>\n",
       "      <td>0.878659</td>\n",
       "      <td>-0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>-2.649841</td>\n",
       "      <td>-1.005892</td>\n",
       "      <td>0.190630</td>\n",
       "      <td>-5.362860</td>\n",
       "      <td>-1.022167</td>\n",
       "      <td>0.243538</td>\n",
       "      <td>-8.214614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829807</td>\n",
       "      <td>0.604378</td>\n",
       "      <td>0.950633</td>\n",
       "      <td>-0.247957</td>\n",
       "      <td>0.445261</td>\n",
       "      <td>0.344964</td>\n",
       "      <td>-0.765876</td>\n",
       "      <td>0.552638</td>\n",
       "      <td>0.877294</td>\n",
       "      <td>-0.045415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.255172</td>\n",
       "      <td>-14.686667</td>\n",
       "      <td>2.176972</td>\n",
       "      <td>-0.137746</td>\n",
       "      <td>-15.407824</td>\n",
       "      <td>3.206680</td>\n",
       "      <td>-0.088495</td>\n",
       "      <td>-16.454638</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189617</td>\n",
       "      <td>0.678464</td>\n",
       "      <td>0.430918</td>\n",
       "      <td>-0.998512</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>0.832112</td>\n",
       "      <td>-0.110380</td>\n",
       "      <td>0.559508</td>\n",
       "      <td>0.746648</td>\n",
       "      <td>0.006331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14136</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.247349</td>\n",
       "      <td>-18.125843</td>\n",
       "      <td>0.387121</td>\n",
       "      <td>-0.123823</td>\n",
       "      <td>-19.070908</td>\n",
       "      <td>-0.996675</td>\n",
       "      <td>-0.073921</td>\n",
       "      <td>-20.329048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235708</td>\n",
       "      <td>0.673772</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>-0.173289</td>\n",
       "      <td>0.301638</td>\n",
       "      <td>0.830727</td>\n",
       "      <td>-0.135007</td>\n",
       "      <td>0.558916</td>\n",
       "      <td>0.736754</td>\n",
       "      <td>0.016739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14137</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.252560</td>\n",
       "      <td>-28.690787</td>\n",
       "      <td>5.913587</td>\n",
       "      <td>-0.133411</td>\n",
       "      <td>-30.150542</td>\n",
       "      <td>5.626362</td>\n",
       "      <td>-0.083474</td>\n",
       "      <td>-32.168021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.643596</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>0.818583</td>\n",
       "      <td>-0.402061</td>\n",
       "      <td>0.293410</td>\n",
       "      <td>0.830436</td>\n",
       "      <td>-0.519518</td>\n",
       "      <td>0.550947</td>\n",
       "      <td>0.735413</td>\n",
       "      <td>0.027168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14138</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.263622</td>\n",
       "      <td>-44.649560</td>\n",
       "      <td>6.507309</td>\n",
       "      <td>-0.145083</td>\n",
       "      <td>-46.797549</td>\n",
       "      <td>6.096631</td>\n",
       "      <td>-0.096428</td>\n",
       "      <td>-49.825290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573453</td>\n",
       "      <td>0.627514</td>\n",
       "      <td>0.836671</td>\n",
       "      <td>-0.288298</td>\n",
       "      <td>0.281208</td>\n",
       "      <td>0.831862</td>\n",
       "      <td>-0.455801</td>\n",
       "      <td>0.543025</td>\n",
       "      <td>0.740520</td>\n",
       "      <td>0.026633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14139</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.281000</td>\n",
       "      <td>-67.864949</td>\n",
       "      <td>7.771813</td>\n",
       "      <td>-0.160708</td>\n",
       "      <td>-71.308601</td>\n",
       "      <td>7.211858</td>\n",
       "      <td>-0.112549</td>\n",
       "      <td>-76.094353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521230</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>-0.280609</td>\n",
       "      <td>0.273794</td>\n",
       "      <td>0.836519</td>\n",
       "      <td>-0.409446</td>\n",
       "      <td>0.534971</td>\n",
       "      <td>0.747671</td>\n",
       "      <td>0.022982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14140 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Frame       X00         Y00       Z00       X01         Y01  \\\n",
       "0      Boy      1 -0.290488  -75.818914 -3.229877 -0.166410  -77.328566   \n",
       "1      Boy      2 -2.630732  163.577346 -2.202535 -2.630732  163.577346   \n",
       "2      Boy      3 -2.649066   62.903229 -2.865200 -2.649066   62.903229   \n",
       "3      Boy      4 -2.641445   73.387394 -6.816939 -2.641445   73.387394   \n",
       "4      Boy      5  0.075973   -2.649841 -1.005892  0.190630   -5.362860   \n",
       "...    ...    ...       ...         ...       ...       ...         ...   \n",
       "14135  You     16 -0.255172  -14.686667  2.176972 -0.137746  -15.407824   \n",
       "14136  You     17 -0.247349  -18.125843  0.387121 -0.123823  -19.070908   \n",
       "14137  You     18 -0.252560  -28.690787  5.913587 -0.133411  -30.150542   \n",
       "14138  You     19 -0.263622  -44.649560  6.507309 -0.145083  -46.797549   \n",
       "14139  You     20 -0.281000  -67.864949  7.771813 -0.160708  -71.308601   \n",
       "\n",
       "            Z01       X02         Y02  ...       Z62       X63       Y63  \\\n",
       "0     -3.061451 -0.098528  -82.594196  ... -0.349284  0.610715  0.973082   \n",
       "1     -2.202535 -2.630732  163.577346  ... -0.346683  0.613981  0.946346   \n",
       "2     -2.865200 -2.649066   62.903229  ... -0.820756  0.612683  0.950004   \n",
       "3     -6.816939 -2.641445   73.387394  ... -0.892051  0.607402  0.961182   \n",
       "4     -1.022167  0.243538   -8.214614  ... -0.829807  0.604378  0.950633   \n",
       "...         ...       ...         ...  ...       ...       ...       ...   \n",
       "14135  3.206680 -0.088495  -16.454638  ... -0.189617  0.678464  0.430918   \n",
       "14136 -0.996675 -0.073921  -20.329048  ... -0.235708  0.673772  0.673639   \n",
       "14137  5.626362 -0.083474  -32.168021  ... -0.643596  0.653561  0.818583   \n",
       "14138  6.096631 -0.096428  -49.825290  ... -0.573453  0.627514  0.836671   \n",
       "14139  7.211858 -0.112549  -76.094353  ... -0.521230  0.608247  0.844743   \n",
       "\n",
       "            Z63       X64       Y64       Z64       X65       Y65       Z65  \n",
       "0     -0.303378  0.405078  0.922159 -0.281479  0.559808  0.851646 -0.028510  \n",
       "1     -0.395646  0.413621  0.845886 -0.267950  0.558445  0.851427 -0.036818  \n",
       "2     -0.379825  0.448817  0.581546 -0.748774  0.557367  0.867107 -0.040340  \n",
       "3     -0.300694  0.439404  0.449596 -0.823084  0.556838  0.878659 -0.022312  \n",
       "4     -0.247957  0.445261  0.344964 -0.765876  0.552638  0.877294 -0.045415  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "14135 -0.998512  0.307135  0.832112 -0.110380  0.559508  0.746648  0.006331  \n",
       "14136 -0.173289  0.301638  0.830727 -0.135007  0.558916  0.736754  0.016739  \n",
       "14137 -0.402061  0.293410  0.830436 -0.519518  0.550947  0.735413  0.027168  \n",
       "14138 -0.288298  0.281208  0.831862 -0.455801  0.543025  0.740520  0.026633  \n",
       "14139 -0.280609  0.273794  0.836519 -0.409446  0.534971  0.747671  0.022982  \n",
       "\n",
       "[14140 rows x 200 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(group):\n",
    "    # Calculate the number of padding rows needed\n",
    "    padding_rows = max_sequence_length - len(group)\n",
    "    \n",
    "    # Create a DataFrame with padding rows filled with NaN (or any other padding value)\n",
    "    padding_df = pd.DataFrame({\n",
    "        'Word': [group['Word'].iloc[0]] * padding_rows,\n",
    "        'Frame': np.arange(len(group) + 1, max_sequence_length + 1),\n",
    "    })\n",
    "    \n",
    "    # Concatenate the original group with the padding DataFrame\n",
    "    return pd.concat([group, padding_df], ignore_index=True)\n",
    "\n",
    "# Group the DataFrame by 'Word' and apply the padding function\n",
    "padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n",
    "padded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = padded_df.drop('Word', axis=1)\n",
    "target = padded_df['Word']\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "features_scaled = scaler.fit_transform(data)\n",
    "\n",
    "import pickle\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the interval\n",
    "# interval = 20\n",
    "\n",
    "# # Initialize lists to hold the training and testing sets\n",
    "# X_train_list = []\n",
    "# y_train_list = []\n",
    "# X_test_list = []\n",
    "# y_test_list = []\n",
    "\n",
    "# # Iterate over features_scaled with a step of 20\n",
    "# for i in range(0, len(padded_df) - interval, 3*interval):\n",
    "#     # Slice the dataset for the current interval\n",
    "#     X_train = pd.DataFrame(features_scaled[i:i+2*interval])\n",
    "#     y_train = pd.DataFrame(target[i:i+2*interval])\n",
    "\n",
    "#     X_test = pd.DataFrame(features_scaled[i+2*interval:i+3*interval])\n",
    "#     y_test = pd.DataFrame(target[i+2*interval:i+3*interval])\n",
    "\n",
    "#     # Append the sliced DataFrames to the lists\n",
    "#     X_train_list.append(X_train)\n",
    "#     y_train_list.append(y_train)\n",
    "#     X_test_list.append(X_test)\n",
    "#     y_test_list.append(y_test)\n",
    "\n",
    "# # Convert the lists to DataFrames\n",
    "# X_train_df = pd.concat(X_train_list)\n",
    "# y_train_df = pd.concat(y_train_list)\n",
    "# X_test_df = pd.concat(X_test_list)\n",
    "# y_test_df = pd.concat(y_test_list)\n",
    "\n",
    "# X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Initialize the model\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = clf.predict(X_test_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(classification_report(y_test_df, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('rfmodel.pkl', 'wb') as file:\n",
    "#     pickle.dump(clf, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of time steps\n",
    "time_steps = 20\n",
    "\n",
    "# Prepare the data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(features_scaled) - time_steps, time_steps):\n",
    "    X.append(features_scaled[i:i + time_steps])\n",
    "    y.append(target[i])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape X to fit the LSTM input shape: [samples, time steps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'y_train' is your target variable with categorical values\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax')) # num_classes is the number of unique words you're predicting\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/73\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 1.1155 - accuracy: 0.6114 - val_loss: 1.2368 - val_accuracy: 0.5614\n",
      "Epoch 2/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0788 - accuracy: 0.6016 - val_loss: 1.1945 - val_accuracy: 0.5614\n",
      "Epoch 3/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0613 - accuracy: 0.6272 - val_loss: 1.1595 - val_accuracy: 0.5789\n",
      "Epoch 4/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0367 - accuracy: 0.6469 - val_loss: 1.0458 - val_accuracy: 0.6667\n",
      "Epoch 5/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0517 - accuracy: 0.6095 - val_loss: 0.9866 - val_accuracy: 0.7193\n",
      "Epoch 6/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.1408 - accuracy: 0.5523 - val_loss: 1.4100 - val_accuracy: 0.3509\n",
      "Epoch 7/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.3324 - accuracy: 0.4852 - val_loss: 1.1322 - val_accuracy: 0.5614\n",
      "Epoch 8/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2945 - accuracy: 0.4990 - val_loss: 1.3935 - val_accuracy: 0.3860\n",
      "Epoch 9/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.2166 - accuracy: 0.5108 - val_loss: 1.0852 - val_accuracy: 0.5439\n",
      "Epoch 10/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0836 - accuracy: 0.6213 - val_loss: 1.0546 - val_accuracy: 0.5789\n",
      "Epoch 11/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9512 - accuracy: 0.6607 - val_loss: 0.8930 - val_accuracy: 0.6667\n",
      "Epoch 12/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8849 - accuracy: 0.6607 - val_loss: 0.9517 - val_accuracy: 0.6316\n",
      "Epoch 13/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9234 - accuracy: 0.6568 - val_loss: 0.9028 - val_accuracy: 0.7018\n",
      "Epoch 14/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.9249 - accuracy: 0.6627 - val_loss: 0.8789 - val_accuracy: 0.6842\n",
      "Epoch 15/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0448 - accuracy: 0.6114 - val_loss: 1.0043 - val_accuracy: 0.6491\n",
      "Epoch 16/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0216 - accuracy: 0.6036 - val_loss: 0.9198 - val_accuracy: 0.6842\n",
      "Epoch 17/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9224 - accuracy: 0.6824 - val_loss: 0.9928 - val_accuracy: 0.5789\n",
      "Epoch 18/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.9393 - accuracy: 0.6489 - val_loss: 0.9399 - val_accuracy: 0.6842\n",
      "Epoch 19/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9023 - accuracy: 0.6647 - val_loss: 0.8905 - val_accuracy: 0.6667\n",
      "Epoch 20/73\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.8210 - accuracy: 0.7258 - val_loss: 0.8247 - val_accuracy: 0.7193\n",
      "Epoch 21/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.7781 - accuracy: 0.7278 - val_loss: 0.9051 - val_accuracy: 0.6316\n",
      "Epoch 22/73\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8094 - accuracy: 0.7199 - val_loss: 0.8592 - val_accuracy: 0.6491\n",
      "Epoch 23/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7719 - accuracy: 0.7318 - val_loss: 0.8464 - val_accuracy: 0.6316\n",
      "Epoch 24/73\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9366 - accuracy: 0.6607 - val_loss: 1.2607 - val_accuracy: 0.4912\n",
      "Epoch 25/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9203 - accuracy: 0.6529 - val_loss: 1.0409 - val_accuracy: 0.5439\n",
      "Epoch 26/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.8085 - accuracy: 0.7258 - val_loss: 0.8773 - val_accuracy: 0.7193\n",
      "Epoch 27/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.9183 - accuracy: 0.6509 - val_loss: 0.8373 - val_accuracy: 0.7193\n",
      "Epoch 28/73\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.8131 - accuracy: 0.7219 - val_loss: 0.8001 - val_accuracy: 0.6667\n",
      "Epoch 29/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7280 - accuracy: 0.7515 - val_loss: 0.8780 - val_accuracy: 0.6491\n",
      "Epoch 30/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7478 - accuracy: 0.7377 - val_loss: 0.8040 - val_accuracy: 0.6667\n",
      "Epoch 31/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6987 - accuracy: 0.7830 - val_loss: 0.7439 - val_accuracy: 0.7368\n",
      "Epoch 32/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6822 - accuracy: 0.7771 - val_loss: 0.8024 - val_accuracy: 0.6842\n",
      "Epoch 33/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.7179 - accuracy: 0.7416 - val_loss: 0.8200 - val_accuracy: 0.7018\n",
      "Epoch 34/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7113 - accuracy: 0.7475 - val_loss: 0.7868 - val_accuracy: 0.7193\n",
      "Epoch 35/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6950 - accuracy: 0.7594 - val_loss: 0.7640 - val_accuracy: 0.6491\n",
      "Epoch 36/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6605 - accuracy: 0.7732 - val_loss: 0.7356 - val_accuracy: 0.7018\n",
      "Epoch 37/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6472 - accuracy: 0.7751 - val_loss: 0.7768 - val_accuracy: 0.7193\n",
      "Epoch 38/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6514 - accuracy: 0.7751 - val_loss: 0.8498 - val_accuracy: 0.6140\n",
      "Epoch 39/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7403 - accuracy: 0.7140 - val_loss: 0.7381 - val_accuracy: 0.7193\n",
      "Epoch 40/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6997 - accuracy: 0.7633 - val_loss: 0.9790 - val_accuracy: 0.5789\n",
      "Epoch 41/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7519 - accuracy: 0.7120 - val_loss: 0.9801 - val_accuracy: 0.5614\n",
      "Epoch 42/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7097 - accuracy: 0.7475 - val_loss: 0.6891 - val_accuracy: 0.7368\n",
      "Epoch 43/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7448 - accuracy: 0.7219 - val_loss: 0.6966 - val_accuracy: 0.7368\n",
      "Epoch 44/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6207 - accuracy: 0.7929 - val_loss: 0.6635 - val_accuracy: 0.7719\n",
      "Epoch 45/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5614 - accuracy: 0.8225 - val_loss: 0.6959 - val_accuracy: 0.7368\n",
      "Epoch 46/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6175 - accuracy: 0.7968 - val_loss: 0.6588 - val_accuracy: 0.7368\n",
      "Epoch 47/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6179 - accuracy: 0.7968 - val_loss: 0.7111 - val_accuracy: 0.7368\n",
      "Epoch 48/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6259 - accuracy: 0.7909 - val_loss: 0.6802 - val_accuracy: 0.7368\n",
      "Epoch 49/73\n",
      "16/16 [==============================] - 0s 14ms/step - loss: 0.6060 - accuracy: 0.7732 - val_loss: 0.7204 - val_accuracy: 0.7193\n",
      "Epoch 50/73\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5683 - accuracy: 0.8047 - val_loss: 0.7305 - val_accuracy: 0.7193\n",
      "Epoch 51/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.6130 - accuracy: 0.7613 - val_loss: 0.6881 - val_accuracy: 0.7544\n",
      "Epoch 52/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6471 - accuracy: 0.7535 - val_loss: 0.7918 - val_accuracy: 0.7018\n",
      "Epoch 53/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5837 - accuracy: 0.7968 - val_loss: 0.7230 - val_accuracy: 0.7544\n",
      "Epoch 54/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6417 - accuracy: 0.7791 - val_loss: 0.6995 - val_accuracy: 0.7368\n",
      "Epoch 55/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7491 - accuracy: 0.7179 - val_loss: 0.6344 - val_accuracy: 0.8070\n",
      "Epoch 56/73\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5769 - accuracy: 0.8067 - val_loss: 0.6492 - val_accuracy: 0.7193\n",
      "Epoch 57/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5081 - accuracy: 0.8245 - val_loss: 0.7110 - val_accuracy: 0.7193\n",
      "Epoch 58/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.7066 - accuracy: 0.7239 - val_loss: 0.8518 - val_accuracy: 0.6842\n",
      "Epoch 59/73\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.9543 - accuracy: 0.6568 - val_loss: 0.7191 - val_accuracy: 0.7018\n",
      "Epoch 60/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6575 - accuracy: 0.7416 - val_loss: 0.6907 - val_accuracy: 0.6667\n",
      "Epoch 61/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5400 - accuracy: 0.8146 - val_loss: 0.6489 - val_accuracy: 0.7193\n",
      "Epoch 62/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5216 - accuracy: 0.8146 - val_loss: 0.6448 - val_accuracy: 0.7544\n",
      "Epoch 63/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5586 - accuracy: 0.7949 - val_loss: 0.6364 - val_accuracy: 0.7895\n",
      "Epoch 64/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5290 - accuracy: 0.8343 - val_loss: 0.6625 - val_accuracy: 0.7368\n",
      "Epoch 65/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5195 - accuracy: 0.8245 - val_loss: 0.6362 - val_accuracy: 0.7544\n",
      "Epoch 66/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4714 - accuracy: 0.8383 - val_loss: 0.6063 - val_accuracy: 0.8070\n",
      "Epoch 67/73\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.7659 - accuracy: 0.7179 - val_loss: 0.8232 - val_accuracy: 0.6842\n",
      "Epoch 68/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6555 - accuracy: 0.7535 - val_loss: 0.7406 - val_accuracy: 0.7018\n",
      "Epoch 69/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6262 - accuracy: 0.7633 - val_loss: 0.6837 - val_accuracy: 0.7018\n",
      "Epoch 70/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.5326 - accuracy: 0.8146 - val_loss: 0.6038 - val_accuracy: 0.7368\n",
      "Epoch 71/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4531 - accuracy: 0.8540 - val_loss: 0.5435 - val_accuracy: 0.8246\n",
      "Epoch 72/73\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.4124 - accuracy: 0.8698 - val_loss: 0.5166 - val_accuracy: 0.8246\n",
      "Epoch 73/73\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4456 - accuracy: 0.8422 - val_loss: 0.6355 - val_accuracy: 0.7544\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_encoded, epochs=73, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step - loss: 0.7038 - accuracy: 0.7254\n",
      "Test Loss: 0.7038061618804932, Test Accuracy: 0.7253521084785461\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.80      0.73        10\n",
      "           1       0.60      0.53      0.56        17\n",
      "           2       0.62      0.50      0.56        10\n",
      "           3       0.75      1.00      0.86         6\n",
      "           4       1.00      0.10      0.18        10\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.54      0.78      0.64         9\n",
      "           7       0.90      0.64      0.75        14\n",
      "           8       0.71      0.83      0.77         6\n",
      "           9       0.71      1.00      0.83         5\n",
      "          10       1.00      0.75      0.86         4\n",
      "          11       0.30      1.00      0.46         3\n",
      "          12       0.80      0.44      0.57         9\n",
      "          13       0.75      1.00      0.86         3\n",
      "          14       1.00      0.83      0.91         6\n",
      "          15       0.82      1.00      0.90         9\n",
      "          16       0.86      1.00      0.92        12\n",
      "\n",
      "    accuracy                           0.73       142\n",
      "   macro avg       0.77      0.78      0.73       142\n",
      "weighted avg       0.77      0.73      0.71       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.2291 - accuracy: 0.9224 - val_loss: 0.6272 - val_accuracy: 0.7965\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2113 - accuracy: 0.9446 - val_loss: 0.5262 - val_accuracy: 0.8142\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2081 - accuracy: 0.9313 - val_loss: 0.4871 - val_accuracy: 0.8319\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1585 - accuracy: 0.9579 - val_loss: 0.4442 - val_accuracy: 0.8584\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1701 - accuracy: 0.9490 - val_loss: 0.5781 - val_accuracy: 0.7788\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1974 - accuracy: 0.9468 - val_loss: 0.5358 - val_accuracy: 0.8407\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1768 - accuracy: 0.9424 - val_loss: 0.4191 - val_accuracy: 0.8938\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1376 - accuracy: 0.9645 - val_loss: 0.4713 - val_accuracy: 0.8319\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1427 - accuracy: 0.9645 - val_loss: 0.4315 - val_accuracy: 0.8673\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1589 - accuracy: 0.9579 - val_loss: 0.4396 - val_accuracy: 0.8673\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1367 - accuracy: 0.9645 - val_loss: 0.4730 - val_accuracy: 0.8496\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1337 - accuracy: 0.9667 - val_loss: 0.4048 - val_accuracy: 0.8761\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1290 - accuracy: 0.9667 - val_loss: 0.4537 - val_accuracy: 0.8496\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1298 - accuracy: 0.9756 - val_loss: 0.4594 - val_accuracy: 0.8584\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1380 - accuracy: 0.9712 - val_loss: 0.5504 - val_accuracy: 0.8319\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1682 - accuracy: 0.9557 - val_loss: 0.5693 - val_accuracy: 0.8407\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1925 - accuracy: 0.9335 - val_loss: 0.4557 - val_accuracy: 0.8673\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1707 - accuracy: 0.9490 - val_loss: 0.6109 - val_accuracy: 0.8142\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2459 - accuracy: 0.9135 - val_loss: 0.5352 - val_accuracy: 0.8230\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1909 - accuracy: 0.9335 - val_loss: 0.4292 - val_accuracy: 0.8496\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1629 - accuracy: 0.9490 - val_loss: 0.5689 - val_accuracy: 0.8230\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1461 - accuracy: 0.9623 - val_loss: 0.4623 - val_accuracy: 0.8230\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1329 - accuracy: 0.9645 - val_loss: 0.4291 - val_accuracy: 0.8584\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1338 - accuracy: 0.9645 - val_loss: 0.4874 - val_accuracy: 0.8230\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1514 - accuracy: 0.9601 - val_loss: 0.5080 - val_accuracy: 0.8407\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1552 - accuracy: 0.9579 - val_loss: 0.4848 - val_accuracy: 0.8673\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1344 - accuracy: 0.9712 - val_loss: 0.4473 - val_accuracy: 0.8761\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1192 - accuracy: 0.9778 - val_loss: 0.4907 - val_accuracy: 0.8407\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1220 - accuracy: 0.9712 - val_loss: 0.5553 - val_accuracy: 0.8142\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1966 - accuracy: 0.9401 - val_loss: 0.6644 - val_accuracy: 0.7611\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4717 - accuracy: 0.8226 - val_loss: 1.5821 - val_accuracy: 0.4159\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1.0312 - accuracy: 0.6497 - val_loss: 1.0032 - val_accuracy: 0.6549\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5383 - accuracy: 0.7982 - val_loss: 0.8318 - val_accuracy: 0.7345\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5007 - accuracy: 0.8071 - val_loss: 0.7069 - val_accuracy: 0.7522\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4624 - accuracy: 0.8226 - val_loss: 0.9425 - val_accuracy: 0.6637\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5238 - accuracy: 0.7894 - val_loss: 0.6232 - val_accuracy: 0.7876\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2569 - accuracy: 0.8958 - val_loss: 0.7123 - val_accuracy: 0.7876\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2470 - accuracy: 0.9313 - val_loss: 0.5332 - val_accuracy: 0.8053\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3242 - accuracy: 0.8869 - val_loss: 0.6182 - val_accuracy: 0.8142\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2085 - accuracy: 0.9490 - val_loss: 0.8303 - val_accuracy: 0.7257\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3669 - accuracy: 0.8936 - val_loss: 0.5178 - val_accuracy: 0.8584\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2489 - accuracy: 0.9113 - val_loss: 0.5074 - val_accuracy: 0.8319\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1959 - accuracy: 0.9424 - val_loss: 0.5726 - val_accuracy: 0.8230\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1702 - accuracy: 0.9512 - val_loss: 0.4920 - val_accuracy: 0.8053\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1405 - accuracy: 0.9468 - val_loss: 0.5038 - val_accuracy: 0.8407\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1410 - accuracy: 0.9623 - val_loss: 0.6473 - val_accuracy: 0.8230\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1869 - accuracy: 0.9379 - val_loss: 0.4599 - val_accuracy: 0.8673\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1450 - accuracy: 0.9557 - val_loss: 0.5210 - val_accuracy: 0.8319\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1176 - accuracy: 0.9734 - val_loss: 0.4656 - val_accuracy: 0.8584\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1125 - accuracy: 0.9712 - val_loss: 0.4765 - val_accuracy: 0.8496\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_encoded, epochs=50, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.5597 - accuracy: 0.8239\n",
      "Test Loss: 0.5596828460693359, Test Accuracy: 0.8239436745643616\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.76      0.87        17\n",
      "           2       0.86      0.60      0.71        10\n",
      "           3       0.83      0.83      0.83         6\n",
      "           4       0.00      0.00      0.00        10\n",
      "           5       1.00      1.00      1.00         9\n",
      "           6       0.67      0.89      0.76         9\n",
      "           7       1.00      1.00      1.00        14\n",
      "           8       1.00      1.00      1.00         6\n",
      "           9       0.43      0.60      0.50         5\n",
      "          10       0.75      0.75      0.75         4\n",
      "          11       0.20      0.67      0.31         3\n",
      "          12       1.00      0.89      0.94         9\n",
      "          13       1.00      1.00      1.00         3\n",
      "          14       0.86      1.00      0.92         6\n",
      "          15       1.00      1.00      1.00         9\n",
      "          16       0.75      1.00      0.86        12\n",
      "\n",
      "    accuracy                           0.82       142\n",
      "   macro avg       0.78      0.82      0.79       142\n",
      "weighted avg       0.82      0.82      0.81       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as pickle file\n",
    "with open('lstmmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
