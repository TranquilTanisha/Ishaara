{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z62</th>\n",
       "      <th>X63</th>\n",
       "      <th>Y63</th>\n",
       "      <th>Z63</th>\n",
       "      <th>X64</th>\n",
       "      <th>Y64</th>\n",
       "      <th>Z64</th>\n",
       "      <th>X65</th>\n",
       "      <th>Y65</th>\n",
       "      <th>Z65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.290488</td>\n",
       "      <td>-75.818914</td>\n",
       "      <td>-3.229877</td>\n",
       "      <td>-0.166410</td>\n",
       "      <td>-77.328566</td>\n",
       "      <td>-3.061451</td>\n",
       "      <td>-0.098528</td>\n",
       "      <td>-82.594196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349284</td>\n",
       "      <td>0.610715</td>\n",
       "      <td>0.973082</td>\n",
       "      <td>-0.303378</td>\n",
       "      <td>0.405078</td>\n",
       "      <td>0.922159</td>\n",
       "      <td>-0.281479</td>\n",
       "      <td>0.559808</td>\n",
       "      <td>0.851646</td>\n",
       "      <td>-0.028510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346683</td>\n",
       "      <td>0.613981</td>\n",
       "      <td>0.946346</td>\n",
       "      <td>-0.395646</td>\n",
       "      <td>0.413621</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>-0.267950</td>\n",
       "      <td>0.558445</td>\n",
       "      <td>0.851427</td>\n",
       "      <td>-0.036818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820756</td>\n",
       "      <td>0.612683</td>\n",
       "      <td>0.950004</td>\n",
       "      <td>-0.379825</td>\n",
       "      <td>0.448817</td>\n",
       "      <td>0.581546</td>\n",
       "      <td>-0.748774</td>\n",
       "      <td>0.557367</td>\n",
       "      <td>0.867107</td>\n",
       "      <td>-0.040340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892051</td>\n",
       "      <td>0.607402</td>\n",
       "      <td>0.961182</td>\n",
       "      <td>-0.300694</td>\n",
       "      <td>0.439404</td>\n",
       "      <td>0.449596</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>0.556838</td>\n",
       "      <td>0.878659</td>\n",
       "      <td>-0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>-2.649841</td>\n",
       "      <td>-1.005892</td>\n",
       "      <td>0.190630</td>\n",
       "      <td>-5.362860</td>\n",
       "      <td>-1.022167</td>\n",
       "      <td>0.243538</td>\n",
       "      <td>-8.214614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829807</td>\n",
       "      <td>0.604378</td>\n",
       "      <td>0.950633</td>\n",
       "      <td>-0.247957</td>\n",
       "      <td>0.445261</td>\n",
       "      <td>0.344964</td>\n",
       "      <td>-0.765876</td>\n",
       "      <td>0.552638</td>\n",
       "      <td>0.877294</td>\n",
       "      <td>-0.045415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.255172</td>\n",
       "      <td>-14.686667</td>\n",
       "      <td>2.176972</td>\n",
       "      <td>-0.137746</td>\n",
       "      <td>-15.407824</td>\n",
       "      <td>3.206680</td>\n",
       "      <td>-0.088495</td>\n",
       "      <td>-16.454638</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189617</td>\n",
       "      <td>0.678464</td>\n",
       "      <td>0.430918</td>\n",
       "      <td>-0.998512</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>0.832112</td>\n",
       "      <td>-0.110380</td>\n",
       "      <td>0.559508</td>\n",
       "      <td>0.746648</td>\n",
       "      <td>0.006331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14136</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.247349</td>\n",
       "      <td>-18.125843</td>\n",
       "      <td>0.387121</td>\n",
       "      <td>-0.123823</td>\n",
       "      <td>-19.070908</td>\n",
       "      <td>-0.996675</td>\n",
       "      <td>-0.073921</td>\n",
       "      <td>-20.329048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235708</td>\n",
       "      <td>0.673772</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>-0.173289</td>\n",
       "      <td>0.301638</td>\n",
       "      <td>0.830727</td>\n",
       "      <td>-0.135007</td>\n",
       "      <td>0.558916</td>\n",
       "      <td>0.736754</td>\n",
       "      <td>0.016739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14137</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.252560</td>\n",
       "      <td>-28.690787</td>\n",
       "      <td>5.913587</td>\n",
       "      <td>-0.133411</td>\n",
       "      <td>-30.150542</td>\n",
       "      <td>5.626362</td>\n",
       "      <td>-0.083474</td>\n",
       "      <td>-32.168021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.643596</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>0.818583</td>\n",
       "      <td>-0.402061</td>\n",
       "      <td>0.293410</td>\n",
       "      <td>0.830436</td>\n",
       "      <td>-0.519518</td>\n",
       "      <td>0.550947</td>\n",
       "      <td>0.735413</td>\n",
       "      <td>0.027168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14138</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.263622</td>\n",
       "      <td>-44.649560</td>\n",
       "      <td>6.507309</td>\n",
       "      <td>-0.145083</td>\n",
       "      <td>-46.797549</td>\n",
       "      <td>6.096631</td>\n",
       "      <td>-0.096428</td>\n",
       "      <td>-49.825290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573453</td>\n",
       "      <td>0.627514</td>\n",
       "      <td>0.836671</td>\n",
       "      <td>-0.288298</td>\n",
       "      <td>0.281208</td>\n",
       "      <td>0.831862</td>\n",
       "      <td>-0.455801</td>\n",
       "      <td>0.543025</td>\n",
       "      <td>0.740520</td>\n",
       "      <td>0.026633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14139</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.281000</td>\n",
       "      <td>-67.864949</td>\n",
       "      <td>7.771813</td>\n",
       "      <td>-0.160708</td>\n",
       "      <td>-71.308601</td>\n",
       "      <td>7.211858</td>\n",
       "      <td>-0.112549</td>\n",
       "      <td>-76.094353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521230</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>-0.280609</td>\n",
       "      <td>0.273794</td>\n",
       "      <td>0.836519</td>\n",
       "      <td>-0.409446</td>\n",
       "      <td>0.534971</td>\n",
       "      <td>0.747671</td>\n",
       "      <td>0.022982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14140 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Frame       X00         Y00       Z00       X01         Y01  \\\n",
       "0      Boy      1 -0.290488  -75.818914 -3.229877 -0.166410  -77.328566   \n",
       "1      Boy      2 -2.630732  163.577346 -2.202535 -2.630732  163.577346   \n",
       "2      Boy      3 -2.649066   62.903229 -2.865200 -2.649066   62.903229   \n",
       "3      Boy      4 -2.641445   73.387394 -6.816939 -2.641445   73.387394   \n",
       "4      Boy      5  0.075973   -2.649841 -1.005892  0.190630   -5.362860   \n",
       "...    ...    ...       ...         ...       ...       ...         ...   \n",
       "14135  You     16 -0.255172  -14.686667  2.176972 -0.137746  -15.407824   \n",
       "14136  You     17 -0.247349  -18.125843  0.387121 -0.123823  -19.070908   \n",
       "14137  You     18 -0.252560  -28.690787  5.913587 -0.133411  -30.150542   \n",
       "14138  You     19 -0.263622  -44.649560  6.507309 -0.145083  -46.797549   \n",
       "14139  You     20 -0.281000  -67.864949  7.771813 -0.160708  -71.308601   \n",
       "\n",
       "            Z01       X02         Y02  ...       Z62       X63       Y63  \\\n",
       "0     -3.061451 -0.098528  -82.594196  ... -0.349284  0.610715  0.973082   \n",
       "1     -2.202535 -2.630732  163.577346  ... -0.346683  0.613981  0.946346   \n",
       "2     -2.865200 -2.649066   62.903229  ... -0.820756  0.612683  0.950004   \n",
       "3     -6.816939 -2.641445   73.387394  ... -0.892051  0.607402  0.961182   \n",
       "4     -1.022167  0.243538   -8.214614  ... -0.829807  0.604378  0.950633   \n",
       "...         ...       ...         ...  ...       ...       ...       ...   \n",
       "14135  3.206680 -0.088495  -16.454638  ... -0.189617  0.678464  0.430918   \n",
       "14136 -0.996675 -0.073921  -20.329048  ... -0.235708  0.673772  0.673639   \n",
       "14137  5.626362 -0.083474  -32.168021  ... -0.643596  0.653561  0.818583   \n",
       "14138  6.096631 -0.096428  -49.825290  ... -0.573453  0.627514  0.836671   \n",
       "14139  7.211858 -0.112549  -76.094353  ... -0.521230  0.608247  0.844743   \n",
       "\n",
       "            Z63       X64       Y64       Z64       X65       Y65       Z65  \n",
       "0     -0.303378  0.405078  0.922159 -0.281479  0.559808  0.851646 -0.028510  \n",
       "1     -0.395646  0.413621  0.845886 -0.267950  0.558445  0.851427 -0.036818  \n",
       "2     -0.379825  0.448817  0.581546 -0.748774  0.557367  0.867107 -0.040340  \n",
       "3     -0.300694  0.439404  0.449596 -0.823084  0.556838  0.878659 -0.022312  \n",
       "4     -0.247957  0.445261  0.344964 -0.765876  0.552638  0.877294 -0.045415  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "14135 -0.998512  0.307135  0.832112 -0.110380  0.559508  0.746648  0.006331  \n",
       "14136 -0.173289  0.301638  0.830727 -0.135007  0.558916  0.736754  0.016739  \n",
       "14137 -0.402061  0.293410  0.830436 -0.519518  0.550947  0.735413  0.027168  \n",
       "14138 -0.288298  0.281208  0.831862 -0.455801  0.543025  0.740520  0.026633  \n",
       "14139 -0.280609  0.273794  0.836519 -0.409446  0.534971  0.747671  0.022982  \n",
       "\n",
       "[14140 rows x 200 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load each .npy file and convert the list of dictionaries to a DataFrame\n",
    "dataframes = []\n",
    "for file in os.listdir('dataset'):\n",
    "    if file.endswith('.npy'):\n",
    "        data = np.load('dataset/' + file, allow_pickle=True)\n",
    "        dataframes.append(pd.DataFrame.from_records(data))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = merged_df['Frame'].max()\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z62</th>\n",
       "      <th>X63</th>\n",
       "      <th>Y63</th>\n",
       "      <th>Z63</th>\n",
       "      <th>X64</th>\n",
       "      <th>Y64</th>\n",
       "      <th>Z64</th>\n",
       "      <th>X65</th>\n",
       "      <th>Y65</th>\n",
       "      <th>Z65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.290488</td>\n",
       "      <td>-75.818914</td>\n",
       "      <td>-3.229877</td>\n",
       "      <td>-0.166410</td>\n",
       "      <td>-77.328566</td>\n",
       "      <td>-3.061451</td>\n",
       "      <td>-0.098528</td>\n",
       "      <td>-82.594196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349284</td>\n",
       "      <td>0.610715</td>\n",
       "      <td>0.973082</td>\n",
       "      <td>-0.303378</td>\n",
       "      <td>0.405078</td>\n",
       "      <td>0.922159</td>\n",
       "      <td>-0.281479</td>\n",
       "      <td>0.559808</td>\n",
       "      <td>0.851646</td>\n",
       "      <td>-0.028510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>-2.202535</td>\n",
       "      <td>-2.630732</td>\n",
       "      <td>163.577346</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346683</td>\n",
       "      <td>0.613981</td>\n",
       "      <td>0.946346</td>\n",
       "      <td>-0.395646</td>\n",
       "      <td>0.413621</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>-0.267950</td>\n",
       "      <td>0.558445</td>\n",
       "      <td>0.851427</td>\n",
       "      <td>-0.036818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>-2.865200</td>\n",
       "      <td>-2.649066</td>\n",
       "      <td>62.903229</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820756</td>\n",
       "      <td>0.612683</td>\n",
       "      <td>0.950004</td>\n",
       "      <td>-0.379825</td>\n",
       "      <td>0.448817</td>\n",
       "      <td>0.581546</td>\n",
       "      <td>-0.748774</td>\n",
       "      <td>0.557367</td>\n",
       "      <td>0.867107</td>\n",
       "      <td>-0.040340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>-6.816939</td>\n",
       "      <td>-2.641445</td>\n",
       "      <td>73.387394</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.892051</td>\n",
       "      <td>0.607402</td>\n",
       "      <td>0.961182</td>\n",
       "      <td>-0.300694</td>\n",
       "      <td>0.439404</td>\n",
       "      <td>0.449596</td>\n",
       "      <td>-0.823084</td>\n",
       "      <td>0.556838</td>\n",
       "      <td>0.878659</td>\n",
       "      <td>-0.022312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>-2.649841</td>\n",
       "      <td>-1.005892</td>\n",
       "      <td>0.190630</td>\n",
       "      <td>-5.362860</td>\n",
       "      <td>-1.022167</td>\n",
       "      <td>0.243538</td>\n",
       "      <td>-8.214614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829807</td>\n",
       "      <td>0.604378</td>\n",
       "      <td>0.950633</td>\n",
       "      <td>-0.247957</td>\n",
       "      <td>0.445261</td>\n",
       "      <td>0.344964</td>\n",
       "      <td>-0.765876</td>\n",
       "      <td>0.552638</td>\n",
       "      <td>0.877294</td>\n",
       "      <td>-0.045415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14135</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.255172</td>\n",
       "      <td>-14.686667</td>\n",
       "      <td>2.176972</td>\n",
       "      <td>-0.137746</td>\n",
       "      <td>-15.407824</td>\n",
       "      <td>3.206680</td>\n",
       "      <td>-0.088495</td>\n",
       "      <td>-16.454638</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189617</td>\n",
       "      <td>0.678464</td>\n",
       "      <td>0.430918</td>\n",
       "      <td>-0.998512</td>\n",
       "      <td>0.307135</td>\n",
       "      <td>0.832112</td>\n",
       "      <td>-0.110380</td>\n",
       "      <td>0.559508</td>\n",
       "      <td>0.746648</td>\n",
       "      <td>0.006331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14136</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.247349</td>\n",
       "      <td>-18.125843</td>\n",
       "      <td>0.387121</td>\n",
       "      <td>-0.123823</td>\n",
       "      <td>-19.070908</td>\n",
       "      <td>-0.996675</td>\n",
       "      <td>-0.073921</td>\n",
       "      <td>-20.329048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235708</td>\n",
       "      <td>0.673772</td>\n",
       "      <td>0.673639</td>\n",
       "      <td>-0.173289</td>\n",
       "      <td>0.301638</td>\n",
       "      <td>0.830727</td>\n",
       "      <td>-0.135007</td>\n",
       "      <td>0.558916</td>\n",
       "      <td>0.736754</td>\n",
       "      <td>0.016739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14137</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.252560</td>\n",
       "      <td>-28.690787</td>\n",
       "      <td>5.913587</td>\n",
       "      <td>-0.133411</td>\n",
       "      <td>-30.150542</td>\n",
       "      <td>5.626362</td>\n",
       "      <td>-0.083474</td>\n",
       "      <td>-32.168021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.643596</td>\n",
       "      <td>0.653561</td>\n",
       "      <td>0.818583</td>\n",
       "      <td>-0.402061</td>\n",
       "      <td>0.293410</td>\n",
       "      <td>0.830436</td>\n",
       "      <td>-0.519518</td>\n",
       "      <td>0.550947</td>\n",
       "      <td>0.735413</td>\n",
       "      <td>0.027168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14138</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.263622</td>\n",
       "      <td>-44.649560</td>\n",
       "      <td>6.507309</td>\n",
       "      <td>-0.145083</td>\n",
       "      <td>-46.797549</td>\n",
       "      <td>6.096631</td>\n",
       "      <td>-0.096428</td>\n",
       "      <td>-49.825290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573453</td>\n",
       "      <td>0.627514</td>\n",
       "      <td>0.836671</td>\n",
       "      <td>-0.288298</td>\n",
       "      <td>0.281208</td>\n",
       "      <td>0.831862</td>\n",
       "      <td>-0.455801</td>\n",
       "      <td>0.543025</td>\n",
       "      <td>0.740520</td>\n",
       "      <td>0.026633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14139</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.281000</td>\n",
       "      <td>-67.864949</td>\n",
       "      <td>7.771813</td>\n",
       "      <td>-0.160708</td>\n",
       "      <td>-71.308601</td>\n",
       "      <td>7.211858</td>\n",
       "      <td>-0.112549</td>\n",
       "      <td>-76.094353</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.521230</td>\n",
       "      <td>0.608247</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>-0.280609</td>\n",
       "      <td>0.273794</td>\n",
       "      <td>0.836519</td>\n",
       "      <td>-0.409446</td>\n",
       "      <td>0.534971</td>\n",
       "      <td>0.747671</td>\n",
       "      <td>0.022982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14140 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  Frame       X00         Y00       Z00       X01         Y01  \\\n",
       "0      Boy      1 -0.290488  -75.818914 -3.229877 -0.166410  -77.328566   \n",
       "1      Boy      2 -2.630732  163.577346 -2.202535 -2.630732  163.577346   \n",
       "2      Boy      3 -2.649066   62.903229 -2.865200 -2.649066   62.903229   \n",
       "3      Boy      4 -2.641445   73.387394 -6.816939 -2.641445   73.387394   \n",
       "4      Boy      5  0.075973   -2.649841 -1.005892  0.190630   -5.362860   \n",
       "...    ...    ...       ...         ...       ...       ...         ...   \n",
       "14135  You     16 -0.255172  -14.686667  2.176972 -0.137746  -15.407824   \n",
       "14136  You     17 -0.247349  -18.125843  0.387121 -0.123823  -19.070908   \n",
       "14137  You     18 -0.252560  -28.690787  5.913587 -0.133411  -30.150542   \n",
       "14138  You     19 -0.263622  -44.649560  6.507309 -0.145083  -46.797549   \n",
       "14139  You     20 -0.281000  -67.864949  7.771813 -0.160708  -71.308601   \n",
       "\n",
       "            Z01       X02         Y02  ...       Z62       X63       Y63  \\\n",
       "0     -3.061451 -0.098528  -82.594196  ... -0.349284  0.610715  0.973082   \n",
       "1     -2.202535 -2.630732  163.577346  ... -0.346683  0.613981  0.946346   \n",
       "2     -2.865200 -2.649066   62.903229  ... -0.820756  0.612683  0.950004   \n",
       "3     -6.816939 -2.641445   73.387394  ... -0.892051  0.607402  0.961182   \n",
       "4     -1.022167  0.243538   -8.214614  ... -0.829807  0.604378  0.950633   \n",
       "...         ...       ...         ...  ...       ...       ...       ...   \n",
       "14135  3.206680 -0.088495  -16.454638  ... -0.189617  0.678464  0.430918   \n",
       "14136 -0.996675 -0.073921  -20.329048  ... -0.235708  0.673772  0.673639   \n",
       "14137  5.626362 -0.083474  -32.168021  ... -0.643596  0.653561  0.818583   \n",
       "14138  6.096631 -0.096428  -49.825290  ... -0.573453  0.627514  0.836671   \n",
       "14139  7.211858 -0.112549  -76.094353  ... -0.521230  0.608247  0.844743   \n",
       "\n",
       "            Z63       X64       Y64       Z64       X65       Y65       Z65  \n",
       "0     -0.303378  0.405078  0.922159 -0.281479  0.559808  0.851646 -0.028510  \n",
       "1     -0.395646  0.413621  0.845886 -0.267950  0.558445  0.851427 -0.036818  \n",
       "2     -0.379825  0.448817  0.581546 -0.748774  0.557367  0.867107 -0.040340  \n",
       "3     -0.300694  0.439404  0.449596 -0.823084  0.556838  0.878659 -0.022312  \n",
       "4     -0.247957  0.445261  0.344964 -0.765876  0.552638  0.877294 -0.045415  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "14135 -0.998512  0.307135  0.832112 -0.110380  0.559508  0.746648  0.006331  \n",
       "14136 -0.173289  0.301638  0.830727 -0.135007  0.558916  0.736754  0.016739  \n",
       "14137 -0.402061  0.293410  0.830436 -0.519518  0.550947  0.735413  0.027168  \n",
       "14138 -0.288298  0.281208  0.831862 -0.455801  0.543025  0.740520  0.026633  \n",
       "14139 -0.280609  0.273794  0.836519 -0.409446  0.534971  0.747671  0.022982  \n",
       "\n",
       "[14140 rows x 200 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(group):\n",
    "    # Calculate the number of padding rows needed\n",
    "    padding_rows = max_sequence_length - len(group)\n",
    "    \n",
    "    # Create a DataFrame with padding rows filled with NaN (or any other padding value)\n",
    "    padding_df = pd.DataFrame({\n",
    "        'Word': [group['Word'].iloc[0]] * padding_rows,\n",
    "        'Frame': np.arange(len(group) + 1, max_sequence_length + 1),\n",
    "    })\n",
    "    \n",
    "    # Concatenate the original group with the padding DataFrame\n",
    "    return pd.concat([group, padding_df], ignore_index=True)\n",
    "\n",
    "# Group the DataFrame by 'Word' and apply the padding function\n",
    "padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n",
    "padded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = padded_df.drop('Word', axis=1)\n",
    "target = padded_df['Word']\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "features_scaled = scaler.fit_transform(data)\n",
    "\n",
    "import pickle\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952978</td>\n",
       "      <td>0.699372</td>\n",
       "      <td>0.454278</td>\n",
       "      <td>0.953955</td>\n",
       "      <td>0.713726</td>\n",
       "      <td>0.441782</td>\n",
       "      <td>0.954489</td>\n",
       "      <td>0.730702</td>\n",
       "      <td>0.435487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609107</td>\n",
       "      <td>0.485042</td>\n",
       "      <td>0.933159</td>\n",
       "      <td>0.577927</td>\n",
       "      <td>0.529268</td>\n",
       "      <td>0.860612</td>\n",
       "      <td>0.616883</td>\n",
       "      <td>0.373888</td>\n",
       "      <td>0.509505</td>\n",
       "      <td>0.514733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.934556</td>\n",
       "      <td>0.699717</td>\n",
       "      <td>0.454491</td>\n",
       "      <td>0.934556</td>\n",
       "      <td>0.714057</td>\n",
       "      <td>0.441959</td>\n",
       "      <td>0.934556</td>\n",
       "      <td>0.731019</td>\n",
       "      <td>0.435650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609978</td>\n",
       "      <td>0.488517</td>\n",
       "      <td>0.903483</td>\n",
       "      <td>0.534605</td>\n",
       "      <td>0.538125</td>\n",
       "      <td>0.776289</td>\n",
       "      <td>0.621676</td>\n",
       "      <td>0.368709</td>\n",
       "      <td>0.509020</td>\n",
       "      <td>0.489929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.934412</td>\n",
       "      <td>0.699572</td>\n",
       "      <td>0.454354</td>\n",
       "      <td>0.934412</td>\n",
       "      <td>0.713919</td>\n",
       "      <td>0.441823</td>\n",
       "      <td>0.934412</td>\n",
       "      <td>0.730890</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451326</td>\n",
       "      <td>0.487136</td>\n",
       "      <td>0.907543</td>\n",
       "      <td>0.542033</td>\n",
       "      <td>0.574620</td>\n",
       "      <td>0.484052</td>\n",
       "      <td>0.451347</td>\n",
       "      <td>0.364614</td>\n",
       "      <td>0.543859</td>\n",
       "      <td>0.479415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.934472</td>\n",
       "      <td>0.699587</td>\n",
       "      <td>0.453535</td>\n",
       "      <td>0.934472</td>\n",
       "      <td>0.713933</td>\n",
       "      <td>0.441012</td>\n",
       "      <td>0.934472</td>\n",
       "      <td>0.730903</td>\n",
       "      <td>0.434701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427466</td>\n",
       "      <td>0.481516</td>\n",
       "      <td>0.919951</td>\n",
       "      <td>0.579187</td>\n",
       "      <td>0.564860</td>\n",
       "      <td>0.338176</td>\n",
       "      <td>0.425023</td>\n",
       "      <td>0.362603</td>\n",
       "      <td>0.569525</td>\n",
       "      <td>0.533235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.955863</td>\n",
       "      <td>0.699477</td>\n",
       "      <td>0.454739</td>\n",
       "      <td>0.956765</td>\n",
       "      <td>0.713825</td>\n",
       "      <td>0.442201</td>\n",
       "      <td>0.957182</td>\n",
       "      <td>0.730798</td>\n",
       "      <td>0.435892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448296</td>\n",
       "      <td>0.478297</td>\n",
       "      <td>0.908241</td>\n",
       "      <td>0.603949</td>\n",
       "      <td>0.570932</td>\n",
       "      <td>0.222502</td>\n",
       "      <td>0.445288</td>\n",
       "      <td>0.346646</td>\n",
       "      <td>0.566492</td>\n",
       "      <td>0.464263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.953256</td>\n",
       "      <td>0.699460</td>\n",
       "      <td>0.455399</td>\n",
       "      <td>0.954180</td>\n",
       "      <td>0.713811</td>\n",
       "      <td>0.443068</td>\n",
       "      <td>0.954568</td>\n",
       "      <td>0.730787</td>\n",
       "      <td>0.436809</td>\n",
       "      <td>...</td>\n",
       "      <td>0.662541</td>\n",
       "      <td>0.557140</td>\n",
       "      <td>0.331366</td>\n",
       "      <td>0.251541</td>\n",
       "      <td>0.427714</td>\n",
       "      <td>0.761062</td>\n",
       "      <td>0.677494</td>\n",
       "      <td>0.372749</td>\n",
       "      <td>0.276217</td>\n",
       "      <td>0.618748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.953318</td>\n",
       "      <td>0.699455</td>\n",
       "      <td>0.455028</td>\n",
       "      <td>0.954290</td>\n",
       "      <td>0.713806</td>\n",
       "      <td>0.442206</td>\n",
       "      <td>0.954683</td>\n",
       "      <td>0.730782</td>\n",
       "      <td>0.435825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647116</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>0.600782</td>\n",
       "      <td>0.639008</td>\n",
       "      <td>0.422015</td>\n",
       "      <td>0.759530</td>\n",
       "      <td>0.668770</td>\n",
       "      <td>0.370499</td>\n",
       "      <td>0.254234</td>\n",
       "      <td>0.649819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.953277</td>\n",
       "      <td>0.699440</td>\n",
       "      <td>0.456174</td>\n",
       "      <td>0.954215</td>\n",
       "      <td>0.713791</td>\n",
       "      <td>0.443565</td>\n",
       "      <td>0.954608</td>\n",
       "      <td>0.730767</td>\n",
       "      <td>0.437247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510613</td>\n",
       "      <td>0.530639</td>\n",
       "      <td>0.761668</td>\n",
       "      <td>0.531593</td>\n",
       "      <td>0.413484</td>\n",
       "      <td>0.759208</td>\n",
       "      <td>0.532559</td>\n",
       "      <td>0.340221</td>\n",
       "      <td>0.251254</td>\n",
       "      <td>0.680956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.953190</td>\n",
       "      <td>0.699417</td>\n",
       "      <td>0.456297</td>\n",
       "      <td>0.954123</td>\n",
       "      <td>0.713768</td>\n",
       "      <td>0.443661</td>\n",
       "      <td>0.954506</td>\n",
       "      <td>0.730744</td>\n",
       "      <td>0.437335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534087</td>\n",
       "      <td>0.502919</td>\n",
       "      <td>0.781745</td>\n",
       "      <td>0.585008</td>\n",
       "      <td>0.400832</td>\n",
       "      <td>0.760785</td>\n",
       "      <td>0.555131</td>\n",
       "      <td>0.310119</td>\n",
       "      <td>0.262601</td>\n",
       "      <td>0.679359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953053</td>\n",
       "      <td>0.699383</td>\n",
       "      <td>0.456559</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>0.713735</td>\n",
       "      <td>0.443890</td>\n",
       "      <td>0.954379</td>\n",
       "      <td>0.730710</td>\n",
       "      <td>0.437559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551564</td>\n",
       "      <td>0.482415</td>\n",
       "      <td>0.790705</td>\n",
       "      <td>0.588618</td>\n",
       "      <td>0.393145</td>\n",
       "      <td>0.765934</td>\n",
       "      <td>0.571551</td>\n",
       "      <td>0.279520</td>\n",
       "      <td>0.278490</td>\n",
       "      <td>0.668459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9440 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.000000  0.952978  0.699372  0.454278  0.953955  0.713726  0.441782   \n",
       "1   0.052632  0.934556  0.699717  0.454491  0.934556  0.714057  0.441959   \n",
       "2   0.105263  0.934412  0.699572  0.454354  0.934412  0.713919  0.441823   \n",
       "3   0.157895  0.934472  0.699587  0.453535  0.934472  0.713933  0.441012   \n",
       "4   0.210526  0.955863  0.699477  0.454739  0.956765  0.713825  0.442201   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "35  0.789474  0.953256  0.699460  0.455399  0.954180  0.713811  0.443068   \n",
       "36  0.842105  0.953318  0.699455  0.455028  0.954290  0.713806  0.442206   \n",
       "37  0.894737  0.953277  0.699440  0.456174  0.954215  0.713791  0.443565   \n",
       "38  0.947368  0.953190  0.699417  0.456297  0.954123  0.713768  0.443661   \n",
       "39  1.000000  0.953053  0.699383  0.456559  0.954000  0.713735  0.443890   \n",
       "\n",
       "         7         8         9    ...       189       190       191       192  \\\n",
       "0   0.954489  0.730702  0.435487  ...  0.609107  0.485042  0.933159  0.577927   \n",
       "1   0.934556  0.731019  0.435650  ...  0.609978  0.488517  0.903483  0.534605   \n",
       "2   0.934412  0.730890  0.435514  ...  0.451326  0.487136  0.907543  0.542033   \n",
       "3   0.934472  0.730903  0.434701  ...  0.427466  0.481516  0.919951  0.579187   \n",
       "4   0.957182  0.730798  0.435892  ...  0.448296  0.478297  0.908241  0.603949   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "35  0.954568  0.730787  0.436809  ...  0.662541  0.557140  0.331366  0.251541   \n",
       "36  0.954683  0.730782  0.435825  ...  0.647116  0.552147  0.600782  0.639008   \n",
       "37  0.954608  0.730767  0.437247  ...  0.510613  0.530639  0.761668  0.531593   \n",
       "38  0.954506  0.730744  0.437335  ...  0.534087  0.502919  0.781745  0.585008   \n",
       "39  0.954379  0.730710  0.437559  ...  0.551564  0.482415  0.790705  0.588618   \n",
       "\n",
       "         193       194       195       196       197       198  \n",
       "0   0.529268  0.860612  0.616883  0.373888  0.509505  0.514733  \n",
       "1   0.538125  0.776289  0.621676  0.368709  0.509020  0.489929  \n",
       "2   0.574620  0.484052  0.451347  0.364614  0.543859  0.479415  \n",
       "3   0.564860  0.338176  0.425023  0.362603  0.569525  0.533235  \n",
       "4   0.570932  0.222502  0.445288  0.346646  0.566492  0.464263  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "35  0.427714  0.761062  0.677494  0.372749  0.276217  0.618748  \n",
       "36  0.422015  0.759530  0.668770  0.370499  0.254234  0.649819  \n",
       "37  0.413484  0.759208  0.532559  0.340221  0.251254  0.680956  \n",
       "38  0.400832  0.760785  0.555131  0.310119  0.262601  0.679359  \n",
       "39  0.393145  0.765934  0.571551  0.279520  0.278490  0.668459  \n",
       "\n",
       "[9440 rows x 199 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the interval\n",
    "interval = 20\n",
    "\n",
    "# Initialize lists to hold the training and testing sets\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "# Iterate over features_scaled with a step of 20\n",
    "for i in range(0, len(padded_df) - interval, 3*interval):\n",
    "    # Slice the dataset for the current interval\n",
    "    X_train = pd.DataFrame(features_scaled[i:i+2*interval])\n",
    "    y_train = pd.DataFrame(target[i:i+2*interval])\n",
    "\n",
    "    X_test = pd.DataFrame(features_scaled[i+2*interval:i+3*interval])\n",
    "    y_test = pd.DataFrame(target[i+2*interval:i+3*interval])\n",
    "\n",
    "    # Append the sliced DataFrames to the lists\n",
    "    X_train_list.append(X_train)\n",
    "    y_train_list.append(y_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_test_list.append(y_test)\n",
    "\n",
    "# Convert the lists to DataFrames\n",
    "X_train_df = pd.concat(X_train_list)\n",
    "y_train_df = pd.concat(y_train_list)\n",
    "X_test_df = pd.concat(X_test_list)\n",
    "y_test_df = pd.concat(y_test_list)\n",
    "\n",
    "X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mello\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "clf.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Boy       0.88      0.89      0.89       340\n",
      "         Can       0.92      0.82      0.87       300\n",
      "         Eat       0.87      0.96      0.91       260\n",
      "        Fine       0.88      0.88      0.88       294\n",
      "        Girl       0.86      0.89      0.87       226\n",
      "        Help       0.99      0.99      0.99       300\n",
      "         How       0.86      0.91      0.89       300\n",
      "      Hungry       0.88      0.95      0.91       288\n",
      "           I       0.98      0.96      0.97       232\n",
      "      Mother       0.83      0.85      0.84       260\n",
      "     Namaste       0.93      0.85      0.89       200\n",
      "        Name       0.95      0.88      0.91       286\n",
      "     Parents       0.98      0.98      0.98       294\n",
      "      Sister       0.98      0.96      0.97       100\n",
      "       Sleep       0.91      0.84      0.87       280\n",
      "        This       0.87      0.88      0.88       320\n",
      "         You       0.98      1.00      0.99       420\n",
      "\n",
      "    accuracy                           0.91      4700\n",
      "   macro avg       0.92      0.91      0.91      4700\n",
      "weighted avg       0.91      0.91      0.91      4700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test_df, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rfmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of time steps\n",
    "time_steps = 20\n",
    "\n",
    "# Prepare the data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(features_scaled) - time_steps, time_steps):\n",
    "    X.append(features_scaled[i:i + time_steps])\n",
    "    y.append(target[i])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape X to fit the LSTM input shape: [samples, time steps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'y_train' is your target variable with categorical values\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax')) # num_classes is the number of unique words you're predicting\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.2160 - accuracy: 0.9313 - val_loss: 0.4930 - val_accuracy: 0.8230\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1887 - accuracy: 0.9490 - val_loss: 0.5273 - val_accuracy: 0.8496\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1599 - accuracy: 0.9601 - val_loss: 0.5128 - val_accuracy: 0.8407\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1779 - accuracy: 0.9557 - val_loss: 1.0857 - val_accuracy: 0.6637\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4920 - accuracy: 0.8426 - val_loss: 0.5843 - val_accuracy: 0.7965\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.7582 - accuracy: 0.7672 - val_loss: 0.8771 - val_accuracy: 0.6283\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5273 - accuracy: 0.7805 - val_loss: 0.7532 - val_accuracy: 0.7345\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4061 - accuracy: 0.8537 - val_loss: 0.7870 - val_accuracy: 0.7522\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3063 - accuracy: 0.9069 - val_loss: 0.6570 - val_accuracy: 0.7257\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3388 - accuracy: 0.8670 - val_loss: 0.6435 - val_accuracy: 0.7699\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2827 - accuracy: 0.8914 - val_loss: 0.8539 - val_accuracy: 0.6549\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.8584 - accuracy: 0.6874 - val_loss: 0.9874 - val_accuracy: 0.6637\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4612 - accuracy: 0.8071 - val_loss: 0.7298 - val_accuracy: 0.7257\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3396 - accuracy: 0.8847 - val_loss: 0.6154 - val_accuracy: 0.7876\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2541 - accuracy: 0.9135 - val_loss: 0.5483 - val_accuracy: 0.7965\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2300 - accuracy: 0.9202 - val_loss: 0.5469 - val_accuracy: 0.8319\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2012 - accuracy: 0.9379 - val_loss: 0.7296 - val_accuracy: 0.7345\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5301 - accuracy: 0.8182 - val_loss: 0.8470 - val_accuracy: 0.7522\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4269 - accuracy: 0.8426 - val_loss: 0.6205 - val_accuracy: 0.7876\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2667 - accuracy: 0.8980 - val_loss: 0.6397 - val_accuracy: 0.7965\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2642 - accuracy: 0.9135 - val_loss: 0.5356 - val_accuracy: 0.7876\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1905 - accuracy: 0.9446 - val_loss: 0.5335 - val_accuracy: 0.8142\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1758 - accuracy: 0.9468 - val_loss: 0.5034 - val_accuracy: 0.8407\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1650 - accuracy: 0.9534 - val_loss: 0.5972 - val_accuracy: 0.8142\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1552 - accuracy: 0.9512 - val_loss: 0.5442 - val_accuracy: 0.8230\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1499 - accuracy: 0.9579 - val_loss: 0.5650 - val_accuracy: 0.8319\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1696 - accuracy: 0.9557 - val_loss: 0.7340 - val_accuracy: 0.7699\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.2314 - accuracy: 0.9290 - val_loss: 0.5858 - val_accuracy: 0.7876\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1746 - accuracy: 0.9468 - val_loss: 0.6181 - val_accuracy: 0.8053\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1670 - accuracy: 0.9579 - val_loss: 0.6176 - val_accuracy: 0.7611\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1938 - accuracy: 0.9357 - val_loss: 0.5326 - val_accuracy: 0.8230\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1750 - accuracy: 0.9490 - val_loss: 0.6999 - val_accuracy: 0.7522\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2419 - accuracy: 0.9246 - val_loss: 0.5575 - val_accuracy: 0.7965\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1639 - accuracy: 0.9534 - val_loss: 0.4924 - val_accuracy: 0.8230\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1446 - accuracy: 0.9601 - val_loss: 0.4869 - val_accuracy: 0.8496\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2190 - accuracy: 0.9180 - val_loss: 0.6907 - val_accuracy: 0.7522\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5541 - accuracy: 0.8071 - val_loss: 0.5853 - val_accuracy: 0.7611\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2732 - accuracy: 0.9024 - val_loss: 0.6550 - val_accuracy: 0.7611\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.2317 - accuracy: 0.9335 - val_loss: 0.6789 - val_accuracy: 0.7257\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2268 - accuracy: 0.9224 - val_loss: 0.6621 - val_accuracy: 0.7522\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1922 - accuracy: 0.9401 - val_loss: 0.6221 - val_accuracy: 0.7522\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2275 - accuracy: 0.9424 - val_loss: 0.7979 - val_accuracy: 0.7080\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2611 - accuracy: 0.9047 - val_loss: 0.6040 - val_accuracy: 0.7876\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1461 - accuracy: 0.9667 - val_loss: 0.5150 - val_accuracy: 0.8142\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1215 - accuracy: 0.9823 - val_loss: 0.5450 - val_accuracy: 0.7965\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1207 - accuracy: 0.9712 - val_loss: 0.5164 - val_accuracy: 0.8142\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2776 - accuracy: 0.9135 - val_loss: 1.0102 - val_accuracy: 0.6814\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3836 - accuracy: 0.8670 - val_loss: 0.5062 - val_accuracy: 0.8319\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3120 - accuracy: 0.8936 - val_loss: 0.6250 - val_accuracy: 0.7611\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2293 - accuracy: 0.9290 - val_loss: 0.7415 - val_accuracy: 0.7434\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2026 - accuracy: 0.9401 - val_loss: 0.5052 - val_accuracy: 0.8230\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2179 - accuracy: 0.9357 - val_loss: 0.6651 - val_accuracy: 0.7345\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2154 - accuracy: 0.9290 - val_loss: 0.7295 - val_accuracy: 0.7699\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1705 - accuracy: 0.9579 - val_loss: 0.5739 - val_accuracy: 0.8053\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1492 - accuracy: 0.9623 - val_loss: 0.5396 - val_accuracy: 0.8053\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1216 - accuracy: 0.9734 - val_loss: 0.5054 - val_accuracy: 0.8319\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1250 - accuracy: 0.9712 - val_loss: 0.5688 - val_accuracy: 0.7965\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1045 - accuracy: 0.9756 - val_loss: 0.5184 - val_accuracy: 0.8230\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1022 - accuracy: 0.9800 - val_loss: 0.9613 - val_accuracy: 0.6903\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6137 - accuracy: 0.7938 - val_loss: 0.9926 - val_accuracy: 0.6726\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.7826 - accuracy: 0.7228 - val_loss: 0.8583 - val_accuracy: 0.6460\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.8271 - val_loss: 0.7141 - val_accuracy: 0.7345\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2559 - accuracy: 0.9246 - val_loss: 0.5730 - val_accuracy: 0.7611\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2096 - accuracy: 0.9357 - val_loss: 0.6285 - val_accuracy: 0.8142\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1503 - accuracy: 0.9623 - val_loss: 0.6215 - val_accuracy: 0.8230\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1609 - accuracy: 0.9490 - val_loss: 0.6339 - val_accuracy: 0.7522\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1301 - accuracy: 0.9645 - val_loss: 0.5456 - val_accuracy: 0.8496\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1148 - accuracy: 0.9734 - val_loss: 0.5520 - val_accuracy: 0.7611\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1032 - accuracy: 0.9867 - val_loss: 0.5351 - val_accuracy: 0.8053\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1667 - accuracy: 0.9557 - val_loss: 0.5856 - val_accuracy: 0.8319\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2064 - accuracy: 0.9180 - val_loss: 0.7226 - val_accuracy: 0.7522\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2669 - accuracy: 0.9047 - val_loss: 0.6428 - val_accuracy: 0.7788\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1455 - accuracy: 0.9601 - val_loss: 0.4844 - val_accuracy: 0.8142\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1689 - accuracy: 0.9490 - val_loss: 0.6802 - val_accuracy: 0.7611\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1385 - accuracy: 0.9601 - val_loss: 0.5274 - val_accuracy: 0.7965\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1248 - accuracy: 0.9645 - val_loss: 0.6111 - val_accuracy: 0.7965\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0991 - accuracy: 0.9845 - val_loss: 0.5858 - val_accuracy: 0.8053\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1382 - accuracy: 0.9512 - val_loss: 1.0610 - val_accuracy: 0.6814\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.8913 - accuracy: 0.7472 - val_loss: 1.1063 - val_accuracy: 0.6283\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5447 - accuracy: 0.7716 - val_loss: 0.7702 - val_accuracy: 0.7345\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4376 - accuracy: 0.8514 - val_loss: 1.1355 - val_accuracy: 0.6372\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.7268 - accuracy: 0.7384 - val_loss: 0.8786 - val_accuracy: 0.7080\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5112 - accuracy: 0.8182 - val_loss: 0.7933 - val_accuracy: 0.7168\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.8041 - accuracy: 0.7539 - val_loss: 0.9609 - val_accuracy: 0.6814\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3763 - accuracy: 0.8692 - val_loss: 0.6590 - val_accuracy: 0.7876\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2065 - accuracy: 0.9180 - val_loss: 0.6405 - val_accuracy: 0.7699\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1860 - accuracy: 0.9424 - val_loss: 0.7591 - val_accuracy: 0.7345\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2121 - accuracy: 0.9313 - val_loss: 0.5696 - val_accuracy: 0.8053\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.4287 - accuracy: 0.8470 - val_loss: 1.6203 - val_accuracy: 0.5133\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6651 - accuracy: 0.7517 - val_loss: 1.0029 - val_accuracy: 0.6903\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 1.1454 - accuracy: 0.6475 - val_loss: 0.9722 - val_accuracy: 0.6726\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.5312 - accuracy: 0.8071 - val_loss: 0.6859 - val_accuracy: 0.7699\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2808 - accuracy: 0.9157 - val_loss: 0.5360 - val_accuracy: 0.8053\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2111 - accuracy: 0.9357 - val_loss: 0.5230 - val_accuracy: 0.8496\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1452 - accuracy: 0.9690 - val_loss: 0.4906 - val_accuracy: 0.8142\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1218 - accuracy: 0.9756 - val_loss: 0.6481 - val_accuracy: 0.8230\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1221 - accuracy: 0.9734 - val_loss: 0.6314 - val_accuracy: 0.7434\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1243 - accuracy: 0.9778 - val_loss: 0.5191 - val_accuracy: 0.7965\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1070 - accuracy: 0.9756 - val_loss: 0.5169 - val_accuracy: 0.7788\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1003 - accuracy: 0.9800 - val_loss: 0.5344 - val_accuracy: 0.8230\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_encoded, epochs=100, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.8310\n",
      "Test Loss: 0.492537260055542, Test Accuracy: 0.8309859037399292\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as pickle file\n",
    "with open('lstmmodel.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
