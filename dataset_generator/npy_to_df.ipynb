{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y24</th>\n",
       "      <th>Z24</th>\n",
       "      <th>X25</th>\n",
       "      <th>Y25</th>\n",
       "      <th>Z25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y26</th>\n",
       "      <th>Z26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530313</td>\n",
       "      <td>0.585678</td>\n",
       "      <td>-0.365724</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.130718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.441575</td>\n",
       "      <td>-10.572632</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>-0.500577</td>\n",
       "      <td>-12.161836</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>-0.622653</td>\n",
       "      <td>-11.623393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531883</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>-0.381889</td>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.241074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.754394</td>\n",
       "      <td>-7.421648</td>\n",
       "      <td>0.392176</td>\n",
       "      <td>-0.842538</td>\n",
       "      <td>-8.545950</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>-0.967975</td>\n",
       "      <td>-8.040165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.574829</td>\n",
       "      <td>-0.470731</td>\n",
       "      <td>0.376408</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.216554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.678109</td>\n",
       "      <td>-8.186996</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>-0.814776</td>\n",
       "      <td>-9.373079</td>\n",
       "      <td>0.455979</td>\n",
       "      <td>-0.910327</td>\n",
       "      <td>-8.907041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>-0.516997</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.629331</td>\n",
       "      <td>0.273374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.797789</td>\n",
       "      <td>-10.197847</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>-0.955783</td>\n",
       "      <td>-11.699729</td>\n",
       "      <td>0.451075</td>\n",
       "      <td>-1.066465</td>\n",
       "      <td>-11.020497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>0.384628</td>\n",
       "      <td>0.589920</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112924</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587871</td>\n",
       "      <td>9.669083</td>\n",
       "      <td>1.162361</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>11.412089</td>\n",
       "      <td>1.129761</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>10.942098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574938</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>-0.300268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112925</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>21.491173</td>\n",
       "      <td>1.587999</td>\n",
       "      <td>0.362523</td>\n",
       "      <td>25.172693</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>24.418745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578150</td>\n",
       "      <td>0.539364</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>0.398031</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>-0.181356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112926</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>30.405003</td>\n",
       "      <td>1.653764</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>35.272096</td>\n",
       "      <td>1.652273</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>34.866246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580092</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.398705</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>-0.237497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112927</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182174</td>\n",
       "      <td>30.964133</td>\n",
       "      <td>1.847807</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>36.075314</td>\n",
       "      <td>1.835613</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>35.360101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580864</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.538843</td>\n",
       "      <td>-0.243124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112928</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>0.140643</td>\n",
       "      <td>43.708800</td>\n",
       "      <td>2.127749</td>\n",
       "      <td>0.198996</td>\n",
       "      <td>50.094664</td>\n",
       "      <td>2.107137</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>49.969990</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585219</td>\n",
       "      <td>0.542360</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>0.397739</td>\n",
       "      <td>0.534382</td>\n",
       "      <td>-0.228005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112929 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frame       X00        Y00       Z00       X01        Y01  \\\n",
       "0       Boy      1 -3.320268  12.714561  0.223574 -3.320268  12.714561   \n",
       "1       Boy      2 -0.441575 -10.572632  0.370819 -0.500577 -12.161836   \n",
       "2       Boy      3 -0.754394  -7.421648  0.392176 -0.842538  -8.545950   \n",
       "3       Boy      4 -0.678109  -8.186996  0.441856 -0.814776  -9.373079   \n",
       "4       Boy      5 -0.797789 -10.197847  0.429872 -0.955783 -11.699729   \n",
       "...     ...    ...       ...        ...       ...       ...        ...   \n",
       "112924  You     16  0.587871   9.669083  1.162361  0.726044  11.412089   \n",
       "112925  You     17  0.337344  21.491173  1.587999  0.362523  25.172693   \n",
       "112926  You     18  0.215001  30.405003  1.653764  0.260286  35.272096   \n",
       "112927  You     19  0.182174  30.964133  1.847807  0.224303  36.075314   \n",
       "112928  You     20  0.140643  43.708800  2.127749  0.198996  50.094664   \n",
       "\n",
       "             Z01       X02        Y02  ...  Z23  X24  Y24  Z24       X25  \\\n",
       "0       0.223574 -3.320268  12.714561  ...  1.0  0.0  0.0  0.0  0.530313   \n",
       "1       0.379360 -0.622653 -11.623393  ...  1.0  0.0  0.0  0.0  0.531883   \n",
       "2       0.401212 -0.967975  -8.040165  ...  1.0  0.0  0.0  0.0  0.534591   \n",
       "3       0.455979 -0.910327  -8.907041  ...  1.0  0.0  0.0  0.0  0.484453   \n",
       "4       0.451075 -1.066465 -11.020497  ...  1.0  0.0  0.0  0.0  0.474471   \n",
       "...          ...       ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "112924  1.129761  0.655170  10.942098  ...  1.0  0.0  0.0  0.0  0.574938   \n",
       "112925  1.563665  0.306954  24.418745  ...  1.0  0.0  0.0  0.0  0.578150   \n",
       "112926  1.652273  0.141393  34.866246  ...  1.0  0.0  0.0  0.0  0.580092   \n",
       "112927  1.835613  0.095749  35.360101  ...  1.0  0.0  0.0  0.0  0.580864   \n",
       "112928  2.107137  0.070886  49.969990  ...  1.0  0.0  0.0  0.0  0.585219   \n",
       "\n",
       "             Y25       Z25       X26       Y26       Z26  \n",
       "0       0.585678 -0.365724  0.393299  0.607514  0.130718  \n",
       "1       0.588693 -0.381889  0.385701  0.626965  0.241074  \n",
       "2       0.574829 -0.470731  0.376408  0.645435  0.216554  \n",
       "3       0.453059 -0.516997  0.365139  0.629331  0.273374  \n",
       "4       0.342270 -0.453968  0.384628  0.589920  0.266067  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "112924  0.538055  0.070751  0.395394  0.452951 -0.300268  \n",
       "112925  0.539364  0.051295  0.398031  0.515749 -0.181356  \n",
       "112926  0.541042  0.027703  0.398705  0.534306 -0.237497  \n",
       "112927  0.542060  0.006164  0.398723  0.538843 -0.243124  \n",
       "112928  0.542360 -0.016540  0.397739  0.534382 -0.228005  \n",
       "\n",
       "[112929 rows x 83 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load each .npy file and convert the list of dictionaries to a DataFrame\n",
    "dataframes = []\n",
    "for file in os.listdir('dataset_backup'):\n",
    "    if file.endswith('.npy'):\n",
    "        data = np.load('dataset_backup/' + file, allow_pickle=True)\n",
    "        dataframes.append(pd.DataFrame.from_records(data))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = merged_df['Frame'].max()\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df['Frame'] != 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_df = merged_df[merged_df['Frame'] != 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y24</th>\n",
       "      <th>Z24</th>\n",
       "      <th>X25</th>\n",
       "      <th>Y25</th>\n",
       "      <th>Z25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y26</th>\n",
       "      <th>Z26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530313</td>\n",
       "      <td>0.585678</td>\n",
       "      <td>-0.365724</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.130718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.441575</td>\n",
       "      <td>-10.572632</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>-0.500577</td>\n",
       "      <td>-12.161836</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>-0.622653</td>\n",
       "      <td>-11.623393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531883</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>-0.381889</td>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.241074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.754394</td>\n",
       "      <td>-7.421648</td>\n",
       "      <td>0.392176</td>\n",
       "      <td>-0.842538</td>\n",
       "      <td>-8.545950</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>-0.967975</td>\n",
       "      <td>-8.040165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.574829</td>\n",
       "      <td>-0.470731</td>\n",
       "      <td>0.376408</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.216554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.678109</td>\n",
       "      <td>-8.186996</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>-0.814776</td>\n",
       "      <td>-9.373079</td>\n",
       "      <td>0.455979</td>\n",
       "      <td>-0.910327</td>\n",
       "      <td>-8.907041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>-0.516997</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.629331</td>\n",
       "      <td>0.273374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.797789</td>\n",
       "      <td>-10.197847</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>-0.955783</td>\n",
       "      <td>-11.699729</td>\n",
       "      <td>0.451075</td>\n",
       "      <td>-1.066465</td>\n",
       "      <td>-11.020497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>0.384628</td>\n",
       "      <td>0.589920</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112923</th>\n",
       "      <td>You</td>\n",
       "      <td>15</td>\n",
       "      <td>0.822056</td>\n",
       "      <td>5.677708</td>\n",
       "      <td>0.710343</td>\n",
       "      <td>1.048670</td>\n",
       "      <td>7.904536</td>\n",
       "      <td>0.604539</td>\n",
       "      <td>0.998043</td>\n",
       "      <td>5.860801</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.570543</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.108942</td>\n",
       "      <td>0.402942</td>\n",
       "      <td>0.371251</td>\n",
       "      <td>-0.373206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112924</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587871</td>\n",
       "      <td>9.669083</td>\n",
       "      <td>1.162361</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>11.412089</td>\n",
       "      <td>1.129761</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>10.942098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574938</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>-0.300268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112925</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>21.491173</td>\n",
       "      <td>1.587999</td>\n",
       "      <td>0.362523</td>\n",
       "      <td>25.172693</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>24.418745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578150</td>\n",
       "      <td>0.539364</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>0.398031</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>-0.181356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112926</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>30.405003</td>\n",
       "      <td>1.653764</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>35.272096</td>\n",
       "      <td>1.652273</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>34.866246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580092</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.398705</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>-0.237497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112927</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182174</td>\n",
       "      <td>30.964133</td>\n",
       "      <td>1.847807</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>36.075314</td>\n",
       "      <td>1.835613</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>35.360101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580864</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.538843</td>\n",
       "      <td>-0.243124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109440 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frame       X00        Y00       Z00       X01        Y01  \\\n",
       "0       Boy      1 -3.320268  12.714561  0.223574 -3.320268  12.714561   \n",
       "1       Boy      2 -0.441575 -10.572632  0.370819 -0.500577 -12.161836   \n",
       "2       Boy      3 -0.754394  -7.421648  0.392176 -0.842538  -8.545950   \n",
       "3       Boy      4 -0.678109  -8.186996  0.441856 -0.814776  -9.373079   \n",
       "4       Boy      5 -0.797789 -10.197847  0.429872 -0.955783 -11.699729   \n",
       "...     ...    ...       ...        ...       ...       ...        ...   \n",
       "112923  You     15  0.822056   5.677708  0.710343  1.048670   7.904536   \n",
       "112924  You     16  0.587871   9.669083  1.162361  0.726044  11.412089   \n",
       "112925  You     17  0.337344  21.491173  1.587999  0.362523  25.172693   \n",
       "112926  You     18  0.215001  30.405003  1.653764  0.260286  35.272096   \n",
       "112927  You     19  0.182174  30.964133  1.847807  0.224303  36.075314   \n",
       "\n",
       "             Z01       X02        Y02  ...  Z23  X24  Y24  Z24       X25  \\\n",
       "0       0.223574 -3.320268  12.714561  ...  1.0  0.0  0.0  0.0  0.530313   \n",
       "1       0.379360 -0.622653 -11.623393  ...  1.0  0.0  0.0  0.0  0.531883   \n",
       "2       0.401212 -0.967975  -8.040165  ...  1.0  0.0  0.0  0.0  0.534591   \n",
       "3       0.455979 -0.910327  -8.907041  ...  1.0  0.0  0.0  0.0  0.484453   \n",
       "4       0.451075 -1.066465 -11.020497  ...  1.0  0.0  0.0  0.0  0.474471   \n",
       "...          ...       ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "112923  0.604539  0.998043   5.860801  ...  1.0  0.0  0.0  0.0  0.570543   \n",
       "112924  1.129761  0.655170  10.942098  ...  1.0  0.0  0.0  0.0  0.574938   \n",
       "112925  1.563665  0.306954  24.418745  ...  1.0  0.0  0.0  0.0  0.578150   \n",
       "112926  1.652273  0.141393  34.866246  ...  1.0  0.0  0.0  0.0  0.580092   \n",
       "112927  1.835613  0.095749  35.360101  ...  1.0  0.0  0.0  0.0  0.580864   \n",
       "\n",
       "             Y25       Z25       X26       Y26       Z26  \n",
       "0       0.585678 -0.365724  0.393299  0.607514  0.130718  \n",
       "1       0.588693 -0.381889  0.385701  0.626965  0.241074  \n",
       "2       0.574829 -0.470731  0.376408  0.645435  0.216554  \n",
       "3       0.453059 -0.516997  0.365139  0.629331  0.273374  \n",
       "4       0.342270 -0.453968  0.384628  0.589920  0.266067  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "112923  0.518519  0.108942  0.402942  0.371251 -0.373206  \n",
       "112924  0.538055  0.070751  0.395394  0.452951 -0.300268  \n",
       "112925  0.539364  0.051295  0.398031  0.515749 -0.181356  \n",
       "112926  0.541042  0.027703  0.398705  0.534306 -0.237497  \n",
       "112927  0.542060  0.006164  0.398723  0.538843 -0.243124  \n",
       "\n",
       "[109440 rows x 83 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y24</th>\n",
       "      <th>Z24</th>\n",
       "      <th>X25</th>\n",
       "      <th>Y25</th>\n",
       "      <th>Z25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y26</th>\n",
       "      <th>Z26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530313</td>\n",
       "      <td>0.585678</td>\n",
       "      <td>-0.365724</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.130718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.441575</td>\n",
       "      <td>-10.572632</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>-0.500577</td>\n",
       "      <td>-12.161836</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>-0.622653</td>\n",
       "      <td>-11.623393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531883</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>-0.381889</td>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.241074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.754394</td>\n",
       "      <td>-7.421648</td>\n",
       "      <td>0.392176</td>\n",
       "      <td>-0.842538</td>\n",
       "      <td>-8.545950</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>-0.967975</td>\n",
       "      <td>-8.040165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.574829</td>\n",
       "      <td>-0.470731</td>\n",
       "      <td>0.376408</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.216554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.678109</td>\n",
       "      <td>-8.186996</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>-0.814776</td>\n",
       "      <td>-9.373079</td>\n",
       "      <td>0.455979</td>\n",
       "      <td>-0.910327</td>\n",
       "      <td>-8.907041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>-0.516997</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.629331</td>\n",
       "      <td>0.273374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.797789</td>\n",
       "      <td>-10.197847</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>-0.955783</td>\n",
       "      <td>-11.699729</td>\n",
       "      <td>0.451075</td>\n",
       "      <td>-1.066465</td>\n",
       "      <td>-11.020497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>0.384628</td>\n",
       "      <td>0.589920</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109435</th>\n",
       "      <td>You</td>\n",
       "      <td>15</td>\n",
       "      <td>0.822056</td>\n",
       "      <td>5.677708</td>\n",
       "      <td>0.710343</td>\n",
       "      <td>1.048670</td>\n",
       "      <td>7.904536</td>\n",
       "      <td>0.604539</td>\n",
       "      <td>0.998043</td>\n",
       "      <td>5.860801</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.570543</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.108942</td>\n",
       "      <td>0.402942</td>\n",
       "      <td>0.371251</td>\n",
       "      <td>-0.373206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109436</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587871</td>\n",
       "      <td>9.669083</td>\n",
       "      <td>1.162361</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>11.412089</td>\n",
       "      <td>1.129761</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>10.942098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574938</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>-0.300268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109437</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>21.491173</td>\n",
       "      <td>1.587999</td>\n",
       "      <td>0.362523</td>\n",
       "      <td>25.172693</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>24.418745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578150</td>\n",
       "      <td>0.539364</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>0.398031</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>-0.181356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109438</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>30.405003</td>\n",
       "      <td>1.653764</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>35.272096</td>\n",
       "      <td>1.652273</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>34.866246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580092</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.398705</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>-0.237497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109439</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182174</td>\n",
       "      <td>30.964133</td>\n",
       "      <td>1.847807</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>36.075314</td>\n",
       "      <td>1.835613</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>35.360101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580864</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.538843</td>\n",
       "      <td>-0.243124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109440 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frame       X00        Y00       Z00       X01        Y01  \\\n",
       "0       Boy      1 -3.320268  12.714561  0.223574 -3.320268  12.714561   \n",
       "1       Boy      2 -0.441575 -10.572632  0.370819 -0.500577 -12.161836   \n",
       "2       Boy      3 -0.754394  -7.421648  0.392176 -0.842538  -8.545950   \n",
       "3       Boy      4 -0.678109  -8.186996  0.441856 -0.814776  -9.373079   \n",
       "4       Boy      5 -0.797789 -10.197847  0.429872 -0.955783 -11.699729   \n",
       "...     ...    ...       ...        ...       ...       ...        ...   \n",
       "109435  You     15  0.822056   5.677708  0.710343  1.048670   7.904536   \n",
       "109436  You     16  0.587871   9.669083  1.162361  0.726044  11.412089   \n",
       "109437  You     17  0.337344  21.491173  1.587999  0.362523  25.172693   \n",
       "109438  You     18  0.215001  30.405003  1.653764  0.260286  35.272096   \n",
       "109439  You     19  0.182174  30.964133  1.847807  0.224303  36.075314   \n",
       "\n",
       "             Z01       X02        Y02  ...  Z23  X24  Y24  Z24       X25  \\\n",
       "0       0.223574 -3.320268  12.714561  ...  1.0  0.0  0.0  0.0  0.530313   \n",
       "1       0.379360 -0.622653 -11.623393  ...  1.0  0.0  0.0  0.0  0.531883   \n",
       "2       0.401212 -0.967975  -8.040165  ...  1.0  0.0  0.0  0.0  0.534591   \n",
       "3       0.455979 -0.910327  -8.907041  ...  1.0  0.0  0.0  0.0  0.484453   \n",
       "4       0.451075 -1.066465 -11.020497  ...  1.0  0.0  0.0  0.0  0.474471   \n",
       "...          ...       ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "109435  0.604539  0.998043   5.860801  ...  1.0  0.0  0.0  0.0  0.570543   \n",
       "109436  1.129761  0.655170  10.942098  ...  1.0  0.0  0.0  0.0  0.574938   \n",
       "109437  1.563665  0.306954  24.418745  ...  1.0  0.0  0.0  0.0  0.578150   \n",
       "109438  1.652273  0.141393  34.866246  ...  1.0  0.0  0.0  0.0  0.580092   \n",
       "109439  1.835613  0.095749  35.360101  ...  1.0  0.0  0.0  0.0  0.580864   \n",
       "\n",
       "             Y25       Z25       X26       Y26       Z26  \n",
       "0       0.585678 -0.365724  0.393299  0.607514  0.130718  \n",
       "1       0.588693 -0.381889  0.385701  0.626965  0.241074  \n",
       "2       0.574829 -0.470731  0.376408  0.645435  0.216554  \n",
       "3       0.453059 -0.516997  0.365139  0.629331  0.273374  \n",
       "4       0.342270 -0.453968  0.384628  0.589920  0.266067  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "109435  0.518519  0.108942  0.402942  0.371251 -0.373206  \n",
       "109436  0.538055  0.070751  0.395394  0.452951 -0.300268  \n",
       "109437  0.539364  0.051295  0.398031  0.515749 -0.181356  \n",
       "109438  0.541042  0.027703  0.398705  0.534306 -0.237497  \n",
       "109439  0.542060  0.006164  0.398723  0.538843 -0.243124  \n",
       "\n",
       "[109440 rows x 83 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(group):\n",
    "    # Calculate the number of padding rows needed\n",
    "    padding_rows = max_sequence_length - len(group)\n",
    "    \n",
    "    # Create a DataFrame with padding rows filled with NaN (or any other padding value)\n",
    "    padding_df = pd.DataFrame({\n",
    "        'Word': [group['Word'].iloc[0]] * padding_rows,\n",
    "        'Frame': np.arange(len(group) + 1, max_sequence_length + 1),\n",
    "    })\n",
    "    \n",
    "    # Concatenate the original group with the padding DataFrame\n",
    "    return pd.concat([group, padding_df], ignore_index=True)\n",
    "\n",
    "# Group the DataFrame by 'Word' and apply the padding function\n",
    "padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n",
    "padded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = padded_df.drop('Word', axis=1)\n",
    "target = padded_df['Word']\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "features_scaled = scaler.fit_transform(data)\n",
    "\n",
    "import pickle\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the interval\n",
    "# interval = 20\n",
    "\n",
    "# # Initialize lists to hold the training and testing sets\n",
    "# X_train_list = []\n",
    "# y_train_list = []\n",
    "# X_test_list = []\n",
    "# y_test_list = []\n",
    "\n",
    "# # Iterate over features_scaled with a step of 20\n",
    "# for i in range(0, len(padded_df) - interval, 3*interval):\n",
    "#     # Slice the dataset for the current interval\n",
    "#     X_train = pd.DataFrame(features_scaled[i:i+2*interval])\n",
    "#     y_train = pd.DataFrame(target[i:i+2*interval])\n",
    "\n",
    "#     X_test = pd.DataFrame(features_scaled[i+2*interval:i+3*interval])\n",
    "#     y_test = pd.DataFrame(target[i+2*interval:i+3*interval])\n",
    "\n",
    "#     # Append the sliced DataFrames to the lists\n",
    "#     X_train_list.append(X_train)\n",
    "#     y_train_list.append(y_train)\n",
    "#     X_test_list.append(X_test)\n",
    "#     y_test_list.append(y_test)\n",
    "\n",
    "# # Convert the lists to DataFrames\n",
    "# X_train_df = pd.concat(X_train_list)\n",
    "# y_train_df = pd.concat(y_train_list)\n",
    "# X_test_df = pd.concat(X_test_list)\n",
    "# y_test_df = pd.concat(y_test_list)\n",
    "\n",
    "# X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Initialize the model\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = clf.predict(X_test_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(classification_report(y_test_df, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('rfmodel.pkl', 'wb') as file:\n",
    "#     pickle.dump(clf, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5760"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of time steps\n",
    "time_steps = 19\n",
    "\n",
    "# Prepare the data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(features_scaled) - time_steps + 1, time_steps):\n",
    "    X.append(features_scaled[i:i + time_steps])\n",
    "    y.append(target[i])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape X to fit the LSTM input shape: [samples, time steps, features]\n",
    "X = X.reshape((5760, time_steps, 82))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'y_train' is your target variable with categorical values\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">26,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,717</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_11 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m26,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_12 (\u001b[38;5;33mLSTM\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │         \u001b[38;5;34m5,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m)             │         \u001b[38;5;34m1,717\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,617</span> (209.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m53,617\u001b[0m (209.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,617</span> (209.44 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m53,617\u001b[0m (209.44 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Assuming X_train has shape (num_samples, timesteps, features)\n",
    "timesteps = X_train.shape[1]\n",
    "features = X_train.shape[2]\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(features, return_sequences=True, input_shape=(timesteps, features)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(features, return_sequences=False))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - accuracy: 0.0673 - loss: 2.7664 - val_accuracy: 0.0998 - val_loss: 2.7212\n",
      "Epoch 2/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0784 - loss: 2.7400 - val_accuracy: 0.0803 - val_loss: 2.7198\n",
      "Epoch 3/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0787 - loss: 2.7282 - val_accuracy: 0.0857 - val_loss: 2.7184\n",
      "Epoch 4/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0709 - loss: 2.7377 - val_accuracy: 0.1030 - val_loss: 2.7196\n",
      "Epoch 5/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0882 - loss: 2.7228 - val_accuracy: 0.0705 - val_loss: 2.7169\n",
      "Epoch 6/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0825 - loss: 2.7164 - val_accuracy: 0.1171 - val_loss: 2.7014\n",
      "Epoch 7/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0751 - loss: 2.7156 - val_accuracy: 0.0857 - val_loss: 2.7186\n",
      "Epoch 8/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.0840 - loss: 2.7214 - val_accuracy: 0.0868 - val_loss: 2.6471\n",
      "Epoch 9/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1056 - loss: 2.6672 - val_accuracy: 0.0954 - val_loss: 2.6144\n",
      "Epoch 10/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0999 - loss: 2.6294 - val_accuracy: 0.1139 - val_loss: 2.6264\n",
      "Epoch 11/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.1085 - loss: 2.6388 - val_accuracy: 0.1453 - val_loss: 2.4530\n",
      "Epoch 12/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1237 - loss: 2.4926 - val_accuracy: 0.1551 - val_loss: 2.4020\n",
      "Epoch 13/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1533 - loss: 2.4497 - val_accuracy: 0.1887 - val_loss: 2.3173\n",
      "Epoch 14/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1699 - loss: 2.3393 - val_accuracy: 0.1844 - val_loss: 2.4938\n",
      "Epoch 15/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.1809 - loss: 2.3083 - val_accuracy: 0.2419 - val_loss: 2.1386\n",
      "Epoch 16/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2110 - loss: 2.1730 - val_accuracy: 0.1985 - val_loss: 2.2663\n",
      "Epoch 17/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2179 - loss: 2.1766 - val_accuracy: 0.2690 - val_loss: 2.0850\n",
      "Epoch 18/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.2502 - loss: 2.0887 - val_accuracy: 0.3059 - val_loss: 1.9631\n",
      "Epoch 19/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3135 - loss: 1.9801 - val_accuracy: 0.3341 - val_loss: 1.9406\n",
      "Epoch 20/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3402 - loss: 1.9036 - val_accuracy: 0.3514 - val_loss: 1.8708\n",
      "Epoch 21/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3173 - loss: 1.9182 - val_accuracy: 0.4403 - val_loss: 1.6312\n",
      "Epoch 22/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3839 - loss: 1.7370 - val_accuracy: 0.3373 - val_loss: 1.7971\n",
      "Epoch 23/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3840 - loss: 1.7076 - val_accuracy: 0.4718 - val_loss: 1.5225\n",
      "Epoch 24/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4281 - loss: 1.6582 - val_accuracy: 0.4382 - val_loss: 1.5807\n",
      "Epoch 25/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4139 - loss: 1.6439 - val_accuracy: 0.4946 - val_loss: 1.4651\n",
      "Epoch 26/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4590 - loss: 1.5416 - val_accuracy: 0.4946 - val_loss: 1.4744\n",
      "Epoch 27/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4560 - loss: 1.5534 - val_accuracy: 0.5586 - val_loss: 1.3315\n",
      "Epoch 28/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4717 - loss: 1.4938 - val_accuracy: 0.4371 - val_loss: 1.6008\n",
      "Epoch 29/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4661 - loss: 1.5199 - val_accuracy: 0.5108 - val_loss: 1.3572\n",
      "Epoch 30/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4928 - loss: 1.3899 - val_accuracy: 0.5412 - val_loss: 1.2945\n",
      "Epoch 31/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5143 - loss: 1.3337 - val_accuracy: 0.5152 - val_loss: 1.3390\n",
      "Epoch 32/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5081 - loss: 1.3985 - val_accuracy: 0.3861 - val_loss: 1.7009\n",
      "Epoch 33/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4846 - loss: 1.4431 - val_accuracy: 0.5672 - val_loss: 1.2361\n",
      "Epoch 34/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5085 - loss: 1.3592 - val_accuracy: 0.5456 - val_loss: 1.3177\n",
      "Epoch 35/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5018 - loss: 1.3600 - val_accuracy: 0.5607 - val_loss: 1.2183\n",
      "Epoch 36/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5534 - loss: 1.2457 - val_accuracy: 0.5933 - val_loss: 1.1678\n",
      "Epoch 37/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5174 - loss: 1.2953 - val_accuracy: 0.5813 - val_loss: 1.1479\n",
      "Epoch 38/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5353 - loss: 1.2876 - val_accuracy: 0.5911 - val_loss: 1.1487\n",
      "Epoch 39/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5261 - loss: 1.3103 - val_accuracy: 0.5933 - val_loss: 1.1421\n",
      "Epoch 40/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5744 - loss: 1.1756 - val_accuracy: 0.6139 - val_loss: 1.0656\n",
      "Epoch 41/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5602 - loss: 1.2196 - val_accuracy: 0.5954 - val_loss: 1.1009\n",
      "Epoch 42/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5463 - loss: 1.2041 - val_accuracy: 0.6106 - val_loss: 1.1015\n",
      "Epoch 43/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5406 - loss: 1.2416 - val_accuracy: 0.6280 - val_loss: 1.0684\n",
      "Epoch 44/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5460 - loss: 1.2429 - val_accuracy: 0.4946 - val_loss: 1.4684\n",
      "Epoch 45/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5553 - loss: 1.2927 - val_accuracy: 0.6388 - val_loss: 1.0344\n",
      "Epoch 46/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.5971 - loss: 1.1151 - val_accuracy: 0.5521 - val_loss: 1.2306\n",
      "Epoch 47/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5620 - loss: 1.1914 - val_accuracy: 0.6410 - val_loss: 1.0299\n",
      "Epoch 48/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5883 - loss: 1.1090 - val_accuracy: 0.6475 - val_loss: 1.0153\n",
      "Epoch 49/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6083 - loss: 1.0811 - val_accuracy: 0.6584 - val_loss: 0.9766\n",
      "Epoch 50/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6132 - loss: 1.0830 - val_accuracy: 0.6388 - val_loss: 1.0352\n",
      "Epoch 51/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6155 - loss: 1.0912 - val_accuracy: 0.6508 - val_loss: 0.9737\n",
      "Epoch 52/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6191 - loss: 1.0433 - val_accuracy: 0.6551 - val_loss: 1.0026\n",
      "Epoch 53/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6081 - loss: 1.0643 - val_accuracy: 0.6670 - val_loss: 0.9605\n",
      "Epoch 54/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6049 - loss: 1.0987 - val_accuracy: 0.6681 - val_loss: 0.9526\n",
      "Epoch 55/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6144 - loss: 1.0408 - val_accuracy: 0.6518 - val_loss: 0.9908\n",
      "Epoch 56/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6223 - loss: 1.0434 - val_accuracy: 0.6453 - val_loss: 0.9900\n",
      "Epoch 57/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6289 - loss: 1.0133 - val_accuracy: 0.6670 - val_loss: 0.9444\n",
      "Epoch 58/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6219 - loss: 1.0334 - val_accuracy: 0.6768 - val_loss: 0.9189\n",
      "Epoch 59/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6326 - loss: 0.9697 - val_accuracy: 0.6410 - val_loss: 0.9986\n",
      "Epoch 60/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6356 - loss: 1.0223 - val_accuracy: 0.6876 - val_loss: 0.9057\n",
      "Epoch 61/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6219 - loss: 0.9966 - val_accuracy: 0.6638 - val_loss: 0.9399\n",
      "Epoch 62/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6343 - loss: 1.0004 - val_accuracy: 0.5607 - val_loss: 1.2027\n",
      "Epoch 63/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6027 - loss: 1.1136 - val_accuracy: 0.6670 - val_loss: 0.9565\n",
      "Epoch 64/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6237 - loss: 1.0402 - val_accuracy: 0.6551 - val_loss: 0.9791\n",
      "Epoch 65/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6470 - loss: 0.9458 - val_accuracy: 0.6681 - val_loss: 0.9217\n",
      "Epoch 66/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6388 - loss: 0.9804 - val_accuracy: 0.6725 - val_loss: 0.9149\n",
      "Epoch 67/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6587 - loss: 0.9349 - val_accuracy: 0.6670 - val_loss: 0.9481\n",
      "Epoch 68/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6467 - loss: 0.9617 - val_accuracy: 0.6735 - val_loss: 0.8816\n",
      "Epoch 69/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6692 - loss: 0.8831 - val_accuracy: 0.6551 - val_loss: 0.9968\n",
      "Epoch 70/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6400 - loss: 0.9646 - val_accuracy: 0.6692 - val_loss: 0.8710\n",
      "Epoch 71/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6562 - loss: 0.9368 - val_accuracy: 0.7007 - val_loss: 0.8561\n",
      "Epoch 72/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6882 - loss: 0.8626 - val_accuracy: 0.6844 - val_loss: 0.8921\n",
      "Epoch 73/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6517 - loss: 0.9626 - val_accuracy: 0.6334 - val_loss: 1.0082\n",
      "Epoch 74/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6360 - loss: 0.9628 - val_accuracy: 0.7039 - val_loss: 0.8692\n",
      "Epoch 75/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6752 - loss: 0.8813 - val_accuracy: 0.6909 - val_loss: 0.9034\n",
      "Epoch 76/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6702 - loss: 0.8877 - val_accuracy: 0.7126 - val_loss: 0.8621\n",
      "Epoch 77/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6973 - loss: 0.8509 - val_accuracy: 0.7278 - val_loss: 0.7814\n",
      "Epoch 78/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6770 - loss: 0.8610 - val_accuracy: 0.6659 - val_loss: 0.9288\n",
      "Epoch 79/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6724 - loss: 0.8789 - val_accuracy: 0.7050 - val_loss: 0.8677\n",
      "Epoch 80/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6725 - loss: 0.8652 - val_accuracy: 0.7093 - val_loss: 0.8134\n",
      "Epoch 81/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7046 - loss: 0.8035 - val_accuracy: 0.7180 - val_loss: 0.8118\n",
      "Epoch 82/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6914 - loss: 0.8271 - val_accuracy: 0.7082 - val_loss: 0.8128\n",
      "Epoch 83/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6863 - loss: 0.8878 - val_accuracy: 0.6952 - val_loss: 0.8173\n",
      "Epoch 84/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7031 - loss: 0.8006 - val_accuracy: 0.7256 - val_loss: 0.7905\n",
      "Epoch 85/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6874 - loss: 0.8424 - val_accuracy: 0.7451 - val_loss: 0.7378\n",
      "Epoch 86/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7217 - loss: 0.7580 - val_accuracy: 0.7126 - val_loss: 0.7672\n",
      "Epoch 87/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7044 - loss: 0.7887 - val_accuracy: 0.7223 - val_loss: 0.7915\n",
      "Epoch 88/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7129 - loss: 0.7939 - val_accuracy: 0.7223 - val_loss: 0.7717\n",
      "Epoch 89/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7015 - loss: 0.7978 - val_accuracy: 0.7115 - val_loss: 0.7942\n",
      "Epoch 90/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6847 - loss: 0.8737 - val_accuracy: 0.6844 - val_loss: 0.8492\n",
      "Epoch 91/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6992 - loss: 0.8132 - val_accuracy: 0.7343 - val_loss: 0.7361\n",
      "Epoch 92/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7214 - loss: 0.7424 - val_accuracy: 0.7451 - val_loss: 0.7442\n",
      "Epoch 93/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7034 - loss: 0.8147 - val_accuracy: 0.6594 - val_loss: 0.9503\n",
      "Epoch 94/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6982 - loss: 0.8183 - val_accuracy: 0.7191 - val_loss: 0.7787\n",
      "Epoch 95/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7268 - loss: 0.7572 - val_accuracy: 0.7386 - val_loss: 0.7382\n",
      "Epoch 96/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7003 - loss: 0.7946 - val_accuracy: 0.7440 - val_loss: 0.7252\n",
      "Epoch 97/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7254 - loss: 0.7233 - val_accuracy: 0.7505 - val_loss: 0.7225\n",
      "Epoch 98/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7505 - loss: 0.6978 - val_accuracy: 0.6421 - val_loss: 1.0900\n",
      "Epoch 99/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6858 - loss: 0.8757 - val_accuracy: 0.7386 - val_loss: 0.7634\n",
      "Epoch 100/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7390 - loss: 0.7043 - val_accuracy: 0.5770 - val_loss: 1.2205\n",
      "Epoch 101/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7107 - loss: 0.7848 - val_accuracy: 0.7256 - val_loss: 0.7700\n",
      "Epoch 102/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7379 - loss: 0.6952 - val_accuracy: 0.7462 - val_loss: 0.7109\n",
      "Epoch 103/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7383 - loss: 0.7218 - val_accuracy: 0.7397 - val_loss: 0.7551\n",
      "Epoch 104/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7504 - loss: 0.6866 - val_accuracy: 0.7495 - val_loss: 0.6874\n",
      "Epoch 105/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7515 - loss: 0.6762 - val_accuracy: 0.7603 - val_loss: 0.6950\n",
      "Epoch 106/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7616 - loss: 0.6681 - val_accuracy: 0.7711 - val_loss: 0.6569\n",
      "Epoch 107/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7621 - loss: 0.6383 - val_accuracy: 0.7440 - val_loss: 0.7039\n",
      "Epoch 108/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7463 - loss: 0.7064 - val_accuracy: 0.7777 - val_loss: 0.6528\n",
      "Epoch 109/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7377 - loss: 0.7262 - val_accuracy: 0.7581 - val_loss: 0.6807\n",
      "Epoch 110/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7803 - loss: 0.6112 - val_accuracy: 0.7386 - val_loss: 0.7448\n",
      "Epoch 111/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7585 - loss: 0.6788 - val_accuracy: 0.7386 - val_loss: 0.7174\n",
      "Epoch 112/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7305 - loss: 0.7444 - val_accuracy: 0.7245 - val_loss: 0.7728\n",
      "Epoch 113/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7564 - loss: 0.6796 - val_accuracy: 0.6725 - val_loss: 0.9633\n",
      "Epoch 114/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7327 - loss: 0.7435 - val_accuracy: 0.7679 - val_loss: 0.6706\n",
      "Epoch 115/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7757 - loss: 0.6135 - val_accuracy: 0.6562 - val_loss: 1.0096\n",
      "Epoch 116/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7334 - loss: 0.7183 - val_accuracy: 0.7570 - val_loss: 0.7064\n",
      "Epoch 117/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7437 - loss: 0.6548 - val_accuracy: 0.7809 - val_loss: 0.6420\n",
      "Epoch 118/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7913 - loss: 0.5804 - val_accuracy: 0.7039 - val_loss: 0.8530\n",
      "Epoch 119/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7583 - loss: 0.6663 - val_accuracy: 0.7354 - val_loss: 0.7863\n",
      "Epoch 120/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7697 - loss: 0.6307 - val_accuracy: 0.7690 - val_loss: 0.6759\n",
      "Epoch 121/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7716 - loss: 0.5979 - val_accuracy: 0.7907 - val_loss: 0.6207\n",
      "Epoch 122/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7832 - loss: 0.5843 - val_accuracy: 0.7787 - val_loss: 0.6562\n",
      "Epoch 123/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7845 - loss: 0.5551 - val_accuracy: 0.6768 - val_loss: 0.9565\n",
      "Epoch 124/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7555 - loss: 0.6681 - val_accuracy: 0.7451 - val_loss: 0.7728\n",
      "Epoch 125/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7954 - loss: 0.5577 - val_accuracy: 0.7744 - val_loss: 0.6628\n",
      "Epoch 126/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7976 - loss: 0.5676 - val_accuracy: 0.7213 - val_loss: 0.7873\n",
      "Epoch 127/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8021 - loss: 0.5506 - val_accuracy: 0.7711 - val_loss: 0.6478\n",
      "Epoch 128/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7764 - loss: 0.6338 - val_accuracy: 0.6497 - val_loss: 1.1600\n",
      "Epoch 129/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7680 - loss: 0.6463 - val_accuracy: 0.7657 - val_loss: 0.6895\n",
      "Epoch 130/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7988 - loss: 0.5647 - val_accuracy: 0.7983 - val_loss: 0.6135\n",
      "Epoch 131/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8160 - loss: 0.5089 - val_accuracy: 0.7918 - val_loss: 0.6088\n",
      "Epoch 132/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8128 - loss: 0.5224 - val_accuracy: 0.7223 - val_loss: 0.7753\n",
      "Epoch 133/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7950 - loss: 0.5543 - val_accuracy: 0.7560 - val_loss: 0.6741\n",
      "Epoch 134/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7943 - loss: 0.5685 - val_accuracy: 0.7701 - val_loss: 0.6652\n",
      "Epoch 135/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8090 - loss: 0.5137 - val_accuracy: 0.7842 - val_loss: 0.6272\n",
      "Epoch 136/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8041 - loss: 0.5189 - val_accuracy: 0.8059 - val_loss: 0.6133\n",
      "Epoch 137/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8152 - loss: 0.4939 - val_accuracy: 0.7928 - val_loss: 0.6331\n",
      "Epoch 138/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8195 - loss: 0.4763 - val_accuracy: 0.7711 - val_loss: 0.6651\n",
      "Epoch 139/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8291 - loss: 0.4766 - val_accuracy: 0.7972 - val_loss: 0.5744\n",
      "Epoch 140/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8355 - loss: 0.4511 - val_accuracy: 0.7885 - val_loss: 0.6465\n",
      "Epoch 141/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8038 - loss: 0.5333 - val_accuracy: 0.7885 - val_loss: 0.5870\n",
      "Epoch 142/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8404 - loss: 0.4446 - val_accuracy: 0.8015 - val_loss: 0.6150\n",
      "Epoch 143/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8204 - loss: 0.5101 - val_accuracy: 0.7397 - val_loss: 0.8335\n",
      "Epoch 144/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7771 - loss: 0.6314 - val_accuracy: 0.7896 - val_loss: 0.6316\n",
      "Epoch 145/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8243 - loss: 0.4772 - val_accuracy: 0.7831 - val_loss: 0.6214\n",
      "Epoch 146/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8102 - loss: 0.5145 - val_accuracy: 0.8091 - val_loss: 0.5648\n",
      "Epoch 147/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8521 - loss: 0.4270 - val_accuracy: 0.8059 - val_loss: 0.6025\n",
      "Epoch 148/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8233 - loss: 0.4642 - val_accuracy: 0.7852 - val_loss: 0.6267\n",
      "Epoch 149/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8355 - loss: 0.4404 - val_accuracy: 0.7668 - val_loss: 0.6691\n",
      "Epoch 150/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8233 - loss: 0.4797 - val_accuracy: 0.7874 - val_loss: 0.6139\n",
      "Epoch 151/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8137 - loss: 0.5272 - val_accuracy: 0.7668 - val_loss: 0.7098\n",
      "Epoch 152/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7905 - loss: 0.5419 - val_accuracy: 0.8113 - val_loss: 0.5822\n",
      "Epoch 153/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8474 - loss: 0.4227 - val_accuracy: 0.7592 - val_loss: 0.7046\n",
      "Epoch 154/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8457 - loss: 0.4461 - val_accuracy: 0.7527 - val_loss: 0.7894\n",
      "Epoch 155/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8250 - loss: 0.4789 - val_accuracy: 0.7863 - val_loss: 0.6246\n",
      "Epoch 156/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8350 - loss: 0.4423 - val_accuracy: 0.8145 - val_loss: 0.5735\n",
      "Epoch 157/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8418 - loss: 0.4271 - val_accuracy: 0.8254 - val_loss: 0.5376\n",
      "Epoch 158/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8334 - loss: 0.4632 - val_accuracy: 0.7484 - val_loss: 0.6694\n",
      "Epoch 159/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8446 - loss: 0.4252 - val_accuracy: 0.8210 - val_loss: 0.5293\n",
      "Epoch 160/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8141 - loss: 0.4962 - val_accuracy: 0.7950 - val_loss: 0.6073\n",
      "Epoch 161/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8602 - loss: 0.3816 - val_accuracy: 0.8254 - val_loss: 0.5551\n",
      "Epoch 162/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8470 - loss: 0.4257 - val_accuracy: 0.8113 - val_loss: 0.5961\n",
      "Epoch 163/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8576 - loss: 0.4031 - val_accuracy: 0.8167 - val_loss: 0.5665\n",
      "Epoch 164/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8476 - loss: 0.4007 - val_accuracy: 0.8200 - val_loss: 0.5747\n",
      "Epoch 165/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8663 - loss: 0.3805 - val_accuracy: 0.7972 - val_loss: 0.6113\n",
      "Epoch 166/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8457 - loss: 0.4402 - val_accuracy: 0.7278 - val_loss: 0.8822\n",
      "Epoch 167/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8322 - loss: 0.4718 - val_accuracy: 0.7907 - val_loss: 0.6266\n",
      "Epoch 168/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8368 - loss: 0.4479 - val_accuracy: 0.8167 - val_loss: 0.5936\n",
      "Epoch 169/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8798 - loss: 0.3420 - val_accuracy: 0.8297 - val_loss: 0.5698\n",
      "Epoch 170/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8494 - loss: 0.4045 - val_accuracy: 0.8004 - val_loss: 0.6322\n",
      "Epoch 171/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8426 - loss: 0.4494 - val_accuracy: 0.7993 - val_loss: 0.6289\n",
      "Epoch 172/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8541 - loss: 0.4058 - val_accuracy: 0.8080 - val_loss: 0.5823\n",
      "Epoch 173/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8018 - loss: 0.5758 - val_accuracy: 0.8069 - val_loss: 0.5459\n",
      "Epoch 174/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8601 - loss: 0.3835 - val_accuracy: 0.7202 - val_loss: 0.8568\n",
      "Epoch 175/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8227 - loss: 0.5042 - val_accuracy: 0.8059 - val_loss: 0.5823\n",
      "Epoch 176/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8090 - loss: 0.5333 - val_accuracy: 0.8351 - val_loss: 0.5386\n",
      "Epoch 177/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8635 - loss: 0.3662 - val_accuracy: 0.8232 - val_loss: 0.5536\n",
      "Epoch 178/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8815 - loss: 0.3209 - val_accuracy: 0.8341 - val_loss: 0.5174\n",
      "Epoch 179/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8823 - loss: 0.3218 - val_accuracy: 0.8221 - val_loss: 0.5292\n",
      "Epoch 180/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8661 - loss: 0.3703 - val_accuracy: 0.8308 - val_loss: 0.5451\n",
      "Epoch 181/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8785 - loss: 0.3481 - val_accuracy: 0.8373 - val_loss: 0.5172\n",
      "Epoch 182/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8796 - loss: 0.3326 - val_accuracy: 0.8275 - val_loss: 0.5608\n",
      "Epoch 183/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8661 - loss: 0.3531 - val_accuracy: 0.8167 - val_loss: 0.5851\n",
      "Epoch 184/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8417 - loss: 0.3908 - val_accuracy: 0.8134 - val_loss: 0.5989\n",
      "Epoch 185/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8308 - loss: 0.4790 - val_accuracy: 0.7668 - val_loss: 0.7418\n",
      "Epoch 186/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8645 - loss: 0.3799 - val_accuracy: 0.8460 - val_loss: 0.5043\n",
      "Epoch 187/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8891 - loss: 0.3111 - val_accuracy: 0.8319 - val_loss: 0.5400\n",
      "Epoch 188/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8791 - loss: 0.3253 - val_accuracy: 0.8286 - val_loss: 0.5334\n",
      "Epoch 189/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8627 - loss: 0.4036 - val_accuracy: 0.8080 - val_loss: 0.6076\n",
      "Epoch 190/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8815 - loss: 0.3309 - val_accuracy: 0.8395 - val_loss: 0.5285\n",
      "Epoch 191/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8805 - loss: 0.3067 - val_accuracy: 0.7972 - val_loss: 0.6417\n",
      "Epoch 192/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8664 - loss: 0.3713 - val_accuracy: 0.8004 - val_loss: 0.6554\n",
      "Epoch 193/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8666 - loss: 0.3638 - val_accuracy: 0.8275 - val_loss: 0.5610\n",
      "Epoch 194/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8729 - loss: 0.3607 - val_accuracy: 0.8232 - val_loss: 0.5682\n",
      "Epoch 195/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8701 - loss: 0.3473 - val_accuracy: 0.8059 - val_loss: 0.6685\n",
      "Epoch 196/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8844 - loss: 0.3163 - val_accuracy: 0.8362 - val_loss: 0.5613\n",
      "Epoch 197/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8461 - loss: 0.4183 - val_accuracy: 0.8492 - val_loss: 0.5019\n",
      "Epoch 198/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8957 - loss: 0.2970 - val_accuracy: 0.8243 - val_loss: 0.6029\n",
      "Epoch 199/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8798 - loss: 0.3313 - val_accuracy: 0.8243 - val_loss: 0.5902\n",
      "Epoch 200/200\n",
      "\u001b[1m116/116\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8709 - loss: 0.3572 - val_accuracy: 0.8319 - val_loss: 0.5780\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train_encoded, epochs=200, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8491 - loss: 0.5790\n",
      "Test Loss: 0.6383593082427979, Test Accuracy: 0.8368055820465088\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94        90\n",
      "           1       0.89      0.92      0.91        90\n",
      "           2       0.79      0.70      0.74        86\n",
      "           3       0.79      0.86      0.82        83\n",
      "           4       0.90      0.87      0.88        83\n",
      "           5       0.93      0.90      0.92        90\n",
      "           6       0.88      0.86      0.87        35\n",
      "           7       0.87      0.67      0.76        86\n",
      "           8       0.70      0.69      0.70        83\n",
      "           9       0.86      0.86      0.86        22\n",
      "          10       0.68      0.72      0.70        54\n",
      "          11       0.92      0.88      0.90        26\n",
      "          12       0.79      0.88      0.84        26\n",
      "          13       0.86      0.92      0.89        13\n",
      "          14       0.91      0.82      0.87        90\n",
      "          15       0.78      0.89      0.83        93\n",
      "          16       0.75      0.87      0.81       102\n",
      "\n",
      "    accuracy                           0.83      1152\n",
      "   macro avg       0.84      0.84      0.84      1152\n",
      "weighted avg       0.84      0.83      0.83      1152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# with open('lstmmodel.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "model.save('lstmmodel.h5')\n",
    "tf.saved_model.save(model, 'saved_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
