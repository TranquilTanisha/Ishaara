{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y24</th>\n",
       "      <th>Z24</th>\n",
       "      <th>X25</th>\n",
       "      <th>Y25</th>\n",
       "      <th>Z25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y26</th>\n",
       "      <th>Z26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530313</td>\n",
       "      <td>0.585678</td>\n",
       "      <td>-0.365724</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.130718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.441575</td>\n",
       "      <td>-10.572632</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>-0.500577</td>\n",
       "      <td>-12.161836</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>-0.622653</td>\n",
       "      <td>-11.623393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531883</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>-0.381889</td>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.241074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.754394</td>\n",
       "      <td>-7.421648</td>\n",
       "      <td>0.392176</td>\n",
       "      <td>-0.842538</td>\n",
       "      <td>-8.545950</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>-0.967975</td>\n",
       "      <td>-8.040165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.574829</td>\n",
       "      <td>-0.470731</td>\n",
       "      <td>0.376408</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.216554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.678109</td>\n",
       "      <td>-8.186996</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>-0.814776</td>\n",
       "      <td>-9.373079</td>\n",
       "      <td>0.455979</td>\n",
       "      <td>-0.910327</td>\n",
       "      <td>-8.907041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>-0.516997</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.629331</td>\n",
       "      <td>0.273374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.797789</td>\n",
       "      <td>-10.197847</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>-0.955783</td>\n",
       "      <td>-11.699729</td>\n",
       "      <td>0.451075</td>\n",
       "      <td>-1.066465</td>\n",
       "      <td>-11.020497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>0.384628</td>\n",
       "      <td>0.589920</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112924</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587871</td>\n",
       "      <td>9.669083</td>\n",
       "      <td>1.162361</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>11.412089</td>\n",
       "      <td>1.129761</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>10.942098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574938</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>-0.300268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112925</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>21.491173</td>\n",
       "      <td>1.587999</td>\n",
       "      <td>0.362523</td>\n",
       "      <td>25.172693</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>24.418745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578150</td>\n",
       "      <td>0.539364</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>0.398031</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>-0.181356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112926</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>30.405003</td>\n",
       "      <td>1.653764</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>35.272096</td>\n",
       "      <td>1.652273</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>34.866246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580092</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.398705</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>-0.237497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112927</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182174</td>\n",
       "      <td>30.964133</td>\n",
       "      <td>1.847807</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>36.075314</td>\n",
       "      <td>1.835613</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>35.360101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580864</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.538843</td>\n",
       "      <td>-0.243124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112928</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>0.140643</td>\n",
       "      <td>43.708800</td>\n",
       "      <td>2.127749</td>\n",
       "      <td>0.198996</td>\n",
       "      <td>50.094664</td>\n",
       "      <td>2.107137</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>49.969990</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585219</td>\n",
       "      <td>0.542360</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>0.397739</td>\n",
       "      <td>0.534382</td>\n",
       "      <td>-0.228005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112929 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frame       X00        Y00       Z00       X01        Y01  \\\n",
       "0       Boy      1 -3.320268  12.714561  0.223574 -3.320268  12.714561   \n",
       "1       Boy      2 -0.441575 -10.572632  0.370819 -0.500577 -12.161836   \n",
       "2       Boy      3 -0.754394  -7.421648  0.392176 -0.842538  -8.545950   \n",
       "3       Boy      4 -0.678109  -8.186996  0.441856 -0.814776  -9.373079   \n",
       "4       Boy      5 -0.797789 -10.197847  0.429872 -0.955783 -11.699729   \n",
       "...     ...    ...       ...        ...       ...       ...        ...   \n",
       "112924  You     16  0.587871   9.669083  1.162361  0.726044  11.412089   \n",
       "112925  You     17  0.337344  21.491173  1.587999  0.362523  25.172693   \n",
       "112926  You     18  0.215001  30.405003  1.653764  0.260286  35.272096   \n",
       "112927  You     19  0.182174  30.964133  1.847807  0.224303  36.075314   \n",
       "112928  You     20  0.140643  43.708800  2.127749  0.198996  50.094664   \n",
       "\n",
       "             Z01       X02        Y02  ...  Z23  X24  Y24  Z24       X25  \\\n",
       "0       0.223574 -3.320268  12.714561  ...  1.0  0.0  0.0  0.0  0.530313   \n",
       "1       0.379360 -0.622653 -11.623393  ...  1.0  0.0  0.0  0.0  0.531883   \n",
       "2       0.401212 -0.967975  -8.040165  ...  1.0  0.0  0.0  0.0  0.534591   \n",
       "3       0.455979 -0.910327  -8.907041  ...  1.0  0.0  0.0  0.0  0.484453   \n",
       "4       0.451075 -1.066465 -11.020497  ...  1.0  0.0  0.0  0.0  0.474471   \n",
       "...          ...       ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "112924  1.129761  0.655170  10.942098  ...  1.0  0.0  0.0  0.0  0.574938   \n",
       "112925  1.563665  0.306954  24.418745  ...  1.0  0.0  0.0  0.0  0.578150   \n",
       "112926  1.652273  0.141393  34.866246  ...  1.0  0.0  0.0  0.0  0.580092   \n",
       "112927  1.835613  0.095749  35.360101  ...  1.0  0.0  0.0  0.0  0.580864   \n",
       "112928  2.107137  0.070886  49.969990  ...  1.0  0.0  0.0  0.0  0.585219   \n",
       "\n",
       "             Y25       Z25       X26       Y26       Z26  \n",
       "0       0.585678 -0.365724  0.393299  0.607514  0.130718  \n",
       "1       0.588693 -0.381889  0.385701  0.626965  0.241074  \n",
       "2       0.574829 -0.470731  0.376408  0.645435  0.216554  \n",
       "3       0.453059 -0.516997  0.365139  0.629331  0.273374  \n",
       "4       0.342270 -0.453968  0.384628  0.589920  0.266067  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "112924  0.538055  0.070751  0.395394  0.452951 -0.300268  \n",
       "112925  0.539364  0.051295  0.398031  0.515749 -0.181356  \n",
       "112926  0.541042  0.027703  0.398705  0.534306 -0.237497  \n",
       "112927  0.542060  0.006164  0.398723  0.538843 -0.243124  \n",
       "112928  0.542360 -0.016540  0.397739  0.534382 -0.228005  \n",
       "\n",
       "[112929 rows x 83 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load each .npy file and convert the list of dictionaries to a DataFrame\n",
    "dataframes = []\n",
    "for file in os.listdir('dataset'):\n",
    "    if file.endswith('.npy'):\n",
    "        data = np.load('dataset/' + file, allow_pickle=True)\n",
    "        dataframes.append(pd.DataFrame.from_records(data))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = merged_df['Frame'].max()\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_13876\\2834639197.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y24</th>\n",
       "      <th>Z24</th>\n",
       "      <th>X25</th>\n",
       "      <th>Y25</th>\n",
       "      <th>Z25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y26</th>\n",
       "      <th>Z26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530313</td>\n",
       "      <td>0.585678</td>\n",
       "      <td>-0.365724</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.130718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.441575</td>\n",
       "      <td>-10.572632</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>-0.500577</td>\n",
       "      <td>-12.161836</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>-0.622653</td>\n",
       "      <td>-11.623393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531883</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>-0.381889</td>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.241074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.754394</td>\n",
       "      <td>-7.421648</td>\n",
       "      <td>0.392176</td>\n",
       "      <td>-0.842538</td>\n",
       "      <td>-8.545950</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>-0.967975</td>\n",
       "      <td>-8.040165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.574829</td>\n",
       "      <td>-0.470731</td>\n",
       "      <td>0.376408</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.216554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.678109</td>\n",
       "      <td>-8.186996</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>-0.814776</td>\n",
       "      <td>-9.373079</td>\n",
       "      <td>0.455979</td>\n",
       "      <td>-0.910327</td>\n",
       "      <td>-8.907041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>-0.516997</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.629331</td>\n",
       "      <td>0.273374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.797789</td>\n",
       "      <td>-10.197847</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>-0.955783</td>\n",
       "      <td>-11.699729</td>\n",
       "      <td>0.451075</td>\n",
       "      <td>-1.066465</td>\n",
       "      <td>-11.020497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>0.384628</td>\n",
       "      <td>0.589920</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112924</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587871</td>\n",
       "      <td>9.669083</td>\n",
       "      <td>1.162361</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>11.412089</td>\n",
       "      <td>1.129761</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>10.942098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574938</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>-0.300268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112925</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>21.491173</td>\n",
       "      <td>1.587999</td>\n",
       "      <td>0.362523</td>\n",
       "      <td>25.172693</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>24.418745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578150</td>\n",
       "      <td>0.539364</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>0.398031</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>-0.181356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112926</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>30.405003</td>\n",
       "      <td>1.653764</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>35.272096</td>\n",
       "      <td>1.652273</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>34.866246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580092</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.398705</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>-0.237497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112927</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182174</td>\n",
       "      <td>30.964133</td>\n",
       "      <td>1.847807</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>36.075314</td>\n",
       "      <td>1.835613</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>35.360101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580864</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.538843</td>\n",
       "      <td>-0.243124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112928</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>0.140643</td>\n",
       "      <td>43.708800</td>\n",
       "      <td>2.127749</td>\n",
       "      <td>0.198996</td>\n",
       "      <td>50.094664</td>\n",
       "      <td>2.107137</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>49.969990</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585219</td>\n",
       "      <td>0.542360</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>0.397739</td>\n",
       "      <td>0.534382</td>\n",
       "      <td>-0.228005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112929 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frame       X00        Y00       Z00       X01        Y01  \\\n",
       "0       Boy      1 -3.320268  12.714561  0.223574 -3.320268  12.714561   \n",
       "1       Boy      2 -0.441575 -10.572632  0.370819 -0.500577 -12.161836   \n",
       "2       Boy      3 -0.754394  -7.421648  0.392176 -0.842538  -8.545950   \n",
       "3       Boy      4 -0.678109  -8.186996  0.441856 -0.814776  -9.373079   \n",
       "4       Boy      5 -0.797789 -10.197847  0.429872 -0.955783 -11.699729   \n",
       "...     ...    ...       ...        ...       ...       ...        ...   \n",
       "112924  You     16  0.587871   9.669083  1.162361  0.726044  11.412089   \n",
       "112925  You     17  0.337344  21.491173  1.587999  0.362523  25.172693   \n",
       "112926  You     18  0.215001  30.405003  1.653764  0.260286  35.272096   \n",
       "112927  You     19  0.182174  30.964133  1.847807  0.224303  36.075314   \n",
       "112928  You     20  0.140643  43.708800  2.127749  0.198996  50.094664   \n",
       "\n",
       "             Z01       X02        Y02  ...  Z23  X24  Y24  Z24       X25  \\\n",
       "0       0.223574 -3.320268  12.714561  ...  1.0  0.0  0.0  0.0  0.530313   \n",
       "1       0.379360 -0.622653 -11.623393  ...  1.0  0.0  0.0  0.0  0.531883   \n",
       "2       0.401212 -0.967975  -8.040165  ...  1.0  0.0  0.0  0.0  0.534591   \n",
       "3       0.455979 -0.910327  -8.907041  ...  1.0  0.0  0.0  0.0  0.484453   \n",
       "4       0.451075 -1.066465 -11.020497  ...  1.0  0.0  0.0  0.0  0.474471   \n",
       "...          ...       ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "112924  1.129761  0.655170  10.942098  ...  1.0  0.0  0.0  0.0  0.574938   \n",
       "112925  1.563665  0.306954  24.418745  ...  1.0  0.0  0.0  0.0  0.578150   \n",
       "112926  1.652273  0.141393  34.866246  ...  1.0  0.0  0.0  0.0  0.580092   \n",
       "112927  1.835613  0.095749  35.360101  ...  1.0  0.0  0.0  0.0  0.580864   \n",
       "112928  2.107137  0.070886  49.969990  ...  1.0  0.0  0.0  0.0  0.585219   \n",
       "\n",
       "             Y25       Z25       X26       Y26       Z26  \n",
       "0       0.585678 -0.365724  0.393299  0.607514  0.130718  \n",
       "1       0.588693 -0.381889  0.385701  0.626965  0.241074  \n",
       "2       0.574829 -0.470731  0.376408  0.645435  0.216554  \n",
       "3       0.453059 -0.516997  0.365139  0.629331  0.273374  \n",
       "4       0.342270 -0.453968  0.384628  0.589920  0.266067  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "112924  0.538055  0.070751  0.395394  0.452951 -0.300268  \n",
       "112925  0.539364  0.051295  0.398031  0.515749 -0.181356  \n",
       "112926  0.541042  0.027703  0.398705  0.534306 -0.237497  \n",
       "112927  0.542060  0.006164  0.398723  0.538843 -0.243124  \n",
       "112928  0.542360 -0.016540  0.397739  0.534382 -0.228005  \n",
       "\n",
       "[112929 rows x 83 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(group):\n",
    "    # Calculate the number of padding rows needed\n",
    "    padding_rows = max_sequence_length - len(group)\n",
    "    \n",
    "    # Create a DataFrame with padding rows filled with NaN (or any other padding value)\n",
    "    padding_df = pd.DataFrame({\n",
    "        'Word': [group['Word'].iloc[0]] * padding_rows,\n",
    "        'Frame': np.arange(len(group) + 1, max_sequence_length + 1),\n",
    "    })\n",
    "    \n",
    "    # Concatenate the original group with the padding DataFrame\n",
    "    return pd.concat([group, padding_df], ignore_index=True)\n",
    "\n",
    "# Group the DataFrame by 'Word' and apply the padding function\n",
    "padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n",
    "padded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = padded_df.drop('Word', axis=1)\n",
    "target = padded_df['Word']\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "features_scaled = scaler.fit_transform(data)\n",
    "\n",
    "import pickle\n",
    "with open('scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the interval\n",
    "# interval = 20\n",
    "\n",
    "# # Initialize lists to hold the training and testing sets\n",
    "# X_train_list = []\n",
    "# y_train_list = []\n",
    "# X_test_list = []\n",
    "# y_test_list = []\n",
    "\n",
    "# # Iterate over features_scaled with a step of 20\n",
    "# for i in range(0, len(padded_df) - interval, 3*interval):\n",
    "#     # Slice the dataset for the current interval\n",
    "#     X_train = pd.DataFrame(features_scaled[i:i+2*interval])\n",
    "#     y_train = pd.DataFrame(target[i:i+2*interval])\n",
    "\n",
    "#     X_test = pd.DataFrame(features_scaled[i+2*interval:i+3*interval])\n",
    "#     y_test = pd.DataFrame(target[i+2*interval:i+3*interval])\n",
    "\n",
    "#     # Append the sliced DataFrames to the lists\n",
    "#     X_train_list.append(X_train)\n",
    "#     y_train_list.append(y_train)\n",
    "#     X_test_list.append(X_test)\n",
    "#     y_test_list.append(y_test)\n",
    "\n",
    "# # Convert the lists to DataFrames\n",
    "# X_train_df = pd.concat(X_train_list)\n",
    "# y_train_df = pd.concat(y_train_list)\n",
    "# X_test_df = pd.concat(X_test_list)\n",
    "# y_test_df = pd.concat(y_test_list)\n",
    "\n",
    "# X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. RandomForestClassifier expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1058\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. RandomForestClassifier expected <= 2."
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Initialize the model\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = clf.predict(X_test_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(classification_report(y_test_df, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('rfmodel.pkl', 'wb') as file:\n",
    "#     pickle.dump(clf, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5646"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of time steps\n",
    "time_steps = 20\n",
    "\n",
    "# Prepare the data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(features_scaled) - time_steps, time_steps):\n",
    "    X.append(features_scaled[i:i + time_steps])\n",
    "    y.append(target[i])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape X to fit the LSTM input shape: [samples, time steps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'y_train' is your target variable with categorical values\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "num_classes = len(encoder.classes_)\n",
    "print(X_train.shape[1], X_train.shape[2])\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax')) # num_classes is the number of unique words you're predicting) # num_classes is the number of unique words you're predicting\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8407 - loss: 0.4660 - val_accuracy: 0.7124 - val_loss: 0.9510\n",
      "Epoch 2/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8488 - loss: 0.4496 - val_accuracy: 0.7157 - val_loss: 0.9562\n",
      "Epoch 3/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8699 - loss: 0.4094 - val_accuracy: 0.6825 - val_loss: 1.0614\n",
      "Epoch 4/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8620 - loss: 0.4257 - val_accuracy: 0.7035 - val_loss: 0.9994\n",
      "Epoch 5/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8470 - loss: 0.4330 - val_accuracy: 0.7002 - val_loss: 0.9562\n",
      "Epoch 6/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8293 - loss: 0.4993 - val_accuracy: 0.7190 - val_loss: 0.9325\n",
      "Epoch 7/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8786 - loss: 0.3739 - val_accuracy: 0.6980 - val_loss: 0.9738\n",
      "Epoch 8/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8415 - loss: 0.4534 - val_accuracy: 0.6604 - val_loss: 1.0939\n",
      "Epoch 9/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8316 - loss: 0.4924 - val_accuracy: 0.7124 - val_loss: 0.9432\n",
      "Epoch 10/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8695 - loss: 0.3853 - val_accuracy: 0.6781 - val_loss: 1.0323\n",
      "Epoch 11/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8641 - loss: 0.3907 - val_accuracy: 0.7080 - val_loss: 0.9434\n",
      "Epoch 12/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8838 - loss: 0.3467 - val_accuracy: 0.7212 - val_loss: 0.9392\n",
      "Epoch 13/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8875 - loss: 0.3437 - val_accuracy: 0.7002 - val_loss: 0.9995\n",
      "Epoch 14/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8911 - loss: 0.3389 - val_accuracy: 0.7235 - val_loss: 0.9379\n",
      "Epoch 15/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8962 - loss: 0.3078 - val_accuracy: 0.7046 - val_loss: 0.9819\n",
      "Epoch 16/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8741 - loss: 0.3698 - val_accuracy: 0.6881 - val_loss: 1.0142\n",
      "Epoch 17/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8850 - loss: 0.3354 - val_accuracy: 0.7135 - val_loss: 1.0088\n",
      "Epoch 18/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8515 - loss: 0.4157 - val_accuracy: 0.6980 - val_loss: 1.0353\n",
      "Epoch 19/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8664 - loss: 0.3932 - val_accuracy: 0.7102 - val_loss: 0.9515\n",
      "Epoch 20/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8998 - loss: 0.2931 - val_accuracy: 0.6803 - val_loss: 1.1057\n",
      "Epoch 21/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8976 - loss: 0.3172 - val_accuracy: 0.7013 - val_loss: 0.9892\n",
      "Epoch 22/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8910 - loss: 0.3244 - val_accuracy: 0.7146 - val_loss: 0.9956\n",
      "Epoch 23/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8819 - loss: 0.3363 - val_accuracy: 0.6294 - val_loss: 1.3219\n",
      "Epoch 24/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8774 - loss: 0.3570 - val_accuracy: 0.6958 - val_loss: 1.0038\n",
      "Epoch 25/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8472 - loss: 0.4226 - val_accuracy: 0.7257 - val_loss: 0.9526\n",
      "Epoch 26/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8841 - loss: 0.3571 - val_accuracy: 0.7412 - val_loss: 0.8786\n",
      "Epoch 27/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9222 - loss: 0.2502 - val_accuracy: 0.7412 - val_loss: 0.8902\n",
      "Epoch 28/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9300 - loss: 0.2378 - val_accuracy: 0.7035 - val_loss: 1.0421\n",
      "Epoch 29/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9131 - loss: 0.2895 - val_accuracy: 0.6670 - val_loss: 1.1582\n",
      "Epoch 30/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8637 - loss: 0.3708 - val_accuracy: 0.7301 - val_loss: 0.9537\n",
      "Epoch 31/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9219 - loss: 0.2566 - val_accuracy: 0.7345 - val_loss: 0.9322\n",
      "Epoch 32/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9031 - loss: 0.2917 - val_accuracy: 0.7235 - val_loss: 1.0274\n",
      "Epoch 33/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9108 - loss: 0.2659 - val_accuracy: 0.7124 - val_loss: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x161467fff10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_encoded, epochs=33, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7168 - loss: 1.0735\n",
      "Test Loss: 1.0156817436218262, Test Accuracy: 0.7176991105079651\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       104\n",
      "           1       0.82      0.86      0.84        95\n",
      "           2       0.61      0.58      0.60        91\n",
      "           3       0.65      0.70      0.68        74\n",
      "           4       0.62      0.73      0.67        77\n",
      "           5       0.89      0.96      0.92        91\n",
      "           6       0.55      0.82      0.66        33\n",
      "           7       0.63      0.54      0.58        78\n",
      "           8       0.64      0.60      0.62        78\n",
      "           9       0.92      0.85      0.88        27\n",
      "          10       0.42      0.48      0.44        42\n",
      "          11       0.93      0.76      0.84        17\n",
      "          12       0.87      0.80      0.83        25\n",
      "          13       0.58      0.78      0.67         9\n",
      "          14       0.66      0.70      0.68        91\n",
      "          15       0.84      0.70      0.76        89\n",
      "          16       0.74      0.64      0.69       109\n",
      "\n",
      "    accuracy                           0.72      1130\n",
      "   macro avg       0.72      0.73      0.72      1130\n",
      "weighted avg       0.73      0.72      0.72      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(num_classes, activation='softmax')) # num_classes is the number of unique words you're predicting) # num_classes is the number of unique words you're predicting\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.0746 - loss: 2.7674 - val_accuracy: 0.0874 - val_loss: 2.7068\n",
      "Epoch 2/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0780 - loss: 2.7206 - val_accuracy: 0.0796 - val_loss: 2.7124\n",
      "Epoch 3/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0766 - loss: 2.7258 - val_accuracy: 0.1095 - val_loss: 2.7180\n",
      "Epoch 4/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0831 - loss: 2.7288 - val_accuracy: 0.0962 - val_loss: 2.7074\n",
      "Epoch 5/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0919 - loss: 2.7110 - val_accuracy: 0.1162 - val_loss: 2.6383\n",
      "Epoch 6/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.1170 - loss: 2.6468 - val_accuracy: 0.1538 - val_loss: 2.5534\n",
      "Epoch 7/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.1395 - loss: 2.5264 - val_accuracy: 0.1958 - val_loss: 2.3855\n",
      "Epoch 8/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.1814 - loss: 2.4168 - val_accuracy: 0.1903 - val_loss: 2.4067\n",
      "Epoch 9/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.2251 - loss: 2.2826 - val_accuracy: 0.1449 - val_loss: 2.6644\n",
      "Epoch 10/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2505 - loss: 2.2117 - val_accuracy: 0.2677 - val_loss: 2.0962\n",
      "Epoch 11/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.2759 - loss: 2.0761 - val_accuracy: 0.2832 - val_loss: 2.0420\n",
      "Epoch 12/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.2923 - loss: 2.0112 - val_accuracy: 0.2345 - val_loss: 2.2270\n",
      "Epoch 13/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3020 - loss: 2.0019 - val_accuracy: 0.3374 - val_loss: 1.9259\n",
      "Epoch 14/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3302 - loss: 1.9420 - val_accuracy: 0.3274 - val_loss: 1.9096\n",
      "Epoch 15/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3257 - loss: 1.9238 - val_accuracy: 0.3518 - val_loss: 1.9006\n",
      "Epoch 16/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3774 - loss: 1.8090 - val_accuracy: 0.3252 - val_loss: 1.9378\n",
      "Epoch 17/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3913 - loss: 1.7757 - val_accuracy: 0.3440 - val_loss: 1.8340\n",
      "Epoch 18/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3697 - loss: 1.7896 - val_accuracy: 0.4192 - val_loss: 1.7280\n",
      "Epoch 19/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3776 - loss: 1.7612 - val_accuracy: 0.2954 - val_loss: 2.1224\n",
      "Epoch 20/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3766 - loss: 1.7741 - val_accuracy: 0.2898 - val_loss: 2.0121\n",
      "Epoch 21/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4157 - loss: 1.6837 - val_accuracy: 0.3960 - val_loss: 1.7093\n",
      "Epoch 22/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4573 - loss: 1.5947 - val_accuracy: 0.4159 - val_loss: 1.6353\n",
      "Epoch 23/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4703 - loss: 1.4970 - val_accuracy: 0.4181 - val_loss: 1.6964\n",
      "Epoch 24/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4360 - loss: 1.5793 - val_accuracy: 0.4668 - val_loss: 1.5465\n",
      "Epoch 25/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4993 - loss: 1.4402 - val_accuracy: 0.4735 - val_loss: 1.5225\n",
      "Epoch 26/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5029 - loss: 1.4315 - val_accuracy: 0.4934 - val_loss: 1.4616\n",
      "Epoch 27/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5315 - loss: 1.3511 - val_accuracy: 0.4912 - val_loss: 1.4700\n",
      "Epoch 28/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5408 - loss: 1.3520 - val_accuracy: 0.5221 - val_loss: 1.3876\n",
      "Epoch 29/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5565 - loss: 1.3025 - val_accuracy: 0.5243 - val_loss: 1.3700\n",
      "Epoch 30/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5869 - loss: 1.2128 - val_accuracy: 0.4635 - val_loss: 1.5902\n",
      "Epoch 31/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5733 - loss: 1.2290 - val_accuracy: 0.5254 - val_loss: 1.3975\n",
      "Epoch 32/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5889 - loss: 1.1635 - val_accuracy: 0.5608 - val_loss: 1.2830\n",
      "Epoch 33/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5905 - loss: 1.2074 - val_accuracy: 0.4679 - val_loss: 1.5254\n",
      "Epoch 34/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6039 - loss: 1.1442 - val_accuracy: 0.5044 - val_loss: 1.4943\n",
      "Epoch 35/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6013 - loss: 1.1552 - val_accuracy: 0.5597 - val_loss: 1.2911\n",
      "Epoch 36/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6117 - loss: 1.1083 - val_accuracy: 0.5785 - val_loss: 1.2350\n",
      "Epoch 37/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6279 - loss: 1.0693 - val_accuracy: 0.5520 - val_loss: 1.3308\n",
      "Epoch 38/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6300 - loss: 1.0213 - val_accuracy: 0.5619 - val_loss: 1.2542\n",
      "Epoch 39/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6458 - loss: 1.0228 - val_accuracy: 0.6228 - val_loss: 1.1424\n",
      "Epoch 40/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6675 - loss: 0.9532 - val_accuracy: 0.5863 - val_loss: 1.1585\n",
      "Epoch 41/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6529 - loss: 0.9697 - val_accuracy: 0.5819 - val_loss: 1.2233\n",
      "Epoch 42/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6768 - loss: 0.9373 - val_accuracy: 0.6272 - val_loss: 1.0905\n",
      "Epoch 43/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7007 - loss: 0.8667 - val_accuracy: 0.5885 - val_loss: 1.2106\n",
      "Epoch 44/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6818 - loss: 0.9037 - val_accuracy: 0.6305 - val_loss: 1.1432\n",
      "Epoch 45/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7073 - loss: 0.8619 - val_accuracy: 0.6394 - val_loss: 1.0504\n",
      "Epoch 46/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7194 - loss: 0.8360 - val_accuracy: 0.6283 - val_loss: 1.0651\n",
      "Epoch 47/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6894 - loss: 0.8803 - val_accuracy: 0.6217 - val_loss: 1.0943\n",
      "Epoch 48/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7104 - loss: 0.8367 - val_accuracy: 0.6372 - val_loss: 1.0610\n",
      "Epoch 49/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6938 - loss: 0.8579 - val_accuracy: 0.6184 - val_loss: 1.1739\n",
      "Epoch 50/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7048 - loss: 0.8680 - val_accuracy: 0.5553 - val_loss: 1.3535\n",
      "Epoch 51/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7191 - loss: 0.8374 - val_accuracy: 0.5940 - val_loss: 1.1856\n",
      "Epoch 52/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7360 - loss: 0.7742 - val_accuracy: 0.6493 - val_loss: 1.0385\n",
      "Epoch 53/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7535 - loss: 0.6996 - val_accuracy: 0.6626 - val_loss: 1.0365\n",
      "Epoch 54/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7693 - loss: 0.6821 - val_accuracy: 0.6316 - val_loss: 1.1017\n",
      "Epoch 55/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7771 - loss: 0.7023 - val_accuracy: 0.6482 - val_loss: 1.0415\n",
      "Epoch 56/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7297 - loss: 0.7663 - val_accuracy: 0.6814 - val_loss: 1.0029\n",
      "Epoch 57/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7695 - loss: 0.6625 - val_accuracy: 0.6659 - val_loss: 0.9970\n",
      "Epoch 58/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7652 - loss: 0.6607 - val_accuracy: 0.6704 - val_loss: 0.9997\n",
      "Epoch 59/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7693 - loss: 0.6573 - val_accuracy: 0.6659 - val_loss: 1.0554\n",
      "Epoch 60/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7403 - loss: 0.7348 - val_accuracy: 0.6482 - val_loss: 1.0432\n",
      "Epoch 61/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7615 - loss: 0.6673 - val_accuracy: 0.6792 - val_loss: 0.9686\n",
      "Epoch 62/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7968 - loss: 0.6142 - val_accuracy: 0.5752 - val_loss: 1.3440\n",
      "Epoch 63/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7668 - loss: 0.6846 - val_accuracy: 0.6692 - val_loss: 1.0070\n",
      "Epoch 64/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7682 - loss: 0.6691 - val_accuracy: 0.6549 - val_loss: 1.0283\n",
      "Epoch 65/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7849 - loss: 0.5983 - val_accuracy: 0.6869 - val_loss: 0.9590\n",
      "Epoch 66/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8124 - loss: 0.5512 - val_accuracy: 0.6615 - val_loss: 1.0033\n",
      "Epoch 67/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8062 - loss: 0.5982 - val_accuracy: 0.6460 - val_loss: 1.0548\n",
      "Epoch 68/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8235 - loss: 0.5341 - val_accuracy: 0.7058 - val_loss: 0.9227\n",
      "Epoch 69/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7945 - loss: 0.5767 - val_accuracy: 0.6726 - val_loss: 0.9716\n",
      "Epoch 70/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7924 - loss: 0.6266 - val_accuracy: 0.6305 - val_loss: 1.0887\n",
      "Epoch 71/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7980 - loss: 0.5735 - val_accuracy: 0.6770 - val_loss: 1.0160\n",
      "Epoch 72/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8013 - loss: 0.5665 - val_accuracy: 0.6338 - val_loss: 1.1353\n",
      "Epoch 73/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8266 - loss: 0.5106 - val_accuracy: 0.6825 - val_loss: 0.9692\n",
      "Epoch 74/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8307 - loss: 0.5025 - val_accuracy: 0.6969 - val_loss: 0.9545\n",
      "Epoch 75/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8298 - loss: 0.5114 - val_accuracy: 0.7168 - val_loss: 0.8811\n",
      "Epoch 76/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8433 - loss: 0.4665 - val_accuracy: 0.6825 - val_loss: 0.9445\n",
      "Epoch 77/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8397 - loss: 0.4545 - val_accuracy: 0.7135 - val_loss: 0.8976\n",
      "Epoch 78/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8523 - loss: 0.4395 - val_accuracy: 0.7212 - val_loss: 0.8620\n",
      "Epoch 79/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8380 - loss: 0.4687 - val_accuracy: 0.7135 - val_loss: 0.9492\n",
      "Epoch 80/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8165 - loss: 0.5532 - val_accuracy: 0.6836 - val_loss: 0.9864\n",
      "Epoch 81/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8390 - loss: 0.4613 - val_accuracy: 0.7279 - val_loss: 0.8496\n",
      "Epoch 82/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8124 - loss: 0.5203 - val_accuracy: 0.7323 - val_loss: 0.8646\n",
      "Epoch 83/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8652 - loss: 0.4078 - val_accuracy: 0.6162 - val_loss: 1.3349\n",
      "Epoch 84/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8169 - loss: 0.5139 - val_accuracy: 0.7212 - val_loss: 0.9102\n",
      "Epoch 85/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8654 - loss: 0.4020 - val_accuracy: 0.6814 - val_loss: 1.0081\n",
      "Epoch 86/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8490 - loss: 0.4369 - val_accuracy: 0.6770 - val_loss: 1.0189\n",
      "Epoch 87/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8457 - loss: 0.4462 - val_accuracy: 0.7290 - val_loss: 0.8734\n",
      "Epoch 88/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8689 - loss: 0.3718 - val_accuracy: 0.7201 - val_loss: 0.9496\n",
      "Epoch 89/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8656 - loss: 0.3848 - val_accuracy: 0.6858 - val_loss: 1.0074\n",
      "Epoch 90/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8836 - loss: 0.3454 - val_accuracy: 0.7257 - val_loss: 0.8517\n",
      "Epoch 91/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8806 - loss: 0.3671 - val_accuracy: 0.6969 - val_loss: 0.9838\n",
      "Epoch 92/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8884 - loss: 0.3316 - val_accuracy: 0.7400 - val_loss: 0.8553\n",
      "Epoch 93/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8712 - loss: 0.3611 - val_accuracy: 0.7345 - val_loss: 0.8841\n",
      "Epoch 94/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8873 - loss: 0.3228 - val_accuracy: 0.7323 - val_loss: 0.8985\n",
      "Epoch 95/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9000 - loss: 0.3068 - val_accuracy: 0.7345 - val_loss: 0.8847\n",
      "Epoch 96/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8843 - loss: 0.3432 - val_accuracy: 0.6626 - val_loss: 1.1527\n",
      "Epoch 97/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8461 - loss: 0.4396 - val_accuracy: 0.6626 - val_loss: 1.1509\n",
      "Epoch 98/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8586 - loss: 0.4020 - val_accuracy: 0.7268 - val_loss: 0.9215\n",
      "Epoch 99/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8898 - loss: 0.3211 - val_accuracy: 0.7400 - val_loss: 0.8455\n",
      "Epoch 100/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8990 - loss: 0.3019 - val_accuracy: 0.7235 - val_loss: 0.9042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16147a9fed0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_encoded, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7401 - loss: 0.9165\n",
      "Test Loss: 0.8954323530197144, Test Accuracy: 0.7398229837417603\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       104\n",
      "           1       0.85      0.87      0.86        95\n",
      "           2       0.59      0.64      0.61        91\n",
      "           3       0.73      0.70      0.72        74\n",
      "           4       0.68      0.73      0.70        77\n",
      "           5       0.87      0.98      0.92        91\n",
      "           6       0.54      0.79      0.64        33\n",
      "           7       0.69      0.56      0.62        78\n",
      "           8       0.66      0.65      0.66        78\n",
      "           9       0.91      0.74      0.82        27\n",
      "          10       0.40      0.50      0.45        42\n",
      "          11       0.62      0.88      0.73        17\n",
      "          12       0.83      0.76      0.79        25\n",
      "          13       0.80      0.44      0.57         9\n",
      "          14       0.81      0.60      0.69        91\n",
      "          15       0.87      0.74      0.80        89\n",
      "          16       0.77      0.78      0.78       109\n",
      "\n",
      "    accuracy                           0.74      1130\n",
      "   macro avg       0.73      0.72      0.72      1130\n",
      "weighted avg       0.75      0.74      0.74      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# with open('lstmmodel.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "model.save('lstmmodel.h5')\n",
    "tf.saved_model.save(model, 'saved_model')\n",
    "# pip install openvino-dev[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lstmmodel.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstmmodel.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel):\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lstmmodel.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('lstmmodel.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "if isinstance(model, tf.keras.Model):\n",
    "    model.save('lstmmodel.h5')\n",
    "    \n",
    "    tf.saved_model.save(model, 'saved_model')\n",
    "    # model.save('path_to_your_model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from openvino.runtime import Core, serialize\n",
    "\n",
    "# Convert to IR (example with quantization)\n",
    "mo_cmd = f\"mo --saved_model_dir saved_model --input_shape '[1, 20, 199]' --output_dir ir --data_type FP16\"\n",
    "import subprocess\n",
    "subprocess.call(mo_cmd, shell=True)\n",
    "\n",
    "# convert a saved model to IR format using the Model Optimizer in python through core\n",
    "core = Core()\n",
    "model = core.read_model(\"saved_model\")\n",
    "compiled_model = core.compile_model(model, \"CPU\")\n",
    "serialize(model, xml_path=\"ir/intel_model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Output: names[inputs] shape[?,20,82] type: f32>]\n",
      "[<Output: names[output_0] shape[?,17] type: f32>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_xml = \"ir/intel_model.xml\"\n",
    "model = core.read_model(model=model_xml)\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IR model and compile for target device\n",
    "core = Core()\n",
    "model_ = core.read_model(\"ir/intel_model.xml\")\n",
    "compiled_model = core.compile_model(model, \"CPU\")\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single input models only\n",
    "result = compiled_model(input_data)[output_layer]\n",
    "\n",
    "# for multiple inputs in a list\n",
    "result = compiled_model([input_data])[output_layer]\n",
    "\n",
    "# or using a dictionary, where the key is input tensor name or index\n",
    "result = compiled_model({input_layer.any_name: input_data})[output_layer]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
