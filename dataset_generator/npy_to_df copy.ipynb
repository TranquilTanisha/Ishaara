{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y24</th>\n",
       "      <th>Z24</th>\n",
       "      <th>X25</th>\n",
       "      <th>Y25</th>\n",
       "      <th>Z25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y26</th>\n",
       "      <th>Z26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530313</td>\n",
       "      <td>0.585678</td>\n",
       "      <td>-0.365724</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.130718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.441575</td>\n",
       "      <td>-10.572632</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>-0.500577</td>\n",
       "      <td>-12.161836</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>-0.622653</td>\n",
       "      <td>-11.623393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531883</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>-0.381889</td>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.241074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.754394</td>\n",
       "      <td>-7.421648</td>\n",
       "      <td>0.392176</td>\n",
       "      <td>-0.842538</td>\n",
       "      <td>-8.545950</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>-0.967975</td>\n",
       "      <td>-8.040165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.574829</td>\n",
       "      <td>-0.470731</td>\n",
       "      <td>0.376408</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.216554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.678109</td>\n",
       "      <td>-8.186996</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>-0.814776</td>\n",
       "      <td>-9.373079</td>\n",
       "      <td>0.455979</td>\n",
       "      <td>-0.910327</td>\n",
       "      <td>-8.907041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>-0.516997</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.629331</td>\n",
       "      <td>0.273374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.797789</td>\n",
       "      <td>-10.197847</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>-0.955783</td>\n",
       "      <td>-11.699729</td>\n",
       "      <td>0.451075</td>\n",
       "      <td>-1.066465</td>\n",
       "      <td>-11.020497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>0.384628</td>\n",
       "      <td>0.589920</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112924</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587871</td>\n",
       "      <td>9.669083</td>\n",
       "      <td>1.162361</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>11.412089</td>\n",
       "      <td>1.129761</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>10.942098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574938</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>-0.300268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112925</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>21.491173</td>\n",
       "      <td>1.587999</td>\n",
       "      <td>0.362523</td>\n",
       "      <td>25.172693</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>24.418745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578150</td>\n",
       "      <td>0.539364</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>0.398031</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>-0.181356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112926</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>30.405003</td>\n",
       "      <td>1.653764</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>35.272096</td>\n",
       "      <td>1.652273</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>34.866246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580092</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.398705</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>-0.237497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112927</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182174</td>\n",
       "      <td>30.964133</td>\n",
       "      <td>1.847807</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>36.075314</td>\n",
       "      <td>1.835613</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>35.360101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580864</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.538843</td>\n",
       "      <td>-0.243124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112928</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>0.140643</td>\n",
       "      <td>43.708800</td>\n",
       "      <td>2.127749</td>\n",
       "      <td>0.198996</td>\n",
       "      <td>50.094664</td>\n",
       "      <td>2.107137</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>49.969990</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585219</td>\n",
       "      <td>0.542360</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>0.397739</td>\n",
       "      <td>0.534382</td>\n",
       "      <td>-0.228005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112929 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frame       X00        Y00       Z00       X01        Y01  \\\n",
       "0       Boy      1 -3.320268  12.714561  0.223574 -3.320268  12.714561   \n",
       "1       Boy      2 -0.441575 -10.572632  0.370819 -0.500577 -12.161836   \n",
       "2       Boy      3 -0.754394  -7.421648  0.392176 -0.842538  -8.545950   \n",
       "3       Boy      4 -0.678109  -8.186996  0.441856 -0.814776  -9.373079   \n",
       "4       Boy      5 -0.797789 -10.197847  0.429872 -0.955783 -11.699729   \n",
       "...     ...    ...       ...        ...       ...       ...        ...   \n",
       "112924  You     16  0.587871   9.669083  1.162361  0.726044  11.412089   \n",
       "112925  You     17  0.337344  21.491173  1.587999  0.362523  25.172693   \n",
       "112926  You     18  0.215001  30.405003  1.653764  0.260286  35.272096   \n",
       "112927  You     19  0.182174  30.964133  1.847807  0.224303  36.075314   \n",
       "112928  You     20  0.140643  43.708800  2.127749  0.198996  50.094664   \n",
       "\n",
       "             Z01       X02        Y02  ...  Z23  X24  Y24  Z24       X25  \\\n",
       "0       0.223574 -3.320268  12.714561  ...  1.0  0.0  0.0  0.0  0.530313   \n",
       "1       0.379360 -0.622653 -11.623393  ...  1.0  0.0  0.0  0.0  0.531883   \n",
       "2       0.401212 -0.967975  -8.040165  ...  1.0  0.0  0.0  0.0  0.534591   \n",
       "3       0.455979 -0.910327  -8.907041  ...  1.0  0.0  0.0  0.0  0.484453   \n",
       "4       0.451075 -1.066465 -11.020497  ...  1.0  0.0  0.0  0.0  0.474471   \n",
       "...          ...       ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "112924  1.129761  0.655170  10.942098  ...  1.0  0.0  0.0  0.0  0.574938   \n",
       "112925  1.563665  0.306954  24.418745  ...  1.0  0.0  0.0  0.0  0.578150   \n",
       "112926  1.652273  0.141393  34.866246  ...  1.0  0.0  0.0  0.0  0.580092   \n",
       "112927  1.835613  0.095749  35.360101  ...  1.0  0.0  0.0  0.0  0.580864   \n",
       "112928  2.107137  0.070886  49.969990  ...  1.0  0.0  0.0  0.0  0.585219   \n",
       "\n",
       "             Y25       Z25       X26       Y26       Z26  \n",
       "0       0.585678 -0.365724  0.393299  0.607514  0.130718  \n",
       "1       0.588693 -0.381889  0.385701  0.626965  0.241074  \n",
       "2       0.574829 -0.470731  0.376408  0.645435  0.216554  \n",
       "3       0.453059 -0.516997  0.365139  0.629331  0.273374  \n",
       "4       0.342270 -0.453968  0.384628  0.589920  0.266067  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "112924  0.538055  0.070751  0.395394  0.452951 -0.300268  \n",
       "112925  0.539364  0.051295  0.398031  0.515749 -0.181356  \n",
       "112926  0.541042  0.027703  0.398705  0.534306 -0.237497  \n",
       "112927  0.542060  0.006164  0.398723  0.538843 -0.243124  \n",
       "112928  0.542360 -0.016540  0.397739  0.534382 -0.228005  \n",
       "\n",
       "[112929 rows x 83 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load each .npy file and convert the list of dictionaries to a DataFrame\n",
    "dataframes = []\n",
    "for file in os.listdir('dataset'):\n",
    "    if file.endswith('.npy'):\n",
    "        data = np.load('dataset/' + file, allow_pickle=True)\n",
    "        dataframes.append(pd.DataFrame.from_records(data))\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_length = merged_df['Frame'].max()\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_10832\\2834639197.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frame</th>\n",
       "      <th>X00</th>\n",
       "      <th>Y00</th>\n",
       "      <th>Z00</th>\n",
       "      <th>X01</th>\n",
       "      <th>Y01</th>\n",
       "      <th>Z01</th>\n",
       "      <th>X02</th>\n",
       "      <th>Y02</th>\n",
       "      <th>...</th>\n",
       "      <th>Z23</th>\n",
       "      <th>X24</th>\n",
       "      <th>Y24</th>\n",
       "      <th>Z24</th>\n",
       "      <th>X25</th>\n",
       "      <th>Y25</th>\n",
       "      <th>Z25</th>\n",
       "      <th>X26</th>\n",
       "      <th>Y26</th>\n",
       "      <th>Z26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boy</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>0.223574</td>\n",
       "      <td>-3.320268</td>\n",
       "      <td>12.714561</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530313</td>\n",
       "      <td>0.585678</td>\n",
       "      <td>-0.365724</td>\n",
       "      <td>0.393299</td>\n",
       "      <td>0.607514</td>\n",
       "      <td>0.130718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boy</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.441575</td>\n",
       "      <td>-10.572632</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>-0.500577</td>\n",
       "      <td>-12.161836</td>\n",
       "      <td>0.379360</td>\n",
       "      <td>-0.622653</td>\n",
       "      <td>-11.623393</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531883</td>\n",
       "      <td>0.588693</td>\n",
       "      <td>-0.381889</td>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.626965</td>\n",
       "      <td>0.241074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boy</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.754394</td>\n",
       "      <td>-7.421648</td>\n",
       "      <td>0.392176</td>\n",
       "      <td>-0.842538</td>\n",
       "      <td>-8.545950</td>\n",
       "      <td>0.401212</td>\n",
       "      <td>-0.967975</td>\n",
       "      <td>-8.040165</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534591</td>\n",
       "      <td>0.574829</td>\n",
       "      <td>-0.470731</td>\n",
       "      <td>0.376408</td>\n",
       "      <td>0.645435</td>\n",
       "      <td>0.216554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boy</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.678109</td>\n",
       "      <td>-8.186996</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>-0.814776</td>\n",
       "      <td>-9.373079</td>\n",
       "      <td>0.455979</td>\n",
       "      <td>-0.910327</td>\n",
       "      <td>-8.907041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484453</td>\n",
       "      <td>0.453059</td>\n",
       "      <td>-0.516997</td>\n",
       "      <td>0.365139</td>\n",
       "      <td>0.629331</td>\n",
       "      <td>0.273374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boy</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.797789</td>\n",
       "      <td>-10.197847</td>\n",
       "      <td>0.429872</td>\n",
       "      <td>-0.955783</td>\n",
       "      <td>-11.699729</td>\n",
       "      <td>0.451075</td>\n",
       "      <td>-1.066465</td>\n",
       "      <td>-11.020497</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.342270</td>\n",
       "      <td>-0.453968</td>\n",
       "      <td>0.384628</td>\n",
       "      <td>0.589920</td>\n",
       "      <td>0.266067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112924</th>\n",
       "      <td>You</td>\n",
       "      <td>16</td>\n",
       "      <td>0.587871</td>\n",
       "      <td>9.669083</td>\n",
       "      <td>1.162361</td>\n",
       "      <td>0.726044</td>\n",
       "      <td>11.412089</td>\n",
       "      <td>1.129761</td>\n",
       "      <td>0.655170</td>\n",
       "      <td>10.942098</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574938</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>0.070751</td>\n",
       "      <td>0.395394</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>-0.300268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112925</th>\n",
       "      <td>You</td>\n",
       "      <td>17</td>\n",
       "      <td>0.337344</td>\n",
       "      <td>21.491173</td>\n",
       "      <td>1.587999</td>\n",
       "      <td>0.362523</td>\n",
       "      <td>25.172693</td>\n",
       "      <td>1.563665</td>\n",
       "      <td>0.306954</td>\n",
       "      <td>24.418745</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578150</td>\n",
       "      <td>0.539364</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>0.398031</td>\n",
       "      <td>0.515749</td>\n",
       "      <td>-0.181356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112926</th>\n",
       "      <td>You</td>\n",
       "      <td>18</td>\n",
       "      <td>0.215001</td>\n",
       "      <td>30.405003</td>\n",
       "      <td>1.653764</td>\n",
       "      <td>0.260286</td>\n",
       "      <td>35.272096</td>\n",
       "      <td>1.652273</td>\n",
       "      <td>0.141393</td>\n",
       "      <td>34.866246</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580092</td>\n",
       "      <td>0.541042</td>\n",
       "      <td>0.027703</td>\n",
       "      <td>0.398705</td>\n",
       "      <td>0.534306</td>\n",
       "      <td>-0.237497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112927</th>\n",
       "      <td>You</td>\n",
       "      <td>19</td>\n",
       "      <td>0.182174</td>\n",
       "      <td>30.964133</td>\n",
       "      <td>1.847807</td>\n",
       "      <td>0.224303</td>\n",
       "      <td>36.075314</td>\n",
       "      <td>1.835613</td>\n",
       "      <td>0.095749</td>\n",
       "      <td>35.360101</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580864</td>\n",
       "      <td>0.542060</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.398723</td>\n",
       "      <td>0.538843</td>\n",
       "      <td>-0.243124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112928</th>\n",
       "      <td>You</td>\n",
       "      <td>20</td>\n",
       "      <td>0.140643</td>\n",
       "      <td>43.708800</td>\n",
       "      <td>2.127749</td>\n",
       "      <td>0.198996</td>\n",
       "      <td>50.094664</td>\n",
       "      <td>2.107137</td>\n",
       "      <td>0.070886</td>\n",
       "      <td>49.969990</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585219</td>\n",
       "      <td>0.542360</td>\n",
       "      <td>-0.016540</td>\n",
       "      <td>0.397739</td>\n",
       "      <td>0.534382</td>\n",
       "      <td>-0.228005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112929 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Frame       X00        Y00       Z00       X01        Y01  \\\n",
       "0       Boy      1 -3.320268  12.714561  0.223574 -3.320268  12.714561   \n",
       "1       Boy      2 -0.441575 -10.572632  0.370819 -0.500577 -12.161836   \n",
       "2       Boy      3 -0.754394  -7.421648  0.392176 -0.842538  -8.545950   \n",
       "3       Boy      4 -0.678109  -8.186996  0.441856 -0.814776  -9.373079   \n",
       "4       Boy      5 -0.797789 -10.197847  0.429872 -0.955783 -11.699729   \n",
       "...     ...    ...       ...        ...       ...       ...        ...   \n",
       "112924  You     16  0.587871   9.669083  1.162361  0.726044  11.412089   \n",
       "112925  You     17  0.337344  21.491173  1.587999  0.362523  25.172693   \n",
       "112926  You     18  0.215001  30.405003  1.653764  0.260286  35.272096   \n",
       "112927  You     19  0.182174  30.964133  1.847807  0.224303  36.075314   \n",
       "112928  You     20  0.140643  43.708800  2.127749  0.198996  50.094664   \n",
       "\n",
       "             Z01       X02        Y02  ...  Z23  X24  Y24  Z24       X25  \\\n",
       "0       0.223574 -3.320268  12.714561  ...  1.0  0.0  0.0  0.0  0.530313   \n",
       "1       0.379360 -0.622653 -11.623393  ...  1.0  0.0  0.0  0.0  0.531883   \n",
       "2       0.401212 -0.967975  -8.040165  ...  1.0  0.0  0.0  0.0  0.534591   \n",
       "3       0.455979 -0.910327  -8.907041  ...  1.0  0.0  0.0  0.0  0.484453   \n",
       "4       0.451075 -1.066465 -11.020497  ...  1.0  0.0  0.0  0.0  0.474471   \n",
       "...          ...       ...        ...  ...  ...  ...  ...  ...       ...   \n",
       "112924  1.129761  0.655170  10.942098  ...  1.0  0.0  0.0  0.0  0.574938   \n",
       "112925  1.563665  0.306954  24.418745  ...  1.0  0.0  0.0  0.0  0.578150   \n",
       "112926  1.652273  0.141393  34.866246  ...  1.0  0.0  0.0  0.0  0.580092   \n",
       "112927  1.835613  0.095749  35.360101  ...  1.0  0.0  0.0  0.0  0.580864   \n",
       "112928  2.107137  0.070886  49.969990  ...  1.0  0.0  0.0  0.0  0.585219   \n",
       "\n",
       "             Y25       Z25       X26       Y26       Z26  \n",
       "0       0.585678 -0.365724  0.393299  0.607514  0.130718  \n",
       "1       0.588693 -0.381889  0.385701  0.626965  0.241074  \n",
       "2       0.574829 -0.470731  0.376408  0.645435  0.216554  \n",
       "3       0.453059 -0.516997  0.365139  0.629331  0.273374  \n",
       "4       0.342270 -0.453968  0.384628  0.589920  0.266067  \n",
       "...          ...       ...       ...       ...       ...  \n",
       "112924  0.538055  0.070751  0.395394  0.452951 -0.300268  \n",
       "112925  0.539364  0.051295  0.398031  0.515749 -0.181356  \n",
       "112926  0.541042  0.027703  0.398705  0.534306 -0.237497  \n",
       "112927  0.542060  0.006164  0.398723  0.538843 -0.243124  \n",
       "112928  0.542360 -0.016540  0.397739  0.534382 -0.228005  \n",
       "\n",
       "[112929 rows x 83 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences(group):\n",
    "    # Calculate the number of padding rows needed\n",
    "    padding_rows = max_sequence_length - len(group)\n",
    "    \n",
    "    # Create a DataFrame with padding rows filled with NaN (or any other padding value)\n",
    "    padding_df = pd.DataFrame({\n",
    "        'Word': [group['Word'].iloc[0]] * padding_rows,\n",
    "        'Frame': np.arange(len(group) + 1, max_sequence_length + 1),\n",
    "    })\n",
    "    \n",
    "    # Concatenate the original group with the padding DataFrame\n",
    "    return pd.concat([group, padding_df], ignore_index=True)\n",
    "\n",
    "# Group the DataFrame by 'Word' and apply the padding function\n",
    "padded_df = merged_df.groupby('Word').apply(pad_sequences).reset_index(drop=True)\n",
    "padded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = padded_df.drop('Word', axis=1)\n",
    "target = padded_df['Word']\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "features_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# import pickle\n",
    "# with open('scaler.pkl', 'wb') as file:\n",
    "#     pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the interval\n",
    "# interval = 20\n",
    "\n",
    "# # Initialize lists to hold the training and testing sets\n",
    "# X_train_list = []\n",
    "# y_train_list = []\n",
    "# X_test_list = []\n",
    "# y_test_list = []\n",
    "\n",
    "# # Iterate over features_scaled with a step of 20\n",
    "# for i in range(0, len(padded_df) - interval, 3*interval):\n",
    "#     # Slice the dataset for the current interval\n",
    "#     X_train = pd.DataFrame(features_scaled[i:i+2*interval])\n",
    "#     y_train = pd.DataFrame(target[i:i+2*interval])\n",
    "\n",
    "#     X_test = pd.DataFrame(features_scaled[i+2*interval:i+3*interval])\n",
    "#     y_test = pd.DataFrame(target[i+2*interval:i+3*interval])\n",
    "\n",
    "#     # Append the sliced DataFrames to the lists\n",
    "#     X_train_list.append(X_train)\n",
    "#     y_train_list.append(y_train)\n",
    "#     X_test_list.append(X_test)\n",
    "#     y_test_list.append(y_test)\n",
    "\n",
    "# # Convert the lists to DataFrames\n",
    "# X_train_df = pd.concat(X_train_list)\n",
    "# y_train_df = pd.concat(y_train_list)\n",
    "# X_test_df = pd.concat(X_test_list)\n",
    "# y_test_df = pd.concat(y_test_list)\n",
    "\n",
    "# X_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. RandomForestClassifier expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1058\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m     )\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m   1064\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1065\u001b[0m         array,\n\u001b[0;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1069\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. RandomForestClassifier expected <= 2."
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # Initialize the model\n",
    "# clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Train the model\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = clf.predict(X_test_df)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(classification_report(y_test_df, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('rfmodel.pkl', 'wb') as file:\n",
    "#     pickle.dump(clf, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5646"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of time steps\n",
    "time_steps = 20\n",
    "\n",
    "# Prepare the data\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(0, len(features_scaled) - time_steps, time_steps):\n",
    "    X.append(features_scaled[i:i + time_steps])\n",
    "    y.append(target[i])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape X to fit the LSTM input shape: [samples, time steps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'y_train' is your target variable with categorical values\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = encoder.fit_transform(y_train)\n",
    "y_test_encoded = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 82\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "num_classes = len(encoder.classes_)\n",
    "print(X_train.shape[1], X_train.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(num_classes, activation='softmax')) # num_classes is the number of unique words you're predicting) # num_classes is the number of unique words you're predicting\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8407 - loss: 0.4660 - val_accuracy: 0.7124 - val_loss: 0.9510\n",
      "Epoch 2/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8488 - loss: 0.4496 - val_accuracy: 0.7157 - val_loss: 0.9562\n",
      "Epoch 3/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8699 - loss: 0.4094 - val_accuracy: 0.6825 - val_loss: 1.0614\n",
      "Epoch 4/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8620 - loss: 0.4257 - val_accuracy: 0.7035 - val_loss: 0.9994\n",
      "Epoch 5/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8470 - loss: 0.4330 - val_accuracy: 0.7002 - val_loss: 0.9562\n",
      "Epoch 6/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8293 - loss: 0.4993 - val_accuracy: 0.7190 - val_loss: 0.9325\n",
      "Epoch 7/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8786 - loss: 0.3739 - val_accuracy: 0.6980 - val_loss: 0.9738\n",
      "Epoch 8/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8415 - loss: 0.4534 - val_accuracy: 0.6604 - val_loss: 1.0939\n",
      "Epoch 9/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8316 - loss: 0.4924 - val_accuracy: 0.7124 - val_loss: 0.9432\n",
      "Epoch 10/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8695 - loss: 0.3853 - val_accuracy: 0.6781 - val_loss: 1.0323\n",
      "Epoch 11/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8641 - loss: 0.3907 - val_accuracy: 0.7080 - val_loss: 0.9434\n",
      "Epoch 12/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8838 - loss: 0.3467 - val_accuracy: 0.7212 - val_loss: 0.9392\n",
      "Epoch 13/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8875 - loss: 0.3437 - val_accuracy: 0.7002 - val_loss: 0.9995\n",
      "Epoch 14/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8911 - loss: 0.3389 - val_accuracy: 0.7235 - val_loss: 0.9379\n",
      "Epoch 15/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8962 - loss: 0.3078 - val_accuracy: 0.7046 - val_loss: 0.9819\n",
      "Epoch 16/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.8741 - loss: 0.3698 - val_accuracy: 0.6881 - val_loss: 1.0142\n",
      "Epoch 17/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8850 - loss: 0.3354 - val_accuracy: 0.7135 - val_loss: 1.0088\n",
      "Epoch 18/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8515 - loss: 0.4157 - val_accuracy: 0.6980 - val_loss: 1.0353\n",
      "Epoch 19/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8664 - loss: 0.3932 - val_accuracy: 0.7102 - val_loss: 0.9515\n",
      "Epoch 20/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8998 - loss: 0.2931 - val_accuracy: 0.6803 - val_loss: 1.1057\n",
      "Epoch 21/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8976 - loss: 0.3172 - val_accuracy: 0.7013 - val_loss: 0.9892\n",
      "Epoch 22/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.8910 - loss: 0.3244 - val_accuracy: 0.7146 - val_loss: 0.9956\n",
      "Epoch 23/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8819 - loss: 0.3363 - val_accuracy: 0.6294 - val_loss: 1.3219\n",
      "Epoch 24/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8774 - loss: 0.3570 - val_accuracy: 0.6958 - val_loss: 1.0038\n",
      "Epoch 25/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8472 - loss: 0.4226 - val_accuracy: 0.7257 - val_loss: 0.9526\n",
      "Epoch 26/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8841 - loss: 0.3571 - val_accuracy: 0.7412 - val_loss: 0.8786\n",
      "Epoch 27/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9222 - loss: 0.2502 - val_accuracy: 0.7412 - val_loss: 0.8902\n",
      "Epoch 28/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9300 - loss: 0.2378 - val_accuracy: 0.7035 - val_loss: 1.0421\n",
      "Epoch 29/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.9131 - loss: 0.2895 - val_accuracy: 0.6670 - val_loss: 1.1582\n",
      "Epoch 30/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8637 - loss: 0.3708 - val_accuracy: 0.7301 - val_loss: 0.9537\n",
      "Epoch 31/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9219 - loss: 0.2566 - val_accuracy: 0.7345 - val_loss: 0.9322\n",
      "Epoch 32/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9031 - loss: 0.2917 - val_accuracy: 0.7235 - val_loss: 1.0274\n",
      "Epoch 33/33\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9108 - loss: 0.2659 - val_accuracy: 0.7124 - val_loss: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x161467fff10>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_encoded, epochs=33, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7168 - loss: 1.0735\n",
      "Test Loss: 1.0156817436218262, Test Accuracy: 0.7176991105079651\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       104\n",
      "           1       0.82      0.86      0.84        95\n",
      "           2       0.61      0.58      0.60        91\n",
      "           3       0.65      0.70      0.68        74\n",
      "           4       0.62      0.73      0.67        77\n",
      "           5       0.89      0.96      0.92        91\n",
      "           6       0.55      0.82      0.66        33\n",
      "           7       0.63      0.54      0.58        78\n",
      "           8       0.64      0.60      0.62        78\n",
      "           9       0.92      0.85      0.88        27\n",
      "          10       0.42      0.48      0.44        42\n",
      "          11       0.93      0.76      0.84        17\n",
      "          12       0.87      0.80      0.83        25\n",
      "          13       0.58      0.78      0.67         9\n",
      "          14       0.66      0.70      0.68        91\n",
      "          15       0.84      0.70      0.76        89\n",
      "          16       0.74      0.64      0.69       109\n",
      "\n",
      "    accuracy                           0.72      1130\n",
      "   macro avg       0.72      0.73      0.72      1130\n",
      "weighted avg       0.73      0.72      0.72      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(num_classes, activation='softmax')) # num_classes is the number of unique words you're predicting) # num_classes is the number of unique words you're predicting\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.0746 - loss: 2.7674 - val_accuracy: 0.0874 - val_loss: 2.7068\n",
      "Epoch 2/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0780 - loss: 2.7206 - val_accuracy: 0.0796 - val_loss: 2.7124\n",
      "Epoch 3/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0766 - loss: 2.7258 - val_accuracy: 0.1095 - val_loss: 2.7180\n",
      "Epoch 4/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0831 - loss: 2.7288 - val_accuracy: 0.0962 - val_loss: 2.7074\n",
      "Epoch 5/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.0919 - loss: 2.7110 - val_accuracy: 0.1162 - val_loss: 2.6383\n",
      "Epoch 6/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.1170 - loss: 2.6468 - val_accuracy: 0.1538 - val_loss: 2.5534\n",
      "Epoch 7/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.1395 - loss: 2.5264 - val_accuracy: 0.1958 - val_loss: 2.3855\n",
      "Epoch 8/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.1814 - loss: 2.4168 - val_accuracy: 0.1903 - val_loss: 2.4067\n",
      "Epoch 9/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.2251 - loss: 2.2826 - val_accuracy: 0.1449 - val_loss: 2.6644\n",
      "Epoch 10/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2505 - loss: 2.2117 - val_accuracy: 0.2677 - val_loss: 2.0962\n",
      "Epoch 11/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.2759 - loss: 2.0761 - val_accuracy: 0.2832 - val_loss: 2.0420\n",
      "Epoch 12/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.2923 - loss: 2.0112 - val_accuracy: 0.2345 - val_loss: 2.2270\n",
      "Epoch 13/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3020 - loss: 2.0019 - val_accuracy: 0.3374 - val_loss: 1.9259\n",
      "Epoch 14/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3302 - loss: 1.9420 - val_accuracy: 0.3274 - val_loss: 1.9096\n",
      "Epoch 15/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3257 - loss: 1.9238 - val_accuracy: 0.3518 - val_loss: 1.9006\n",
      "Epoch 16/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3774 - loss: 1.8090 - val_accuracy: 0.3252 - val_loss: 1.9378\n",
      "Epoch 17/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3913 - loss: 1.7757 - val_accuracy: 0.3440 - val_loss: 1.8340\n",
      "Epoch 18/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3697 - loss: 1.7896 - val_accuracy: 0.4192 - val_loss: 1.7280\n",
      "Epoch 19/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3776 - loss: 1.7612 - val_accuracy: 0.2954 - val_loss: 2.1224\n",
      "Epoch 20/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.3766 - loss: 1.7741 - val_accuracy: 0.2898 - val_loss: 2.0121\n",
      "Epoch 21/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4157 - loss: 1.6837 - val_accuracy: 0.3960 - val_loss: 1.7093\n",
      "Epoch 22/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4573 - loss: 1.5947 - val_accuracy: 0.4159 - val_loss: 1.6353\n",
      "Epoch 23/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4703 - loss: 1.4970 - val_accuracy: 0.4181 - val_loss: 1.6964\n",
      "Epoch 24/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4360 - loss: 1.5793 - val_accuracy: 0.4668 - val_loss: 1.5465\n",
      "Epoch 25/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.4993 - loss: 1.4402 - val_accuracy: 0.4735 - val_loss: 1.5225\n",
      "Epoch 26/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5029 - loss: 1.4315 - val_accuracy: 0.4934 - val_loss: 1.4616\n",
      "Epoch 27/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5315 - loss: 1.3511 - val_accuracy: 0.4912 - val_loss: 1.4700\n",
      "Epoch 28/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5408 - loss: 1.3520 - val_accuracy: 0.5221 - val_loss: 1.3876\n",
      "Epoch 29/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5565 - loss: 1.3025 - val_accuracy: 0.5243 - val_loss: 1.3700\n",
      "Epoch 30/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5869 - loss: 1.2128 - val_accuracy: 0.4635 - val_loss: 1.5902\n",
      "Epoch 31/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5733 - loss: 1.2290 - val_accuracy: 0.5254 - val_loss: 1.3975\n",
      "Epoch 32/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5889 - loss: 1.1635 - val_accuracy: 0.5608 - val_loss: 1.2830\n",
      "Epoch 33/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5905 - loss: 1.2074 - val_accuracy: 0.4679 - val_loss: 1.5254\n",
      "Epoch 34/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6039 - loss: 1.1442 - val_accuracy: 0.5044 - val_loss: 1.4943\n",
      "Epoch 35/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6013 - loss: 1.1552 - val_accuracy: 0.5597 - val_loss: 1.2911\n",
      "Epoch 36/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6117 - loss: 1.1083 - val_accuracy: 0.5785 - val_loss: 1.2350\n",
      "Epoch 37/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6279 - loss: 1.0693 - val_accuracy: 0.5520 - val_loss: 1.3308\n",
      "Epoch 38/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6300 - loss: 1.0213 - val_accuracy: 0.5619 - val_loss: 1.2542\n",
      "Epoch 39/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6458 - loss: 1.0228 - val_accuracy: 0.6228 - val_loss: 1.1424\n",
      "Epoch 40/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6675 - loss: 0.9532 - val_accuracy: 0.5863 - val_loss: 1.1585\n",
      "Epoch 41/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.6529 - loss: 0.9697 - val_accuracy: 0.5819 - val_loss: 1.2233\n",
      "Epoch 42/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6768 - loss: 0.9373 - val_accuracy: 0.6272 - val_loss: 1.0905\n",
      "Epoch 43/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7007 - loss: 0.8667 - val_accuracy: 0.5885 - val_loss: 1.2106\n",
      "Epoch 44/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6818 - loss: 0.9037 - val_accuracy: 0.6305 - val_loss: 1.1432\n",
      "Epoch 45/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7073 - loss: 0.8619 - val_accuracy: 0.6394 - val_loss: 1.0504\n",
      "Epoch 46/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7194 - loss: 0.8360 - val_accuracy: 0.6283 - val_loss: 1.0651\n",
      "Epoch 47/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6894 - loss: 0.8803 - val_accuracy: 0.6217 - val_loss: 1.0943\n",
      "Epoch 48/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7104 - loss: 0.8367 - val_accuracy: 0.6372 - val_loss: 1.0610\n",
      "Epoch 49/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.6938 - loss: 0.8579 - val_accuracy: 0.6184 - val_loss: 1.1739\n",
      "Epoch 50/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7048 - loss: 0.8680 - val_accuracy: 0.5553 - val_loss: 1.3535\n",
      "Epoch 51/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.7191 - loss: 0.8374 - val_accuracy: 0.5940 - val_loss: 1.1856\n",
      "Epoch 52/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7360 - loss: 0.7742 - val_accuracy: 0.6493 - val_loss: 1.0385\n",
      "Epoch 53/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7535 - loss: 0.6996 - val_accuracy: 0.6626 - val_loss: 1.0365\n",
      "Epoch 54/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7693 - loss: 0.6821 - val_accuracy: 0.6316 - val_loss: 1.1017\n",
      "Epoch 55/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7771 - loss: 0.7023 - val_accuracy: 0.6482 - val_loss: 1.0415\n",
      "Epoch 56/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7297 - loss: 0.7663 - val_accuracy: 0.6814 - val_loss: 1.0029\n",
      "Epoch 57/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7695 - loss: 0.6625 - val_accuracy: 0.6659 - val_loss: 0.9970\n",
      "Epoch 58/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7652 - loss: 0.6607 - val_accuracy: 0.6704 - val_loss: 0.9997\n",
      "Epoch 59/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7693 - loss: 0.6573 - val_accuracy: 0.6659 - val_loss: 1.0554\n",
      "Epoch 60/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7403 - loss: 0.7348 - val_accuracy: 0.6482 - val_loss: 1.0432\n",
      "Epoch 61/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7615 - loss: 0.6673 - val_accuracy: 0.6792 - val_loss: 0.9686\n",
      "Epoch 62/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7968 - loss: 0.6142 - val_accuracy: 0.5752 - val_loss: 1.3440\n",
      "Epoch 63/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7668 - loss: 0.6846 - val_accuracy: 0.6692 - val_loss: 1.0070\n",
      "Epoch 64/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7682 - loss: 0.6691 - val_accuracy: 0.6549 - val_loss: 1.0283\n",
      "Epoch 65/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7849 - loss: 0.5983 - val_accuracy: 0.6869 - val_loss: 0.9590\n",
      "Epoch 66/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8124 - loss: 0.5512 - val_accuracy: 0.6615 - val_loss: 1.0033\n",
      "Epoch 67/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8062 - loss: 0.5982 - val_accuracy: 0.6460 - val_loss: 1.0548\n",
      "Epoch 68/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8235 - loss: 0.5341 - val_accuracy: 0.7058 - val_loss: 0.9227\n",
      "Epoch 69/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7945 - loss: 0.5767 - val_accuracy: 0.6726 - val_loss: 0.9716\n",
      "Epoch 70/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7924 - loss: 0.6266 - val_accuracy: 0.6305 - val_loss: 1.0887\n",
      "Epoch 71/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7980 - loss: 0.5735 - val_accuracy: 0.6770 - val_loss: 1.0160\n",
      "Epoch 72/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8013 - loss: 0.5665 - val_accuracy: 0.6338 - val_loss: 1.1353\n",
      "Epoch 73/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8266 - loss: 0.5106 - val_accuracy: 0.6825 - val_loss: 0.9692\n",
      "Epoch 74/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8307 - loss: 0.5025 - val_accuracy: 0.6969 - val_loss: 0.9545\n",
      "Epoch 75/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8298 - loss: 0.5114 - val_accuracy: 0.7168 - val_loss: 0.8811\n",
      "Epoch 76/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8433 - loss: 0.4665 - val_accuracy: 0.6825 - val_loss: 0.9445\n",
      "Epoch 77/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8397 - loss: 0.4545 - val_accuracy: 0.7135 - val_loss: 0.8976\n",
      "Epoch 78/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8523 - loss: 0.4395 - val_accuracy: 0.7212 - val_loss: 0.8620\n",
      "Epoch 79/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8380 - loss: 0.4687 - val_accuracy: 0.7135 - val_loss: 0.9492\n",
      "Epoch 80/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8165 - loss: 0.5532 - val_accuracy: 0.6836 - val_loss: 0.9864\n",
      "Epoch 81/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8390 - loss: 0.4613 - val_accuracy: 0.7279 - val_loss: 0.8496\n",
      "Epoch 82/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8124 - loss: 0.5203 - val_accuracy: 0.7323 - val_loss: 0.8646\n",
      "Epoch 83/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8652 - loss: 0.4078 - val_accuracy: 0.6162 - val_loss: 1.3349\n",
      "Epoch 84/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8169 - loss: 0.5139 - val_accuracy: 0.7212 - val_loss: 0.9102\n",
      "Epoch 85/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8654 - loss: 0.4020 - val_accuracy: 0.6814 - val_loss: 1.0081\n",
      "Epoch 86/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8490 - loss: 0.4369 - val_accuracy: 0.6770 - val_loss: 1.0189\n",
      "Epoch 87/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8457 - loss: 0.4462 - val_accuracy: 0.7290 - val_loss: 0.8734\n",
      "Epoch 88/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8689 - loss: 0.3718 - val_accuracy: 0.7201 - val_loss: 0.9496\n",
      "Epoch 89/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8656 - loss: 0.3848 - val_accuracy: 0.6858 - val_loss: 1.0074\n",
      "Epoch 90/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8836 - loss: 0.3454 - val_accuracy: 0.7257 - val_loss: 0.8517\n",
      "Epoch 91/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8806 - loss: 0.3671 - val_accuracy: 0.6969 - val_loss: 0.9838\n",
      "Epoch 92/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8884 - loss: 0.3316 - val_accuracy: 0.7400 - val_loss: 0.8553\n",
      "Epoch 93/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8712 - loss: 0.3611 - val_accuracy: 0.7345 - val_loss: 0.8841\n",
      "Epoch 94/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8873 - loss: 0.3228 - val_accuracy: 0.7323 - val_loss: 0.8985\n",
      "Epoch 95/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9000 - loss: 0.3068 - val_accuracy: 0.7345 - val_loss: 0.8847\n",
      "Epoch 96/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8843 - loss: 0.3432 - val_accuracy: 0.6626 - val_loss: 1.1527\n",
      "Epoch 97/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8461 - loss: 0.4396 - val_accuracy: 0.6626 - val_loss: 1.1509\n",
      "Epoch 98/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8586 - loss: 0.4020 - val_accuracy: 0.7268 - val_loss: 0.9215\n",
      "Epoch 99/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.8898 - loss: 0.3211 - val_accuracy: 0.7400 - val_loss: 0.8455\n",
      "Epoch 100/100\n",
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8990 - loss: 0.3019 - val_accuracy: 0.7235 - val_loss: 0.9042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16147a9fed0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_encoded, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7401 - loss: 0.9165\n",
      "Test Loss: 0.8954323530197144, Test Accuracy: 0.7398229837417603\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       104\n",
      "           1       0.85      0.87      0.86        95\n",
      "           2       0.59      0.64      0.61        91\n",
      "           3       0.73      0.70      0.72        74\n",
      "           4       0.68      0.73      0.70        77\n",
      "           5       0.87      0.98      0.92        91\n",
      "           6       0.54      0.79      0.64        33\n",
      "           7       0.69      0.56      0.62        78\n",
      "           8       0.66      0.65      0.66        78\n",
      "           9       0.91      0.74      0.82        27\n",
      "          10       0.40      0.50      0.45        42\n",
      "          11       0.62      0.88      0.73        17\n",
      "          12       0.83      0.76      0.79        25\n",
      "          13       0.80      0.44      0.57         9\n",
      "          14       0.81      0.60      0.69        91\n",
      "          15       0.87      0.74      0.80        89\n",
      "          16       0.77      0.78      0.78       109\n",
      "\n",
      "    accuracy                           0.74      1130\n",
      "   macro avg       0.73      0.72      0.72      1130\n",
      "weighted avg       0.75      0.74      0.74      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',    # Monitor validation loss\n",
    "    patience=20,            # Stop after 5 epochs with no improvement\n",
    "    restore_best_weights=True  # Restore the best weights at the end\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(64))\n",
    "# model.add(LSTM(16))\n",
    "model.add(Dense(num_classes, activation='softmax')) # num_classes is the number of unique words you're predicting) # num_classes is the number of unique words you're predicting\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.0756 - loss: 2.7640 - val_accuracy: 0.1018 - val_loss: 2.7149\n",
      "Epoch 2/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0876 - loss: 2.7303 - val_accuracy: 0.1018 - val_loss: 2.7038\n",
      "Epoch 3/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0795 - loss: 2.7293 - val_accuracy: 0.0560 - val_loss: 2.7161\n",
      "Epoch 4/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0764 - loss: 2.7284 - val_accuracy: 0.0870 - val_loss: 2.7027\n",
      "Epoch 5/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0815 - loss: 2.7244 - val_accuracy: 0.0708 - val_loss: 2.7017\n",
      "Epoch 6/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0838 - loss: 2.7061 - val_accuracy: 0.1224 - val_loss: 2.7057\n",
      "Epoch 7/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1247 - loss: 2.6283 - val_accuracy: 0.0959 - val_loss: 2.5440\n",
      "Epoch 8/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1671 - loss: 2.4514 - val_accuracy: 0.2242 - val_loss: 2.3538\n",
      "Epoch 9/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2196 - loss: 2.3163 - val_accuracy: 0.2743 - val_loss: 2.1510\n",
      "Epoch 10/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.2541 - loss: 2.1855 - val_accuracy: 0.3112 - val_loss: 2.1079\n",
      "Epoch 11/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2693 - loss: 2.1146 - val_accuracy: 0.2419 - val_loss: 2.1441\n",
      "Epoch 12/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.3092 - loss: 2.0307 - val_accuracy: 0.3230 - val_loss: 1.9618\n",
      "Epoch 13/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3511 - loss: 1.9237 - val_accuracy: 0.3569 - val_loss: 1.9518\n",
      "Epoch 14/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3441 - loss: 1.9409 - val_accuracy: 0.3599 - val_loss: 1.9092\n",
      "Epoch 15/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3621 - loss: 1.9119 - val_accuracy: 0.2463 - val_loss: 2.1839\n",
      "Epoch 16/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3252 - loss: 2.0347 - val_accuracy: 0.4145 - val_loss: 1.8580\n",
      "Epoch 17/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3839 - loss: 1.8435 - val_accuracy: 0.3540 - val_loss: 1.8097\n",
      "Epoch 18/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4009 - loss: 1.7460 - val_accuracy: 0.4086 - val_loss: 1.7477\n",
      "Epoch 19/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4356 - loss: 1.7107 - val_accuracy: 0.3451 - val_loss: 1.8779\n",
      "Epoch 20/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4050 - loss: 1.7426 - val_accuracy: 0.4499 - val_loss: 1.6065\n",
      "Epoch 21/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.4738 - loss: 1.5723 - val_accuracy: 0.3805 - val_loss: 1.8052\n",
      "Epoch 22/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.4307 - loss: 1.6507 - val_accuracy: 0.4528 - val_loss: 1.5978\n",
      "Epoch 23/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.4681 - loss: 1.5553 - val_accuracy: 0.4646 - val_loss: 1.5509\n",
      "Epoch 24/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.4799 - loss: 1.5137 - val_accuracy: 0.4779 - val_loss: 1.4800\n",
      "Epoch 25/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5205 - loss: 1.4171 - val_accuracy: 0.4484 - val_loss: 1.5334\n",
      "Epoch 26/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.4984 - loss: 1.4674 - val_accuracy: 0.4867 - val_loss: 1.5180\n",
      "Epoch 27/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.4896 - loss: 1.4853 - val_accuracy: 0.5118 - val_loss: 1.4427\n",
      "Epoch 28/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5234 - loss: 1.4136 - val_accuracy: 0.4233 - val_loss: 1.7145\n",
      "Epoch 29/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5444 - loss: 1.3330 - val_accuracy: 0.5383 - val_loss: 1.3581\n",
      "Epoch 30/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5888 - loss: 1.2503 - val_accuracy: 0.4808 - val_loss: 1.5348\n",
      "Epoch 31/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5838 - loss: 1.2448 - val_accuracy: 0.4484 - val_loss: 1.6625\n",
      "Epoch 32/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5655 - loss: 1.2731 - val_accuracy: 0.5457 - val_loss: 1.3017\n",
      "Epoch 33/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5915 - loss: 1.1930 - val_accuracy: 0.5251 - val_loss: 1.3989\n",
      "Epoch 34/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6089 - loss: 1.1628 - val_accuracy: 0.5487 - val_loss: 1.2928\n",
      "Epoch 35/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6230 - loss: 1.1190 - val_accuracy: 0.5796 - val_loss: 1.1993\n",
      "Epoch 36/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6218 - loss: 1.1190 - val_accuracy: 0.5457 - val_loss: 1.2736\n",
      "Epoch 37/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6180 - loss: 1.1045 - val_accuracy: 0.5398 - val_loss: 1.2811\n",
      "Epoch 38/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6261 - loss: 1.1004 - val_accuracy: 0.5959 - val_loss: 1.2277\n",
      "Epoch 39/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6259 - loss: 1.0794 - val_accuracy: 0.5487 - val_loss: 1.2996\n",
      "Epoch 40/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6243 - loss: 1.1315 - val_accuracy: 0.5944 - val_loss: 1.2141\n",
      "Epoch 41/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6573 - loss: 1.0320 - val_accuracy: 0.5944 - val_loss: 1.2291\n",
      "Epoch 42/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6589 - loss: 1.0238 - val_accuracy: 0.5826 - val_loss: 1.1832\n",
      "Epoch 43/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6628 - loss: 1.0006 - val_accuracy: 0.6150 - val_loss: 1.1129\n",
      "Epoch 44/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6748 - loss: 0.9651 - val_accuracy: 0.5767 - val_loss: 1.2215\n",
      "Epoch 45/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6668 - loss: 0.9949 - val_accuracy: 0.5059 - val_loss: 1.4924\n",
      "Epoch 46/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6542 - loss: 1.0009 - val_accuracy: 0.5590 - val_loss: 1.2398\n",
      "Epoch 47/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6569 - loss: 0.9843 - val_accuracy: 0.6077 - val_loss: 1.1356\n",
      "Epoch 48/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7080 - loss: 0.8629 - val_accuracy: 0.6032 - val_loss: 1.1312\n",
      "Epoch 49/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6833 - loss: 0.9250 - val_accuracy: 0.5929 - val_loss: 1.1841\n",
      "Epoch 50/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6940 - loss: 0.9009 - val_accuracy: 0.6077 - val_loss: 1.1248\n",
      "Epoch 51/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6925 - loss: 0.8713 - val_accuracy: 0.6534 - val_loss: 1.0267\n",
      "Epoch 52/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6862 - loss: 0.9283 - val_accuracy: 0.6504 - val_loss: 1.0214\n",
      "Epoch 53/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7117 - loss: 0.8723 - val_accuracy: 0.6475 - val_loss: 1.0738\n",
      "Epoch 54/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7049 - loss: 0.8808 - val_accuracy: 0.6460 - val_loss: 1.0284\n",
      "Epoch 55/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7308 - loss: 0.7998 - val_accuracy: 0.6475 - val_loss: 1.0015\n",
      "Epoch 56/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7369 - loss: 0.7810 - val_accuracy: 0.6578 - val_loss: 1.0227\n",
      "Epoch 57/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7184 - loss: 0.8132 - val_accuracy: 0.6283 - val_loss: 1.0562\n",
      "Epoch 58/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7261 - loss: 0.8008 - val_accuracy: 0.6342 - val_loss: 1.0427\n",
      "Epoch 59/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7408 - loss: 0.7938 - val_accuracy: 0.6593 - val_loss: 0.9831\n",
      "Epoch 60/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7283 - loss: 0.7998 - val_accuracy: 0.6386 - val_loss: 1.0731\n",
      "Epoch 61/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7386 - loss: 0.7639 - val_accuracy: 0.6254 - val_loss: 1.0679\n",
      "Epoch 62/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7393 - loss: 0.7553 - val_accuracy: 0.5944 - val_loss: 1.1759\n",
      "Epoch 63/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7160 - loss: 0.8454 - val_accuracy: 0.6401 - val_loss: 1.0549\n",
      "Epoch 64/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7479 - loss: 0.7495 - val_accuracy: 0.6549 - val_loss: 1.0804\n",
      "Epoch 65/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7407 - loss: 0.7677 - val_accuracy: 0.6460 - val_loss: 1.0406\n",
      "Epoch 66/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7517 - loss: 0.7249 - val_accuracy: 0.6062 - val_loss: 1.1005\n",
      "Epoch 67/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7633 - loss: 0.6895 - val_accuracy: 0.6475 - val_loss: 1.0318\n",
      "Epoch 68/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7749 - loss: 0.6802 - val_accuracy: 0.6180 - val_loss: 1.1220\n",
      "Epoch 69/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7808 - loss: 0.6839 - val_accuracy: 0.6401 - val_loss: 1.0185\n",
      "Epoch 70/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.7676 - loss: 0.6595 - val_accuracy: 0.6917 - val_loss: 0.9467\n",
      "Epoch 71/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7705 - loss: 0.6535 - val_accuracy: 0.6858 - val_loss: 0.9607\n",
      "Epoch 72/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7624 - loss: 0.6798 - val_accuracy: 0.6593 - val_loss: 0.9875\n",
      "Epoch 73/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7817 - loss: 0.6594 - val_accuracy: 0.6785 - val_loss: 0.9856\n",
      "Epoch 74/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7503 - loss: 0.7123 - val_accuracy: 0.6755 - val_loss: 1.0279\n",
      "Epoch 75/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7892 - loss: 0.6396 - val_accuracy: 0.6504 - val_loss: 1.0305\n",
      "Epoch 76/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7364 - loss: 0.7804 - val_accuracy: 0.6549 - val_loss: 1.0349\n",
      "Epoch 77/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7607 - loss: 0.6833 - val_accuracy: 0.6298 - val_loss: 1.1637\n",
      "Epoch 78/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - accuracy: 0.7919 - loss: 0.5847 - val_accuracy: 0.6622 - val_loss: 1.0012\n",
      "Epoch 79/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7899 - loss: 0.6162 - val_accuracy: 0.5767 - val_loss: 1.3586\n",
      "Epoch 80/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7927 - loss: 0.6091 - val_accuracy: 0.6696 - val_loss: 0.9737\n",
      "Epoch 81/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8125 - loss: 0.5718 - val_accuracy: 0.6755 - val_loss: 1.0100\n",
      "Epoch 82/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8104 - loss: 0.5640 - val_accuracy: 0.6268 - val_loss: 1.1907\n",
      "Epoch 83/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7901 - loss: 0.6207 - val_accuracy: 0.6401 - val_loss: 1.0740\n",
      "Epoch 84/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7689 - loss: 0.6862 - val_accuracy: 0.6962 - val_loss: 0.9446\n",
      "Epoch 85/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8105 - loss: 0.5349 - val_accuracy: 0.7109 - val_loss: 0.8819\n",
      "Epoch 86/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8087 - loss: 0.5396 - val_accuracy: 0.6652 - val_loss: 0.9957\n",
      "Epoch 87/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7689 - loss: 0.6647 - val_accuracy: 0.6239 - val_loss: 1.1856\n",
      "Epoch 88/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7921 - loss: 0.5872 - val_accuracy: 0.6947 - val_loss: 0.9208\n",
      "Epoch 89/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8235 - loss: 0.5285 - val_accuracy: 0.6917 - val_loss: 0.9469\n",
      "Epoch 90/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8235 - loss: 0.5187 - val_accuracy: 0.7094 - val_loss: 0.9093\n",
      "Epoch 91/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8667 - loss: 0.4311 - val_accuracy: 0.7124 - val_loss: 0.8903\n",
      "Epoch 92/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8124 - loss: 0.5521 - val_accuracy: 0.7021 - val_loss: 0.9624\n",
      "Epoch 93/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8055 - loss: 0.5540 - val_accuracy: 0.6372 - val_loss: 1.1806\n",
      "Epoch 94/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7547 - loss: 0.7174 - val_accuracy: 0.6917 - val_loss: 0.9552\n",
      "Epoch 95/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8439 - loss: 0.4722 - val_accuracy: 0.6799 - val_loss: 1.0154\n",
      "Epoch 96/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8310 - loss: 0.4940 - val_accuracy: 0.6652 - val_loss: 1.0558\n",
      "Epoch 97/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8375 - loss: 0.4883 - val_accuracy: 0.6829 - val_loss: 0.9755\n",
      "Epoch 98/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8219 - loss: 0.5267 - val_accuracy: 0.7035 - val_loss: 0.9004\n",
      "Epoch 99/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8362 - loss: 0.4876 - val_accuracy: 0.6917 - val_loss: 0.9937\n",
      "Epoch 100/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8262 - loss: 0.5124 - val_accuracy: 0.6534 - val_loss: 1.0617\n",
      "Epoch 101/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8386 - loss: 0.4785 - val_accuracy: 0.6976 - val_loss: 0.9691\n",
      "Epoch 102/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.8537 - loss: 0.4524 - val_accuracy: 0.6608 - val_loss: 1.1035\n",
      "Epoch 103/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8186 - loss: 0.5258 - val_accuracy: 0.6785 - val_loss: 0.9772\n",
      "Epoch 104/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8121 - loss: 0.5169 - val_accuracy: 0.7301 - val_loss: 0.8539\n",
      "Epoch 105/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8415 - loss: 0.4429 - val_accuracy: 0.7139 - val_loss: 0.8959\n",
      "Epoch 106/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8546 - loss: 0.4267 - val_accuracy: 0.6150 - val_loss: 1.3118\n",
      "Epoch 107/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8605 - loss: 0.4434 - val_accuracy: 0.7065 - val_loss: 0.9477\n",
      "Epoch 108/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8510 - loss: 0.4333 - val_accuracy: 0.7271 - val_loss: 0.9011\n",
      "Epoch 109/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8561 - loss: 0.4230 - val_accuracy: 0.6991 - val_loss: 0.9621\n",
      "Epoch 110/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8631 - loss: 0.4090 - val_accuracy: 0.7153 - val_loss: 0.8859\n",
      "Epoch 111/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8762 - loss: 0.3714 - val_accuracy: 0.7227 - val_loss: 0.8695\n",
      "Epoch 112/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8426 - loss: 0.4687 - val_accuracy: 0.7021 - val_loss: 0.9866\n",
      "Epoch 113/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8675 - loss: 0.3937 - val_accuracy: 0.7080 - val_loss: 0.9056\n",
      "Epoch 114/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8679 - loss: 0.3842 - val_accuracy: 0.6195 - val_loss: 1.2208\n",
      "Epoch 115/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8417 - loss: 0.4479 - val_accuracy: 0.7183 - val_loss: 0.8814\n",
      "Epoch 116/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8647 - loss: 0.4047 - val_accuracy: 0.6917 - val_loss: 0.9309\n",
      "Epoch 117/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8508 - loss: 0.4411 - val_accuracy: 0.7198 - val_loss: 0.9226\n",
      "Epoch 118/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8656 - loss: 0.4273 - val_accuracy: 0.7360 - val_loss: 0.8437\n",
      "Epoch 119/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9011 - loss: 0.3284 - val_accuracy: 0.6681 - val_loss: 1.0818\n",
      "Epoch 120/120\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8329 - loss: 0.4784 - val_accuracy: 0.6475 - val_loss: 1.1899\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    epochs=120,                 \n",
    "    batch_size=32,              \n",
    "    validation_split=0.15,       \n",
    "    callbacks=[early_stopping]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7611 - loss: 0.7995\n",
      "Test Loss: 0.8094561696052551, Test Accuracy: 0.7504425048828125\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86       104\n",
      "           1       0.88      0.85      0.87        95\n",
      "           2       0.65      0.57      0.61        91\n",
      "           3       0.71      0.77      0.74        74\n",
      "           4       0.74      0.65      0.69        77\n",
      "           5       0.96      0.96      0.96        91\n",
      "           6       0.64      0.70      0.67        33\n",
      "           7       0.62      0.60      0.61        78\n",
      "           8       0.66      0.73      0.69        78\n",
      "           9       0.85      0.85      0.85        27\n",
      "          10       0.51      0.55      0.53        42\n",
      "          11       0.76      0.94      0.84        17\n",
      "          12       0.77      0.92      0.84        25\n",
      "          13       0.70      0.78      0.74         9\n",
      "          14       0.71      0.70      0.71        91\n",
      "          15       0.75      0.85      0.80        89\n",
      "          16       0.79      0.68      0.73       109\n",
      "\n",
      "    accuracy                           0.75      1130\n",
      "   macro avg       0.74      0.76      0.75      1130\n",
      "weighted avg       0.75      0.75      0.75      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0850 - loss: 2.7263 - val_accuracy: 0.1018 - val_loss: 2.7034\n",
      "Epoch 2/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0837 - loss: 2.7320 - val_accuracy: 0.1018 - val_loss: 2.7067\n",
      "Epoch 3/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.0796 - loss: 2.7366 - val_accuracy: 0.1018 - val_loss: 2.6956\n",
      "Epoch 4/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.0789 - loss: 2.7209 - val_accuracy: 0.1165 - val_loss: 2.6919\n",
      "Epoch 5/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.0757 - loss: 2.7116 - val_accuracy: 0.0664 - val_loss: 2.6457\n",
      "Epoch 6/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0776 - loss: 2.6889 - val_accuracy: 0.0634 - val_loss: 2.7560\n",
      "Epoch 7/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0840 - loss: 2.7232 - val_accuracy: 0.1195 - val_loss: 2.6415\n",
      "Epoch 8/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.0966 - loss: 2.6422 - val_accuracy: 0.1475 - val_loss: 2.5541\n",
      "Epoch 9/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1354 - loss: 2.5546 - val_accuracy: 0.1283 - val_loss: 2.4890\n",
      "Epoch 10/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1425 - loss: 2.5014 - val_accuracy: 0.1475 - val_loss: 2.4291\n",
      "Epoch 11/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.1720 - loss: 2.3841 - val_accuracy: 0.2153 - val_loss: 2.3137\n",
      "Epoch 12/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2046 - loss: 2.3250 - val_accuracy: 0.1416 - val_loss: 2.4992\n",
      "Epoch 13/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2125 - loss: 2.2892 - val_accuracy: 0.2625 - val_loss: 2.1859\n",
      "Epoch 14/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2355 - loss: 2.2159 - val_accuracy: 0.2965 - val_loss: 2.0699\n",
      "Epoch 15/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2334 - loss: 2.2306 - val_accuracy: 0.3097 - val_loss: 2.0469\n",
      "Epoch 16/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.2799 - loss: 2.0969 - val_accuracy: 0.2861 - val_loss: 2.0941\n",
      "Epoch 17/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.3151 - loss: 2.0001 - val_accuracy: 0.3201 - val_loss: 2.0178\n",
      "Epoch 18/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3048 - loss: 1.9945 - val_accuracy: 0.3378 - val_loss: 1.9794\n",
      "Epoch 19/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3055 - loss: 2.0173 - val_accuracy: 0.3053 - val_loss: 2.0554\n",
      "Epoch 20/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3190 - loss: 1.9998 - val_accuracy: 0.3422 - val_loss: 1.9233\n",
      "Epoch 21/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3099 - loss: 1.9902 - val_accuracy: 0.3732 - val_loss: 1.8810\n",
      "Epoch 22/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.3538 - loss: 1.8382 - val_accuracy: 0.2566 - val_loss: 2.1043\n",
      "Epoch 23/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3782 - loss: 1.8126 - val_accuracy: 0.3422 - val_loss: 1.9145\n",
      "Epoch 24/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3813 - loss: 1.7969 - val_accuracy: 0.4041 - val_loss: 1.7554\n",
      "Epoch 25/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.3986 - loss: 1.7335 - val_accuracy: 0.4027 - val_loss: 1.7773\n",
      "Epoch 26/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.4015 - loss: 1.7363 - val_accuracy: 0.4204 - val_loss: 1.7230\n",
      "Epoch 27/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4201 - loss: 1.6828 - val_accuracy: 0.4381 - val_loss: 1.6511\n",
      "Epoch 28/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.4556 - loss: 1.5773 - val_accuracy: 0.4484 - val_loss: 1.6152\n",
      "Epoch 29/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4605 - loss: 1.5808 - val_accuracy: 0.4322 - val_loss: 1.5762\n",
      "Epoch 30/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.4775 - loss: 1.5196 - val_accuracy: 0.4749 - val_loss: 1.5660\n",
      "Epoch 31/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5378 - loss: 1.3984 - val_accuracy: 0.4808 - val_loss: 1.5199\n",
      "Epoch 32/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5368 - loss: 1.3812 - val_accuracy: 0.4985 - val_loss: 1.4802\n",
      "Epoch 33/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.5651 - loss: 1.3279 - val_accuracy: 0.4720 - val_loss: 1.5997\n",
      "Epoch 34/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.5384 - loss: 1.3734 - val_accuracy: 0.5383 - val_loss: 1.3396\n",
      "Epoch 35/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5758 - loss: 1.2275 - val_accuracy: 0.5251 - val_loss: 1.3677\n",
      "Epoch 36/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5826 - loss: 1.2779 - val_accuracy: 0.5354 - val_loss: 1.3352\n",
      "Epoch 37/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5901 - loss: 1.2118 - val_accuracy: 0.5428 - val_loss: 1.3403\n",
      "Epoch 38/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5983 - loss: 1.2183 - val_accuracy: 0.5575 - val_loss: 1.2706\n",
      "Epoch 39/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.5890 - loss: 1.1997 - val_accuracy: 0.5841 - val_loss: 1.2032\n",
      "Epoch 40/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6081 - loss: 1.1562 - val_accuracy: 0.5442 - val_loss: 1.3134\n",
      "Epoch 41/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6080 - loss: 1.1261 - val_accuracy: 0.5649 - val_loss: 1.2402\n",
      "Epoch 42/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6214 - loss: 1.1165 - val_accuracy: 0.5841 - val_loss: 1.1952\n",
      "Epoch 43/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6437 - loss: 1.0514 - val_accuracy: 0.4941 - val_loss: 1.4828\n",
      "Epoch 44/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6214 - loss: 1.1393 - val_accuracy: 0.5885 - val_loss: 1.1914\n",
      "Epoch 45/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6551 - loss: 1.0455 - val_accuracy: 0.6106 - val_loss: 1.0908\n",
      "Epoch 46/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6349 - loss: 1.0726 - val_accuracy: 0.6121 - val_loss: 1.1088\n",
      "Epoch 47/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6630 - loss: 0.9974 - val_accuracy: 0.5885 - val_loss: 1.1839\n",
      "Epoch 48/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6534 - loss: 1.0513 - val_accuracy: 0.6298 - val_loss: 1.0677\n",
      "Epoch 49/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6969 - loss: 0.9346 - val_accuracy: 0.6003 - val_loss: 1.1970\n",
      "Epoch 50/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6709 - loss: 0.9697 - val_accuracy: 0.6342 - val_loss: 1.0824\n",
      "Epoch 51/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7021 - loss: 0.8881 - val_accuracy: 0.6091 - val_loss: 1.1259\n",
      "Epoch 52/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6760 - loss: 0.9314 - val_accuracy: 0.6136 - val_loss: 1.1808\n",
      "Epoch 53/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6759 - loss: 0.9761 - val_accuracy: 0.6416 - val_loss: 1.0432\n",
      "Epoch 54/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7045 - loss: 0.8471 - val_accuracy: 0.6268 - val_loss: 1.0539\n",
      "Epoch 55/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7076 - loss: 0.8563 - val_accuracy: 0.6224 - val_loss: 1.0410\n",
      "Epoch 56/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7036 - loss: 0.8949 - val_accuracy: 0.6445 - val_loss: 1.0680\n",
      "Epoch 57/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6992 - loss: 0.8839 - val_accuracy: 0.6165 - val_loss: 1.0962\n",
      "Epoch 58/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6901 - loss: 0.9170 - val_accuracy: 0.6799 - val_loss: 0.9830\n",
      "Epoch 59/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7265 - loss: 0.8052 - val_accuracy: 0.6726 - val_loss: 0.9330\n",
      "Epoch 60/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7395 - loss: 0.7847 - val_accuracy: 0.6652 - val_loss: 0.9909\n",
      "Epoch 61/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7576 - loss: 0.7592 - val_accuracy: 0.6858 - val_loss: 0.9446\n",
      "Epoch 62/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7571 - loss: 0.7052 - val_accuracy: 0.6711 - val_loss: 0.9645\n",
      "Epoch 63/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7478 - loss: 0.7600 - val_accuracy: 0.6726 - val_loss: 0.9806\n",
      "Epoch 64/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7460 - loss: 0.7379 - val_accuracy: 0.5619 - val_loss: 1.3404\n",
      "Epoch 65/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6919 - loss: 0.9273 - val_accuracy: 0.6637 - val_loss: 1.0026\n",
      "Epoch 66/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7471 - loss: 0.7323 - val_accuracy: 0.6829 - val_loss: 0.9507\n",
      "Epoch 67/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7867 - loss: 0.6585 - val_accuracy: 0.6903 - val_loss: 0.9220\n",
      "Epoch 68/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7629 - loss: 0.6904 - val_accuracy: 0.6962 - val_loss: 0.9094\n",
      "Epoch 69/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7660 - loss: 0.6565 - val_accuracy: 0.6578 - val_loss: 1.0076\n",
      "Epoch 70/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7548 - loss: 0.7235 - val_accuracy: 0.6726 - val_loss: 0.9518\n",
      "Epoch 71/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7313 - loss: 0.7517 - val_accuracy: 0.7021 - val_loss: 0.9084\n",
      "Epoch 72/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7829 - loss: 0.6491 - val_accuracy: 0.6696 - val_loss: 0.9840\n",
      "Epoch 73/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7542 - loss: 0.7225 - val_accuracy: 0.6770 - val_loss: 0.9499\n",
      "Epoch 74/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7754 - loss: 0.6437 - val_accuracy: 0.6814 - val_loss: 0.9217\n",
      "Epoch 75/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7720 - loss: 0.6675 - val_accuracy: 0.6873 - val_loss: 0.9481\n",
      "Epoch 76/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7993 - loss: 0.5753 - val_accuracy: 0.6799 - val_loss: 0.9716\n",
      "Epoch 77/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8105 - loss: 0.5624 - val_accuracy: 0.6578 - val_loss: 1.0137\n",
      "Epoch 78/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7788 - loss: 0.6384 - val_accuracy: 0.7227 - val_loss: 0.8622\n",
      "Epoch 79/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8150 - loss: 0.5598 - val_accuracy: 0.7139 - val_loss: 0.8584\n",
      "Epoch 80/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8165 - loss: 0.5429 - val_accuracy: 0.6578 - val_loss: 1.0605\n",
      "Epoch 81/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7817 - loss: 0.6334 - val_accuracy: 0.6726 - val_loss: 0.9425\n",
      "Epoch 82/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7972 - loss: 0.5868 - val_accuracy: 0.7109 - val_loss: 0.8355\n",
      "Epoch 83/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8247 - loss: 0.5125 - val_accuracy: 0.6976 - val_loss: 0.8911\n",
      "Epoch 84/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8104 - loss: 0.5600 - val_accuracy: 0.6652 - val_loss: 1.0069\n",
      "Epoch 85/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8336 - loss: 0.5224 - val_accuracy: 0.7080 - val_loss: 0.8766\n",
      "Epoch 86/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8285 - loss: 0.4848 - val_accuracy: 0.6917 - val_loss: 0.9573\n",
      "Epoch 87/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8364 - loss: 0.4874 - val_accuracy: 0.7050 - val_loss: 0.8646\n",
      "Epoch 88/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8284 - loss: 0.5074 - val_accuracy: 0.6932 - val_loss: 0.9184\n",
      "Epoch 89/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8214 - loss: 0.5119 - val_accuracy: 0.7021 - val_loss: 0.9159\n",
      "Epoch 90/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8093 - loss: 0.5453 - val_accuracy: 0.7021 - val_loss: 0.9007\n",
      "Epoch 91/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8384 - loss: 0.4678 - val_accuracy: 0.7139 - val_loss: 0.8544\n",
      "Epoch 92/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8452 - loss: 0.4431 - val_accuracy: 0.7301 - val_loss: 0.8282\n",
      "Epoch 93/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8491 - loss: 0.4527 - val_accuracy: 0.7168 - val_loss: 0.8425\n",
      "Epoch 94/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8597 - loss: 0.4218 - val_accuracy: 0.7212 - val_loss: 0.8876\n",
      "Epoch 95/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8506 - loss: 0.4480 - val_accuracy: 0.6726 - val_loss: 0.9944\n",
      "Epoch 96/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8430 - loss: 0.4657 - val_accuracy: 0.6740 - val_loss: 1.0328\n",
      "Epoch 97/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8482 - loss: 0.4485 - val_accuracy: 0.7257 - val_loss: 0.8345\n",
      "Epoch 98/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8524 - loss: 0.4291 - val_accuracy: 0.7257 - val_loss: 0.8719\n",
      "Epoch 99/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8382 - loss: 0.4669 - val_accuracy: 0.7109 - val_loss: 0.8502\n",
      "Epoch 100/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8670 - loss: 0.3910 - val_accuracy: 0.6667 - val_loss: 0.9400\n",
      "Epoch 101/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7999 - loss: 0.5468 - val_accuracy: 0.7212 - val_loss: 0.8300\n",
      "Epoch 102/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8784 - loss: 0.3623 - val_accuracy: 0.7065 - val_loss: 0.9523\n",
      "Epoch 103/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.8426 - loss: 0.4424 - val_accuracy: 0.6976 - val_loss: 0.8531\n",
      "Epoch 104/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8452 - loss: 0.4303 - val_accuracy: 0.7153 - val_loss: 0.8336\n",
      "Epoch 105/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8867 - loss: 0.3563 - val_accuracy: 0.6785 - val_loss: 1.0357\n",
      "Epoch 106/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8300 - loss: 0.4959 - val_accuracy: 0.6917 - val_loss: 0.9905\n",
      "Epoch 107/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8559 - loss: 0.4087 - val_accuracy: 0.6844 - val_loss: 0.9898\n",
      "Epoch 108/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8772 - loss: 0.3594 - val_accuracy: 0.7183 - val_loss: 0.8412\n",
      "Epoch 109/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8744 - loss: 0.3829 - val_accuracy: 0.7375 - val_loss: 0.8626\n",
      "Epoch 110/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8960 - loss: 0.3434 - val_accuracy: 0.7168 - val_loss: 0.8784\n",
      "Epoch 111/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8553 - loss: 0.4270 - val_accuracy: 0.7345 - val_loss: 0.8376\n",
      "Epoch 112/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9036 - loss: 0.3008 - val_accuracy: 0.7212 - val_loss: 0.8422\n",
      "Epoch 113/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.8541 - loss: 0.4556 - val_accuracy: 0.7448 - val_loss: 0.8031\n",
      "Epoch 114/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9062 - loss: 0.2899 - val_accuracy: 0.7183 - val_loss: 0.8669\n",
      "Epoch 115/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9089 - loss: 0.2748 - val_accuracy: 0.7257 - val_loss: 0.8914\n",
      "Epoch 116/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8909 - loss: 0.3395 - val_accuracy: 0.7404 - val_loss: 0.8317\n",
      "Epoch 117/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9218 - loss: 0.2618 - val_accuracy: 0.6785 - val_loss: 1.1040\n",
      "Epoch 118/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8836 - loss: 0.3382 - val_accuracy: 0.7065 - val_loss: 0.8818\n",
      "Epoch 119/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8754 - loss: 0.3721 - val_accuracy: 0.7286 - val_loss: 0.9099\n",
      "Epoch 120/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8868 - loss: 0.3470 - val_accuracy: 0.7168 - val_loss: 0.8610\n",
      "Epoch 121/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8800 - loss: 0.3499 - val_accuracy: 0.7227 - val_loss: 0.8769\n",
      "Epoch 122/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9052 - loss: 0.3031 - val_accuracy: 0.7448 - val_loss: 0.7974\n",
      "Epoch 123/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9268 - loss: 0.2316 - val_accuracy: 0.7227 - val_loss: 0.8716\n",
      "Epoch 124/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8770 - loss: 0.3542 - val_accuracy: 0.7257 - val_loss: 0.8260\n",
      "Epoch 125/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8907 - loss: 0.3055 - val_accuracy: 0.7198 - val_loss: 0.8637\n",
      "Epoch 126/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9247 - loss: 0.2559 - val_accuracy: 0.7316 - val_loss: 0.9094\n",
      "Epoch 127/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8980 - loss: 0.3038 - val_accuracy: 0.7330 - val_loss: 0.8470\n",
      "Epoch 128/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8301 - loss: 0.4997 - val_accuracy: 0.7212 - val_loss: 0.8493\n",
      "Epoch 129/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9077 - loss: 0.2797 - val_accuracy: 0.7419 - val_loss: 0.8164\n",
      "Epoch 130/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9182 - loss: 0.2599 - val_accuracy: 0.7463 - val_loss: 0.8139\n",
      "Epoch 131/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9440 - loss: 0.1905 - val_accuracy: 0.6903 - val_loss: 1.0255\n",
      "Epoch 132/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - accuracy: 0.8237 - loss: 0.5103 - val_accuracy: 0.7522 - val_loss: 0.8406\n",
      "Epoch 133/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9309 - loss: 0.2234 - val_accuracy: 0.7448 - val_loss: 0.8515\n",
      "Epoch 134/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9430 - loss: 0.1963 - val_accuracy: 0.7684 - val_loss: 0.7825\n",
      "Epoch 135/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9482 - loss: 0.1880 - val_accuracy: 0.7345 - val_loss: 0.8924\n",
      "Epoch 136/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9164 - loss: 0.2509 - val_accuracy: 0.7153 - val_loss: 0.9131\n",
      "Epoch 137/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9329 - loss: 0.2239 - val_accuracy: 0.7198 - val_loss: 0.9502\n",
      "Epoch 138/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9206 - loss: 0.2484 - val_accuracy: 0.7463 - val_loss: 0.8892\n",
      "Epoch 139/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9245 - loss: 0.2426 - val_accuracy: 0.6770 - val_loss: 1.0475\n",
      "Epoch 140/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9103 - loss: 0.2666 - val_accuracy: 0.7227 - val_loss: 0.9882\n",
      "Epoch 141/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8732 - loss: 0.3623 - val_accuracy: 0.7537 - val_loss: 0.8786\n",
      "Epoch 142/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9463 - loss: 0.1864 - val_accuracy: 0.7537 - val_loss: 0.8610\n",
      "Epoch 143/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9475 - loss: 0.1700 - val_accuracy: 0.6991 - val_loss: 1.0050\n",
      "Epoch 144/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9412 - loss: 0.1924 - val_accuracy: 0.7566 - val_loss: 0.8409\n",
      "Epoch 145/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9455 - loss: 0.1856 - val_accuracy: 0.7448 - val_loss: 0.8624\n",
      "Epoch 146/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9357 - loss: 0.2061 - val_accuracy: 0.7271 - val_loss: 0.9189\n",
      "Epoch 147/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9380 - loss: 0.1953 - val_accuracy: 0.7493 - val_loss: 0.8338\n",
      "Epoch 148/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9664 - loss: 0.1293 - val_accuracy: 0.7242 - val_loss: 0.9517\n",
      "Epoch 149/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.8749 - loss: 0.3532 - val_accuracy: 0.6667 - val_loss: 1.1700\n",
      "Epoch 150/150\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.8564 - loss: 0.4164 - val_accuracy: 0.7286 - val_loss: 0.8612\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train_encoded,\n",
    "    epochs=150,                 \n",
    "    batch_size=32,              \n",
    "    validation_split=0.15,       \n",
    "    # callbacks=[early_stopping]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7560 - loss: 0.8684\n",
      "Test Loss: 0.8895268440246582, Test Accuracy: 0.7486725449562073\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       104\n",
      "           1       0.90      0.84      0.87        95\n",
      "           2       0.69      0.67      0.68        91\n",
      "           3       0.65      0.88      0.75        74\n",
      "           4       0.71      0.70      0.71        77\n",
      "           5       0.94      0.93      0.94        91\n",
      "           6       0.58      0.55      0.56        33\n",
      "           7       0.59      0.55      0.57        78\n",
      "           8       0.67      0.74      0.71        78\n",
      "           9       0.92      0.85      0.88        27\n",
      "          10       0.49      0.50      0.49        42\n",
      "          11       0.74      0.82      0.78        17\n",
      "          12       0.85      0.88      0.86        25\n",
      "          13       0.62      0.56      0.59         9\n",
      "          14       0.70      0.76      0.73        91\n",
      "          15       0.78      0.82      0.80        89\n",
      "          16       0.82      0.64      0.72       109\n",
      "\n",
      "    accuracy                           0.75      1130\n",
      "   macro avg       0.74      0.74      0.73      1130\n",
      "weighted avg       0.75      0.75      0.75      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test) # Replace `model` with your trained LSTM model and `X_test` with your test dataset\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_encoded, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# with open('lstmmodel.pkl', 'wb') as file:\n",
    "#     pickle.dump(model, file)\n",
    "model.save('lstmmodel.h5')\n",
    "tf.saved_model.save(model, 'saved_model')\n",
    "# pip install openvino-dev[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lstmmodel.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstmmodel.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel):\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lstmmodel.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('lstmmodel.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "if isinstance(model, tf.keras.Model):\n",
    "    model.save('lstmmodel.h5')\n",
    "    \n",
    "    tf.saved_model.save(model, 'saved_model')\n",
    "    # model.save('path_to_your_model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from openvino.runtime import Core, serialize\n",
    "\n",
    "# Convert to IR (example with quantization)\n",
    "mo_cmd = f\"mo --saved_model_dir saved_model --input_shape '[1, 20, 199]' --output_dir ir --data_type FP16\"\n",
    "import subprocess\n",
    "subprocess.call(mo_cmd, shell=True)\n",
    "\n",
    "# convert a saved model to IR format using the Model Optimizer in python through core\n",
    "core = Core()\n",
    "model = core.read_model(\"saved_model\")\n",
    "compiled_model = core.compile_model(model, \"CPU\")\n",
    "serialize(model, xml_path=\"ir/intel_model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Output: names[inputs] shape[?,20,82] type: f32>]\n",
      "[<Output: names[output_0] shape[?,17] type: f32>]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_xml = \"ir/intel_model.xml\"\n",
    "model = core.read_model(model=model_xml)\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IR model and compile for target device\n",
    "core = Core()\n",
    "model_ = core.read_model(\"ir/intel_model.xml\")\n",
    "compiled_model = core.compile_model(model, \"CPU\")\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for single input models only\n",
    "result = compiled_model(input_data)[output_layer]\n",
    "\n",
    "# for multiple inputs in a list\n",
    "result = compiled_model([input_data])[output_layer]\n",
    "\n",
    "# or using a dictionary, where the key is input tensor name or index\n",
    "result = compiled_model({input_layer.any_name: input_data})[output_layer]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
